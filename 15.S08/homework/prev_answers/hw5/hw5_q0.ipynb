{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 0: Backoff language model (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We know that having unknown words in text is a problem for a language model. Any estimate of probability is difficult in such a scenario. \n",
    "\n",
    "### In class, we saw a simple way of smoothing probabilities by adding count 1 to every occuring ngram. While this can be a simple and effective technique we can do something a bit more clever. In this exercise we will implement two such techniques. \n",
    "\n",
    "### 1) to deal with unknown unigrams we will introduce a special `<unk>` token in our training data to represent rare tokens\n",
    "\n",
    "### 2) for unknown bigrams we will use a technique called backoff. The idea is to \"backoff\" to a lower order n-gram estimate for the probability if the n-gram is unknown. For example the probability of an unknown bigram `w_1 w_2` can be estimated by looking at the unigram probability of `w_2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "wiki_df = pd.read_csv('./data/kdwd_r1k_articles.csv')\n",
    "\n",
    "def get_tokens(text):\n",
    "    return ['<s>'] + re.findall(r'\\w+', text.lower()) + ['</s>']\n",
    "\n",
    "train_sentences_list = ' '.join(wiki_df['intro_text'].iloc[:-100].tolist()).split('.')\n",
    "test_sentences_list = ' '.join(wiki_df['intro_text'].iloc[-100:].tolist()).split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple Inc',\n",
       " ' is an American multinational technology company headquartered in Cupertino, California, that designs, develops, and sells consumer electronics, computer software, and online services',\n",
       " ' It is considered one of the Big Four tech companies along with Amazon, Google, and Facebook',\n",
       " \" The company's hardware products include the iPhone smartphone, the iPad tablet computer, the Mac personal computer, the iPod portable media player, the Apple Watch smartwatch, the Apple TV digital media player, the AirPods wireless earbuds and the HomePod smart speaker\",\n",
       " \" Apple's software includes the macOS, iOS, iPadOS, watchOS, and tvOS operating systems, the iTunes media player, the Safari web browser, the Shazam acoustic fingerprint utility, and the iLife and iWork creativity and productivity suites, as well as professional applications like Final Cut Pro, Logic Pro, and Xcode\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's build a basic 1-gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_list = [get_tokens(text) for text in train_sentences_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', 'apple', 'inc', '</s>'],\n",
       " ['<s>',\n",
       "  'is',\n",
       "  'an',\n",
       "  'american',\n",
       "  'multinational',\n",
       "  'technology',\n",
       "  'company',\n",
       "  'headquartered',\n",
       "  'in',\n",
       "  'cupertino',\n",
       "  'california',\n",
       "  'that',\n",
       "  'designs',\n",
       "  'develops',\n",
       "  'and',\n",
       "  'sells',\n",
       "  'consumer',\n",
       "  'electronics',\n",
       "  'computer',\n",
       "  'software',\n",
       "  'and',\n",
       "  'online',\n",
       "  'services',\n",
       "  '</s>'],\n",
       " ['<s>',\n",
       "  'it',\n",
       "  'is',\n",
       "  'considered',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'big',\n",
       "  'four',\n",
       "  'tech',\n",
       "  'companies',\n",
       "  'along',\n",
       "  'with',\n",
       "  'amazon',\n",
       "  'google',\n",
       "  'and',\n",
       "  'facebook',\n",
       "  '</s>']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'apple', 'inc']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token_list = [item for sublist in train_token_list for item in sublist]\n",
    "train_token_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counts = Counter()\n",
    "# your code here\n",
    "for word in train_token_list:\n",
    "    unigram_counts[word] += 1\n",
    "n_unigrams = np.sum([v for _, v in unigram_counts.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(n_unigrams == 95491)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_token_prob(token):\n",
    "    return unigram_counts[token] / n_unigrams\n",
    "\n",
    "def get_text_prob_unigram(text):\n",
    "    tokens = get_tokens(text)\n",
    "    logp = 0\n",
    "    for t in tokens:\n",
    "        # code here\n",
    "        logp += np.log(get_unigram_token_prob(t))\n",
    "    return np.exp(logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(get_unigram_token_prob('apple').round(5) == 0.00046)\n",
    "assert(get_text_prob_unigram('the company').round(9) == 2.455e-06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that we haven't yet introduced any smoothing, meaning, out-of-vocabulary words will have a probability of 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lhz98\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in log\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_prob_unigram(\"onomatopoeia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have learned that we can simply add 1 to every word count to prevent this (ref: laplace smoothing). Another way however is to mark rare words within our training set as unknown words. The idea is that the model will then learn how to deal with unknown/rare words, to more correctly evaluate a test text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this, let us first identify all unigrams that occur fewer or equal than k times. Let's use k=1 to start out with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_counts['apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_tokens = set()\n",
    "# you loop code here\n",
    "k = 1\n",
    "for token in train_token_list:\n",
    "    if unigram_counts[token] <= k:\n",
    "        rare_tokens.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(rare_tokens) == 4859)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, let's create a new counter `filtered_unigram_counts` where every token that appears in `rare_tokens` is recorded as the special token `<unk>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_unigram_counts = Counter()\n",
    "for token_list in train_token_list:\n",
    "# your code here\n",
    "    if token_list in rare_tokens:\n",
    "        filtered_unigram_counts['<unk>'] += 1\n",
    "    else:\n",
    "        filtered_unigram_counts[token_list] += 1\n",
    "\n",
    "n_filtered_unigrams = np.sum([v for _, v in filtered_unigram_counts.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(filtered_unigram_counts['<unk>'] == 4859)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use these new counts, let's modify our text probability function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_unigram_token_prob(token):\n",
    "    return filtered_unigram_counts[token] / n_filtered_unigrams\n",
    "\n",
    "def get_text_prob_filtered_unigram(text):\n",
    "    tokens = [token if token in filtered_unigram_counts else '<unk>' for token in get_tokens(text)]# get tokens and convert to <unk> if needed\n",
    "    logp = 0\n",
    "    for t in tokens:\n",
    "        logp += np.log(get_filtered_unigram_token_prob(t))\n",
    "    return np.exp(logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(get_filtered_unigram_token_prob('apple').round(5) == 0.00046)\n",
    "assert(get_text_prob_filtered_unigram('the company').round(9) == 2.455e-06)\n",
    "assert(get_text_prob_filtered_unigram(\"onomatopoeia\").round(5) == 0.00016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that now unknown words actually have a probability higher than some of the rare words that we have already seen before like `apple`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The choice of count 1 to label words as `<unk>`was arbitrary. How could we tune is if we had more time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can change the setup of rare words. For instance, we can set a smaller value for k. In such a way, the probability should be more reasonable.\n"
     ]
    }
   ],
   "source": [
    "# your text answer here\n",
    "print('We can change the setup of rare words. For instance, we can set a smaller value for k. In such a way, the probability should be more reasonable.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's expand our model to bigrams now. Make sure to check if each component in a bigram exists and label it as `<unk>` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bigram_counts = Counter()\n",
    "for i in range(len(train_token_list)-1):\n",
    "    # your loop and 'unk' conversion here\n",
    "    t1 = train_token_list[i] if train_token_list[i] not in rare_tokens else '<unk>'\n",
    "    t2 = train_token_list[i+1] if train_token_list[i+1] not in rare_tokens else '<unk>'\n",
    "    filtered_bigram_counts[t1 + ' ' + t2] += 1\n",
    "\n",
    "def get_filtered_bigram_token_prob(token1, token2):\n",
    "    return filtered_bigram_counts[token1 + ' ' + token2] / filtered_unigram_counts[token1]\n",
    "        \n",
    "def get_text_prob_filtered_bigram(text):\n",
    "    tokens = [token if token in filtered_unigram_counts else '<unk>' for token in get_tokens(text)]\n",
    "    logp = 0\n",
    "    for t1, t2 in zip(tokens[:-1], tokens[1:]):\n",
    "        logp += np.log(get_filtered_bigram_token_prob(t1, t2))\n",
    "    return np.exp(logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(get_text_prob_filtered_bigram('the company').round(5) == 0.00148)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We correctly get a higher probabiliy for `the company`, now that we are respecting bigrams.\n",
    "### However:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lhz98\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in log\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_prob_filtered_bigram('company the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that we still get 0 for unknown bigrams. Let's fix this via Backoff. To reiterate: the idea is to default to unigram probabilities if the bigram is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backoff_bigram_token_prob(token1, token2):\n",
    "    # check if bigram exists and if not return unigram token2 prob \n",
    "    if token1 + ' ' + token2 in filtered_bigram_counts:\n",
    "        return get_filtered_bigram_token_prob(token1, token2)\n",
    "    else:\n",
    "        return get_filtered_unigram_token_prob(token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_prob_backoff_bigram(text):\n",
    "    tokens = [token if token in filtered_unigram_counts else '<unk>' for token in get_tokens(text)]\n",
    "    logp = 0\n",
    "    for t1, t2 in zip(tokens[:-1], tokens[1:]):\n",
    "        logp += np.log(get_backoff_bigram_token_prob(t1, t2))\n",
    "    return np.exp(logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(get_text_prob_backoff_bigram('company the').round(8) == 1.1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can happily now estimate any input text we can think of with running into issues with 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see if this was all worth it. Let's evaluate perplexity.\n",
    "### Specifically compare the perplexity of our filtered unigram model `get_filtered_unigram_token_prob` to our new and improved backoff bigram model `get_backoff_bigram_token_prob`\n",
    "\n",
    "### Note: For easy comparison let's only evaluate `tokens[1:]` for both models such that even the first token can already form a correct bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_ppl_filtered_unigram(text):\n",
    "    tokens = [token if token in filtered_unigram_counts else '<unk>' for token in get_tokens(text)]\n",
    "    # your code\n",
    "    n_tokens = len(tokens)\n",
    "    logp = 0\n",
    "    for t in tokens[1:-1]:\n",
    "        logp += np.log(get_filtered_unigram_token_prob(t))\n",
    "    return (1 / np.exp(logp))**(1 / n_tokens)\n",
    "\n",
    "def get_text_ppl_backoff_bigram(text):\n",
    "    tokens = [token if token in filtered_unigram_counts else '<unk>' for token in get_tokens(text)]\n",
    "    # your code\n",
    "    n_tokens = len(tokens)\n",
    "    logp = 0\n",
    "    for t1, t2 in zip(tokens, tokens[1:-1]):\n",
    "        logp += np.log(get_backoff_bigram_token_prob(t1, t2))\n",
    "    return (1 / np.exp(logp))**(1 / n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_list = []\n",
    "for text in test_sentences_list:\n",
    "    ppl_list.append(get_text_ppl_filtered_unigram(text))\n",
    "model_unigram_ppl = np.mean(ppl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_list = []\n",
    "for text in test_sentences_list:\n",
    "    ppl_list.append(get_text_ppl_backoff_bigram(text))\n",
    "model_bigram_ppl = np.mean(ppl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(model_bigram_ppl < model_unigram_ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seems like it worked very well. Try to find one or two examples of short strings that clearly show that our bigram model is better and why. (Short answer is OK here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'United States'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004633571836538262"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_prob_backoff_bigram(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.395621085297207e-08"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_prob_filtered_unigram(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = 'States United'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6115189766940493e-07"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_prob_backoff_bigram(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.395621085297207e-08"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_prob_filtered_unigram(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The string \"United States\" and its reverse clearly show that our bigram model is better since our bigram model takes order into consideration.\n"
     ]
    }
   ],
   "source": [
    "print('The string \"United States\" and its reverse clearly show that our bigram model is better since our bigram model takes order into consideration.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
