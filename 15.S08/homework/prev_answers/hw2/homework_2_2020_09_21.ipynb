{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Word phrases\n",
    "\n",
    "### In this problem we will look at methods to identify valid n-grams such as 'New York' or 'Barack Obama' while eliminating statistical flukes such as `in the` or `i write`.\n",
    "\n",
    "### Preprocessing such as this can drastically improved embeddings since words can ngrams will often have a different meaning than the sum of its parts\n",
    "### `V('united')` + `V('states')` != `V('united states')`\n",
    "### `V('real')` + `V('estate')` != `V('real estate')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df = pd.read_csv('./data/kdwd_r1k_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>page_title</th>\n",
       "      <th>page_views</th>\n",
       "      <th>intro_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>856</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>190485</td>\n",
       "      <td>Apple Inc. is an American multinational techno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2386</td>\n",
       "      <td>American Airlines</td>\n",
       "      <td>40829</td>\n",
       "      <td>American Airlines, Inc. (AA) is a major Americ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400</td>\n",
       "      <td>Advanced Micro Devices</td>\n",
       "      <td>40665</td>\n",
       "      <td>Advanced Micro Devices, Inc. (AMD) is an Ameri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2439</td>\n",
       "      <td>Anthem</td>\n",
       "      <td>2967</td>\n",
       "      <td>An anthem is a musical composition of celebrat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6690</td>\n",
       "      <td>Coca-Cola</td>\n",
       "      <td>457810</td>\n",
       "      <td>Coca-Cola, or Coke, is a carbonated soft drink...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_id              page_title  page_views  \\\n",
       "0      856              Apple Inc.      190485   \n",
       "1     2386       American Airlines       40829   \n",
       "2     2400  Advanced Micro Devices       40665   \n",
       "3     2439                  Anthem        2967   \n",
       "4     6690               Coca-Cola      457810   \n",
       "\n",
       "                                          intro_text  \n",
       "0  Apple Inc. is an American multinational techno...  \n",
       "1  American Airlines, Inc. (AA) is a major Americ...  \n",
       "2  Advanced Micro Devices, Inc. (AMD) is an Ameri...  \n",
       "3  An anthem is a musical composition of celebrat...  \n",
       "4  Coca-Cola, or Coke, is a carbonated soft drink...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get consecutive unigrams for the 'intro_text' column of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "first argument must be string or compiled pattern",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-901c65aa777a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# list of lists of unigrams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0munigram_pattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munigram_pattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwiki_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'intro_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-901c65aa777a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# list of lists of unigrams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0munigram_pattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munigram_pattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwiki_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'intro_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\lib\\re.py\u001b[0m in \u001b[0;36mfindall\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\re.py\u001b[0m in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: first argument must be string or compiled pattern"
     ]
    }
   ],
   "source": [
    "# list of lists of unigrams\n",
    "unigram_pattern = ...\n",
    "corpus = [re.findall(unigram_pattern, doc.lower()) for doc in wiki_df['intro_text'].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The package `gensim` has a convenient wrapper to obtain statistically significant ngrams/Phrase automatically\n",
    "\n",
    "### we need to first `pip install gensim`\n",
    "### `gensim` is a useful library for anything related to word representations and embeddings. It will come up a few more times. https://radimrehurek.com/gensim/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write some code to parse our corpus and use valid ngrams using `Phrases`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_count_dict = {k.decode('utf8'): v for k, v in phrases.vocab.items()}\n",
    "\n",
    "n_grams = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_grams.shape[0], 'n-grams found')\n",
    "n_grams.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do the results look? Can you improve the results by excluding common terms using the `common_terms` kwarg of `Phrases`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_grams.shape[0], 'n-grams found')\n",
    "n_grams.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This was convenient, but it's also a black box where many of the knobs for tuning are actually broken in the newest version. Let's try to create our own solution for finding n-grams.\n",
    "\n",
    "### To do this, let's start by counting unigrams and bigrams within our corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip: use Counter for easy counting. It behaves similar to a dictionary with some added functionality around counting. such as `my_counter[unknown_key]` returning `0` for all unknown keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "unigram_counter = Counter()\n",
    "bigram_counter = Counter()\n",
    "for tokens in corpus:\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to come up with a score for each bigram that helps us decide on its importance and the fact of whether it is truly a bigram or two independent unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "bigram_df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find ways to sort and filter your output to bigrams that make sense, such as `wells fargo`, `apple inc` or `puerto rico`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "filtered_bigram_df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bigram_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Word vectors via Pointwise Mutual Information (PMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this problem we will investigate another way of creating word representation from word co-occurrences. For this we will create a word-word matrix that counts the number of times that two words appear close to each other.\n",
    "\n",
    "## More formally:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pointwise mutual information (PMI) for a (word, context) pair in a corpus is defined as the probability of their co-occurrence divided by the probabilities of them appearing individually, \n",
    "## $$\n",
    "{\\rm pmi}(w, c) = \\log \\frac{p(w, c)}{p(w) p(c)}\n",
    "$$\n",
    "\n",
    "## $$\n",
    "p(w, c) = \\frac{\n",
    "f_{i,j}\n",
    "}{\n",
    "\\sum_{i=1}^N \\sum_{j=1}^N f_{i,j}\n",
    "}, \\quad \n",
    "p(w) = \\frac{\n",
    "\\sum_{j=1}^N f_{i,j}\n",
    "}{\n",
    "\\sum_{i=1}^N \\sum_{j=1}^N f_{i,j}\n",
    "}, \\quad\n",
    "p(c) = \\frac{\n",
    "\\sum_{i=1}^N f_{i,j}\n",
    "}{\n",
    "\\sum_{i=1}^N \\sum_{j=1}^N f_{i,j}\n",
    "}\n",
    "$$\n",
    "### where $f_{i,j}$ is the word-word count matrix. <br />\n",
    "### In addition we can define the positive pointwise mutual information as, \n",
    "## $$\n",
    "{\\rm ppmi}(w, c) = {\\rm max}\\left[{\\rm pmi(w,c)}, 0 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will implement this on our wiki featured articles dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_feat_df = pd.read_csv('../../data/kdwd_featured_articles.csv')\n",
    "wiki_feat_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = wiki_feat_df['intro_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    token_pattern = ...\n",
    "    return re.findall(token_pattern, text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counts = Counter()\n",
    "for doc in corpus:\n",
    "    # your code here\n",
    "\n",
    "vocab = ...\n",
    "inv_vocab = ...\n",
    "print('vocabulary size: {}'.format(len(unigram_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-grams are a generalization of n-grams: https://en.wikipedia.org/wiki/N-gram#Skip-gram\n",
    "### We will use this term here to find pairs of word within a context window, meaning that all words separated by max N words will be considered a bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use skip-2-grams and context length 2 in each direction\n",
    "word_window_len = 2\n",
    "skipgram_counts = Counter()\n",
    "for doc in corpus:\n",
    "    tokens = get_tokens(doc)\n",
    "    for token_idx, token in enumerate(tokens):\n",
    "        for context_token in tokens[token_idx - word_window_len:token_idx + word_window_len]:\n",
    "            #your code here\n",
    "\n",
    "print('number of skipgrams:', len(skipgram_counts))\n",
    "print('most common:')\n",
    "[((inv_vocab[t1], inv_vocab[t2]), v) for (t1, t2), v in skipgram_counts.most_common(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's create a sparse matrix that contains word-word co-occurrence counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse as ssp\n",
    "\n",
    "row_indxs = []\n",
    "col_indxs = []\n",
    "dat_values = []\n",
    "\n",
    "# your code here\n",
    "\n",
    "wwcnt_mat = ssp.csr_matrix((dat_values, (row_indxs, col_indxs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, create the PPMI matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reusable quantities\n",
    "num_skipgrams = wwcnt_mat.sum()\n",
    "sum_over_words = np.array(wwcnt_mat.sum(axis=0)).flatten()\n",
    "sum_over_contexts = np.array(wwcnt_mat.sum(axis=1)).flatten()\n",
    "\n",
    "ppmi_dat_values = []   # positive pointwise mutial information\n",
    "row_indxs = []  # for creating sparce matrices\n",
    "col_indxs = []  # for creating sparce matrices\n",
    "for (tok_word, tok_context), sg_count in skipgram_counts.items():\n",
    "\n",
    "    nwc = ...\n",
    "    Pwc = ...\n",
    "    nw = ...\n",
    "    Pw = ...\n",
    "    nc = ...\n",
    "    Pc = ...\n",
    "    \n",
    "    pmi = np.log2(Pwc / (Pw * Pc))   \n",
    "    ppmi = max(pmi, 0)\n",
    "    \n",
    "    row_indxs.append(tok_word)\n",
    "    col_indxs.append(tok_context)\n",
    "    ppmi_dat_values.append(ppmi)\n",
    "\n",
    "ppmi_mat = ssp.csr_matrix((ppmi_dat_values, (row_indxs, col_indxs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `ppmi_mat` to investigate the most similar values to a few test terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to speed up calculation we do dimentionality reduction here\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=64, random_state=6006)\n",
    "trafo_ppmi_mat = svd.fit_transform(ppmi_mat)\n",
    "sim_mat = cosine_similarity(trafo_ppmi_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'eminem'\n",
    "# print most similar terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'quantum'\n",
    "# print most similar terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In what way do these embeddings differ the TfIdf based ones we covered in class? Can you think of advantages/disadvantages for each approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Word vectors for different domains\n",
    "\n",
    "\n",
    "### In this problem we will creat embeddings for the `intro_text` column of the datasets `kdwd_featured_articles.csv` and `kdwd_r1k_articles.csv`\n",
    "### We can think of these as examples of 'generic' and 'finance specific' word representations\n",
    "\n",
    "## The goal of this exercise is to compare these two representations and find out which words change meaning the most across these two domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse as ssp\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_df = pd.read_csv('./data/kdwd_featured_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>page_title</th>\n",
       "      <th>page_views</th>\n",
       "      <th>intro_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Autism</td>\n",
       "      <td>49693</td>\n",
       "      <td>Autism is a developmental disorder characteriz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>621</td>\n",
       "      <td>Amphibian</td>\n",
       "      <td>18926</td>\n",
       "      <td>Amphibians are ectothermic, tetrapod vertebrat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>662</td>\n",
       "      <td>Apollo 11</td>\n",
       "      <td>64044</td>\n",
       "      <td>Apollo 11 was the spaceflight that first lande...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>663</td>\n",
       "      <td>Apollo 8</td>\n",
       "      <td>9848</td>\n",
       "      <td>Apollo 8 was the first crewed spacecraft to le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>751</td>\n",
       "      <td>Aikido</td>\n",
       "      <td>18702</td>\n",
       "      <td>is a modern Japanese martial art developed by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5767</th>\n",
       "      <td>61561199</td>\n",
       "      <td>David Hillhouse Buel</td>\n",
       "      <td>86</td>\n",
       "      <td>David Hillhouse Buel (July 19, 1862 – May 23, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5768</th>\n",
       "      <td>61688854</td>\n",
       "      <td>Roman temple of Bziza</td>\n",
       "      <td>627</td>\n",
       "      <td>The Roman temple of Bziza is a well-preserved ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5769</th>\n",
       "      <td>61788967</td>\n",
       "      <td>Hurricane Humberto (2019)</td>\n",
       "      <td>2140</td>\n",
       "      <td>Hurricane Humberto was a large and powerful tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5770</th>\n",
       "      <td>61824268</td>\n",
       "      <td>Battle of Babylon Hill</td>\n",
       "      <td>146</td>\n",
       "      <td>The Battle of Babylon Hill was a skirmish that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5771</th>\n",
       "      <td>62102008</td>\n",
       "      <td>Bombing of Obersalzberg</td>\n",
       "      <td>520</td>\n",
       "      <td>The bombing of Obersalzberg was an air raid ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5772 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_id                 page_title  page_views  \\\n",
       "0           25                     Autism       49693   \n",
       "1          621                  Amphibian       18926   \n",
       "2          662                  Apollo 11       64044   \n",
       "3          663                   Apollo 8        9848   \n",
       "4          751                     Aikido       18702   \n",
       "...        ...                        ...         ...   \n",
       "5767  61561199       David Hillhouse Buel          86   \n",
       "5768  61688854      Roman temple of Bziza         627   \n",
       "5769  61788967  Hurricane Humberto (2019)        2140   \n",
       "5770  61824268     Battle of Babylon Hill         146   \n",
       "5771  62102008    Bombing of Obersalzberg         520   \n",
       "\n",
       "                                             intro_text  \n",
       "0     Autism is a developmental disorder characteriz...  \n",
       "1     Amphibians are ectothermic, tetrapod vertebrat...  \n",
       "2     Apollo 11 was the spaceflight that first lande...  \n",
       "3     Apollo 8 was the first crewed spacecraft to le...  \n",
       "4     is a modern Japanese martial art developed by ...  \n",
       "...                                                 ...  \n",
       "5767  David Hillhouse Buel (July 19, 1862 – May 23, ...  \n",
       "5768  The Roman temple of Bziza is a well-preserved ...  \n",
       "5769  Hurricane Humberto was a large and powerful tr...  \n",
       "5770  The Battle of Babylon Hill was a skirmish that...  \n",
       "5771  The bombing of Obersalzberg was an air raid ca...  \n",
       "\n",
       "[5772 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_df = pd.read_csv('./data/kdwd_r1k_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>page_title</th>\n",
       "      <th>page_views</th>\n",
       "      <th>intro_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>856</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>190485</td>\n",
       "      <td>Apple Inc. is an American multinational techno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2386</td>\n",
       "      <td>American Airlines</td>\n",
       "      <td>40829</td>\n",
       "      <td>American Airlines, Inc. (AA) is a major Americ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400</td>\n",
       "      <td>Advanced Micro Devices</td>\n",
       "      <td>40665</td>\n",
       "      <td>Advanced Micro Devices, Inc. (AMD) is an Ameri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2439</td>\n",
       "      <td>Anthem</td>\n",
       "      <td>2967</td>\n",
       "      <td>An anthem is a musical composition of celebrat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6690</td>\n",
       "      <td>Coca-Cola</td>\n",
       "      <td>457810</td>\n",
       "      <td>Coca-Cola, or Coke, is a carbonated soft drink...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>59351825</td>\n",
       "      <td>Diamondback Energy</td>\n",
       "      <td>1023</td>\n",
       "      <td>Diamondback Energy is a company engaged in hyd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>59690565</td>\n",
       "      <td>The Michaels Companies</td>\n",
       "      <td>4605</td>\n",
       "      <td>The Michaels Companies, Inc. is North America'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>60008806</td>\n",
       "      <td>Cheniere Energy</td>\n",
       "      <td>1124</td>\n",
       "      <td>Cheniere Energy, Inc. is a liquefied natural g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>60653452</td>\n",
       "      <td>CNX Resources</td>\n",
       "      <td>448</td>\n",
       "      <td>CNX Resources is a natural gas company based i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>61188838</td>\n",
       "      <td>L3Harris Technologies</td>\n",
       "      <td>7129</td>\n",
       "      <td>L3Harris Technologies (L3Harris) is an America...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>761 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      page_id              page_title  page_views  \\\n",
       "0         856              Apple Inc.      190485   \n",
       "1        2386       American Airlines       40829   \n",
       "2        2400  Advanced Micro Devices       40665   \n",
       "3        2439                  Anthem        2967   \n",
       "4        6690               Coca-Cola      457810   \n",
       "..        ...                     ...         ...   \n",
       "756  59351825      Diamondback Energy        1023   \n",
       "757  59690565  The Michaels Companies        4605   \n",
       "758  60008806         Cheniere Energy        1124   \n",
       "759  60653452           CNX Resources         448   \n",
       "760  61188838   L3Harris Technologies        7129   \n",
       "\n",
       "                                            intro_text  \n",
       "0    Apple Inc. is an American multinational techno...  \n",
       "1    American Airlines, Inc. (AA) is a major Americ...  \n",
       "2    Advanced Micro Devices, Inc. (AMD) is an Ameri...  \n",
       "3    An anthem is a musical composition of celebrat...  \n",
       "4    Coca-Cola, or Coke, is a carbonated soft drink...  \n",
       "..                                                 ...  \n",
       "756  Diamondback Energy is a company engaged in hyd...  \n",
       "757  The Michaels Companies, Inc. is North America'...  \n",
       "758  Cheniere Energy, Inc. is a liquefied natural g...  \n",
       "759  CNX Resources is a natural gas company based i...  \n",
       "760  L3Harris Technologies (L3Harris) is an America...  \n",
       "\n",
       "[761 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create word representation for our 2 corpora using your favorite method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_vectorizer = TfidfVectorizer(min_df=1, max_df=1.0)\n",
    "generic_mat = generic_vectorizer.fit_transform(generic_df['intro_text'].tolist())\n",
    "generic_vocab = {token: n for n, token in enumerate(pd.Series(generic_vectorizer.vocabulary_).sort_values().index)}\n",
    "\n",
    "finance_vectorizer = TfidfVectorizer(min_df=1, max_df=1.0)\n",
    "finance_mat = finance_vectorizer.fit_transform(finance_df['intro_text'].tolist())\n",
    "finance_vocab = {token: n for n, token in enumerate(pd.Series(finance_vectorizer.vocabulary_).sort_values().index)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since our two corpora use different vocabulary we want to sub-select each representation matrix to be only of vacabulary tokens that occur in both corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_terms = list(set(generic_vocab) & set(finance_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7473"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_generic_mat = []\n",
    "filtered_finance_mat = []\n",
    "\n",
    "# your code here\n",
    "for word in common_terms:\n",
    "    filtered_generic_mat.append(generic_mat[:,generic_vocab[word]])\n",
    "    filtered_finance_mat.append(finance_mat[:,finance_vocab[word]])\n",
    "\n",
    "filtered_generic_mat = ssp.hstack(filtered_generic_mat)\n",
    "filtered_finance_mat = ssp.hstack(filtered_finance_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5772x7473 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 713222 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_generic_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our documents for each corpus are different so there is no direct way of comparing our two representations, even though they now have the same dimension. To get them on equal footing, let's look at the word-word similarlity matrix for each domain.\n",
    "### Comparing these two, find terms that seem to have a drastically different meaning within the two domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the term-term similarity matrix\n",
    "generic_term_sim_mat = cosine_similarity(filtered_generic_mat.T)\n",
    "finance_term_sim_mat = cosine_similarity(filtered_finance_mat.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finance_term_sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 1.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.00691614],\n",
       "       [0.        , 0.        , 1.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.00691614, 0.        , ..., 0.        , 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_term_sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_drift_scores = {}\n",
    "for n, term in enumerate(common_terms):\n",
    "    term_sim = cosine_similarity([generic_term_sim_mat[n]],[finance_term_sim_mat[n]])\n",
    "    term_drift_scores[term] = term_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_drifts = pd.Series(term_drift_scores).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pancakes         [[0.05038100790221931]]\n",
       "bellamy          [[0.06005089573428904]]\n",
       "haas             [[0.07207226355123118]]\n",
       "methodists       [[0.07404461116193248]]\n",
       "circus           [[0.07439743333673383]]\n",
       "oldham           [[0.07447224405096084]]\n",
       "understands      [[0.07458418077041584]]\n",
       "cynthia          [[0.07485072508366464]]\n",
       "individuality    [[0.07515285069220118]]\n",
       "levi             [[0.07648182601374093]]\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_drifts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "it      [[0.7428606442366982]]\n",
       "its     [[0.7529668360683097]]\n",
       "with    [[0.7589772440928931]]\n",
       "by       [[0.765719225451046]]\n",
       "as      [[0.7784307198243836]]\n",
       "to      [[0.7947377813721546]]\n",
       "in      [[0.7976528411563721]]\n",
       "and     [[0.7990783673375985]]\n",
       "of      [[0.8026969896289652]]\n",
       "the     [[0.8192134197968853]]\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_drifts.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Corporate Similarity and Returns\n",
    "### In this example we'll explore how to use NLP to measure corporate similarity\n",
    "\n",
    "### In particular we will\n",
    " - ### Make word vectors for firms in order to get an NLP measure of similarity\n",
    " - ### Measure the quality of this similarity metric by predicting future co-movement of returns. \n",
    " \n",
    "## Step X: This problem uses a few concepts of basic modeling such as `sklearn.model_selection.train_test_split` and `sklearn.linear_model.LinearRegression`\n",
    "## Feel free to read some of the sklearn documentation, but otherwise we will cover these concepts next class\n",
    " \n",
    "\n",
    "# $ \\\\ $\n",
    "## Step 0: Load the MD&A section from Form-10-K from 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/parsed_mda.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, take only the first filing for each company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = []\n",
    "seen = set()\n",
    "for item in data:\n",
    "    if item['ticker'] in seen:\n",
    "        continue\n",
    "    else:\n",
    "        seen.add(item['ticker'])\n",
    "        clean.append(item)\n",
    "data = clean\n",
    "del clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now load the price data for 2015-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.read_csv(\n",
    "    '../../data/sp500_prices.csv', \n",
    "    index_col=0, \n",
    "    parse_dates=True\n",
    ").loc['2015-01-01':'2018-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tickers = [item['ticker'] for item in data]\n",
    "assert len(data_tickers) == len(set(data_tickers)), 'non-unique tickers, this will not work'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: clean the text\n",
    "### Much of NLP boils down to doing reasonable processing on text.\n",
    "### First, we'll try out very minimial processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mda_simple(mda):\n",
    "    return mda.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add import here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pairwise Word similarity\n",
    "### Calculate the pariwise cosine similarity between word vectors\n",
    "### Make the cosine similarities into a dataframe indexed/columned on ticker symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sims = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Why `cosine_similarity` and not another measure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Wrangle the price and word data\n",
    "### Our goal here is to have a dataframe which is indexed on PAIRS of tickers and has columns\n",
    " - ### `returns_correlation`: the correlation of returns for those two tickers from Jan 1 2016 to Jan 1 2017\n",
    " - ### `word_similarity`: the cosine similarity of the word vectors for the two companies' MD&A sections\n",
    " \n",
    "## Tips\n",
    " - ### NB: use pct_change to calculate returns in pandas\n",
    " - ### NB: use the pandas builtin corr function to calculate correlations (we don't need anything fancy)\n",
    " - ### NB: the index of the dataframe should have two columns (the tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one way you might do this is\n",
    "rets_cor = ... # calculate returns correlations\n",
    "word_cor = #  calcuate the word similarities in the right shape\n",
    "\n",
    "all_data = rets_cor.join(word_cor)\n",
    "all_data = all_data.dropna()\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a: \n",
    " - ### What is the contemperaneous correlation of these data?\n",
    " - ### Make a scatter plot of the returns correlation and word similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This should be about 12%. That's not bad, but we can do better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.plot.scatter(x='returns_correlation', y='word_similarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Try to predict the future returns correlations\n",
    "### Use OLS (`LinearRegression`) to predict `returns_correlation` from `word_similarity`. \n",
    "### What is the (contemperaneous) out of sample performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df =  ...\n",
    "feature_cols =  ...\n",
    "target_col =  ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here\n",
    "\n",
    "reg = ...# add code here\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(reg.coef_, index=feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is not amazing. We can do better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ \\\\ $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Repeat, but be careful\n",
    "### Here we will see if we can clean the data better\n",
    "\n",
    "### Things to try\n",
    " - ### Look at the histograms of word similarities to see if we can \"ignore\" some ill-behaved data\n",
    " - ### Try limiting how greedy the `TFIDFVectorizer` is: `min_df`, `max_df`, `max_features`, etc.\n",
    " \n",
    "### We will examine our data and look for things that look out of place\n",
    " - ### We will ultimately want our data to look normally distributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mda(mda):\n",
    "    paras = [p.lower() for p in mda.split('\\n') if len(p) > 40]\n",
    "    cleaned =  ' '.join(paras)\n",
    "    words = cleaned.split()\n",
    "    words = [word for word in words if len(word) > 2]\n",
    "    if len(words) > 10:\n",
    "        return ' '.join(words)\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(\n",
    "    min_df=...\n",
    "    max_df= ...\n",
    "    max_features= ...\n",
    ")\n",
    "word_vecs = vec.fit_transform((clean_mda(item['mda']) for item in data))\n",
    "\n",
    "\n",
    "word_sims =  ...\n",
    "# Lots of word similarities are all zeros- so we'll ignore\n",
    "# add code here to remove rows of word_sims where all the elements are zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the returns correlation and the cosine similarities as above\n",
    "all_data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect your data- make some histograms\n",
    "plt.figure(figsize=(12,7))\n",
    "all_data.returns_correlation.hist(bins=40)\n",
    "plt.title('Returns Correlation')\n",
    "plt.figure(figsize=(12,7))\n",
    "all_data.word_similarity.hist(bins=40)\n",
    "plt.title('Word Similarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning our data\n",
    "### It seems lots of things are identically 0 (no word overlap) or identically 1 (the MD&A section for one company perfectly overlaps itself). We will exclude those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine histograms again\n",
    "plt.figure(figsize=(12,7))\n",
    "all_data.returns_correlation.hist(bins=40)\n",
    "plt.title('Returns Correlation')\n",
    "plt.figure(figsize=(12,7))\n",
    "all_data.word_similarity.hist(bins=40)\n",
    "plt.title('Word Similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly, there is a bit of a \"hump\" at low `word_similarity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here\n",
    "all_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The contemperaneous correlation is twice as large!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Now, repeat the exercise of predicting future returns correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df =  ...# Add code here\n",
    "# add code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg =  ...\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(reg.coef_, index=feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is about 5 times better than before!\n",
    "## $ \\\\ $ \n",
    "## Part 7: What will happen if we include last year's returns correlation as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_year_corr =  ...\n",
    "data_df = last_year_corr.join( ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg =  ...\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(reg.coef_, index=feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indeed, we do much better, but the word features still help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
