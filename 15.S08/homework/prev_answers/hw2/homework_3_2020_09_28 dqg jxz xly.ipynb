{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 0: Word embeddings++ (15%)\n",
    "\n",
    "### This is an open ended expercise. Our goal is to create a robust set of embeddings by combining many of the techniques that we have learned so far. \n",
    "\n",
    "## Part I: Create the embeddings by using techniques such as text cleaning using regex, stemming, ngramming, TfIdf, or whatever else you think may yield good embeddings. (Hint: you may want to consider preserving case information for better entity handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df = pd.read_csv('./kdwd_r1k_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorizer and TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    \n",
    "    analyzer='word',\n",
    "    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "    \n",
    "    lowercase=True,\n",
    "        \n",
    "    min_df=3,  # minimum occurence\n",
    "    max_df=1.0,  # max occurence\n",
    "    \n",
    ")\n",
    "\n",
    "count_matrix = vectorizer.fit_transform(wiki_df['intro_text'].tolist());\n",
    "vocab = {token: n for n, token in enumerate(pd.Series(vectorizer.vocabulary_).sort_values().index)}\n",
    "inv_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=64, random_state=6006)\n",
    "trafo_count_matrix = svd.fit_transform(count_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = cosine_similarity(trafo_count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 microsoft\n",
      "0.924 surface\n",
      "0.923 basic\n",
      "0.906 windows\n",
      "0.898 edge\n",
      "0.895 overall\n",
      "0.892 followed\n",
      "0.848 allen\n",
      "0.816 bill\n",
      "0.800 dominant\n"
     ]
    }
   ],
   "source": [
    "token = 'microsoft'\n",
    "word_idx = vocab[token]\n",
    "for idx, row in pd.Series(sim_mat[word_idx]).sort_values(ascending=False).head(10).iteritems():\n",
    "    print('%.3f' % row, inv_vocab[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 oil\n",
      "0.896 refiner\n",
      "0.890 fuels\n",
      "0.847 refining\n",
      "0.825 irving\n",
      "0.813 standard\n",
      "0.810 widely\n",
      "0.749 refineries\n",
      "0.745 damage\n",
      "0.741 14th\n"
     ]
    }
   ],
   "source": [
    "token = 'oil'\n",
    "word_idx = vocab[token]\n",
    "for idx, row in pd.Series(sim_mat[word_idx]).sort_values(ascending=False).head(10).iteritems():\n",
    "    print('%.3f' % row, inv_vocab[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 ceo\n",
      "0.814 president\n",
      "0.755 board\n",
      "0.732 he\n",
      "0.721 failed\n",
      "0.720 chairman\n",
      "0.710 executive\n",
      "0.707 january\n",
      "0.700 focus\n",
      "0.692 size\n"
     ]
    }
   ],
   "source": [
    "token = 'ceo'\n",
    "word_idx = vocab[token]\n",
    "for idx, row in pd.Series(sim_mat[word_idx]).sort_values(ascending=False).head(10).iteritems():\n",
    "    print('%.3f' % row, inv_vocab[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 cloud\n",
      "0.786 code\n",
      "0.784 weather\n",
      "0.731 measurement\n",
      "0.719 account\n",
      "0.719 characters\n",
      "0.717 view\n",
      "0.714 amount\n",
      "0.678 type\n",
      "0.649 completely\n"
     ]
    }
   ],
   "source": [
    "token = 'cloud'\n",
    "word_idx = vocab[token]\n",
    "for idx, row in pd.Series(sim_mat[word_idx]).sort_values(ascending=False).head(10).iteritems():\n",
    "    print('%.3f' % row, inv_vocab[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include n-Gram and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    "vectorizer = CountVectorizer(\n",
    "    \n",
    "    analyzer='word',\n",
    "    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "    \n",
    "    lowercase=True,\n",
    "    stop_words=stop_words.ENGLISH_STOP_WORDS,\n",
    "\n",
    "    ngram_range=(1, 2),  # n-grams\n",
    "    \n",
    "    min_df=3,  # minimum occurence\n",
    "    max_df=1.0,  # max occurence\n",
    "    \n",
    ")\n",
    "\n",
    "count_matrix = vectorizer.fit_transform(wiki_df['intro_text'].tolist());\n",
    "vocab = {token: n for n, token in enumerate(pd.Series(vectorizer.vocabulary_).sort_values().index)}\n",
    "inv_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=64, random_state=6006)\n",
    "trafo_count_matrix = svd.fit_transform(count_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = cosine_similarity(trafo_count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 microsoft\n",
      "0.957 basic\n",
      "0.912 surface\n",
      "0.910 microsoft windows\n",
      "0.910 windows\n",
      "0.902 followed\n",
      "0.900 cloud computing\n",
      "0.891 edge\n",
      "0.885 largest software\n",
      "0.879 30 2018\n"
     ]
    }
   ],
   "source": [
    "token = 'microsoft'\n",
    "word_idx = vocab[token]\n",
    "for idx, row in pd.Series(sim_mat[word_idx]).sort_values(ascending=False).head(10).iteritems():\n",
    "    print('%.3f' % row, inv_vocab[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 oil\n",
      "0.956 oil gas\n",
      "0.941 oil company\n",
      "0.939 headquartered irving\n",
      "0.919 refiner\n",
      "0.919 oil companies\n",
      "0.911 damage\n",
      "0.904 irving texas\n",
      "0.904 largest oil\n",
      "0.894 fuels\n"
     ]
    }
   ],
   "source": [
    "token = 'oil'\n",
    "word_idx = vocab[token]\n",
    "for idx, row in pd.Series(sim_mat[word_idx]).sort_values(ascending=False).head(10).iteritems():\n",
    "    print('%.3f' % row, inv_vocab[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 ceo\n",
      "0.779 president\n",
      "0.727 board\n",
      "0.726 chairman\n",
      "0.714 new ceo\n",
      "0.694 failed\n",
      "0.689 remained\n",
      "0.681 2007\n",
      "0.677 2011\n",
      "0.667 replaced\n"
     ]
    }
   ],
   "source": [
    "token = 'ceo'\n",
    "word_idx = vocab[token]\n",
    "for idx, row in pd.Series(sim_mat[word_idx]).sort_values(ascending=False).head(10).iteritems():\n",
    "    print('%.3f' % row, inv_vocab[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 cloud\n",
      "0.773 code\n",
      "0.764 servers\n",
      "0.752 cloud computing\n",
      "0.675 type\n",
      "0.673 application\n",
      "0.671 software service\n",
      "0.670 web\n",
      "0.666 headquartered seattle\n",
      "0.657 weather\n"
     ]
    }
   ],
   "source": [
    "token = 'cloud'\n",
    "word_idx = vocab[token]\n",
    "for idx, row in pd.Series(sim_mat[word_idx]).sort_values(ascending=False).head(10).iteritems():\n",
    "    print('%.3f' % row, inv_vocab[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TFIDF-Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=3, max_df=1.0)\n",
    "count_matrix = vectorizer.fit_transform(wiki_df['intro_text'].tolist());\n",
    "vocab = {token: n for n, token in enumerate(pd.Series(vectorizer.vocabulary_).sort_values().index)}\n",
    "inv_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=64, random_state=6006)\n",
    "trafo_count_matrix = svd.fit_transform(count_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = cosine_similarity(trafo_count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 microsoft\n",
      "0.913 desktop\n",
      "0.834 server\n",
      "0.829 dominant\n",
      "0.805 x86\n",
      "0.765 windows\n",
      "0.765 architecture\n",
      "0.752 servers\n",
      "0.742 computing\n",
      "0.719 computers\n"
     ]
    }
   ],
   "source": [
    "token = 'microsoft'\n",
    "word_idx = vocab[token]\n",
    "for idx, row in pd.Series(sim_mat[word_idx]).sort_values(ascending=False).head(10).iteritems():\n",
    "    print('%.3f' % row, inv_vocab[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 oil\n",
      "0.884 upstream\n",
      "0.828 malaysia\n",
      "0.820 offshore\n",
      "0.781 drilling\n",
      "0.770 050\n",
      "0.757 rigs\n",
      "0.753 change\n",
      "0.748 completion\n",
      "0.735 production\n"
     ]
    }
   ],
   "source": [
    "token = 'oil'\n",
    "word_idx = vocab[token]\n",
    "for idx, row in pd.Series(sim_mat[word_idx]).sort_values(ascending=False).head(10).iteritems():\n",
    "    print('%.3f' % row, inv_vocab[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 ceo\n",
      "0.839 chairman\n",
      "0.743 president\n",
      "0.682 former\n",
      "0.650 steve\n",
      "0.650 resigned\n",
      "0.617 led\n",
      "0.604 role\n",
      "0.600 served\n",
      "0.595 license\n"
     ]
    }
   ],
   "source": [
    "token = 'ceo'\n",
    "word_idx = vocab[token]\n",
    "for idx, row in pd.Series(sim_mat[word_idx]).sort_values(ascending=False).head(10).iteritems():\n",
    "    print('%.3f' % row, inv_vocab[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 cloud\n",
      "0.813 application\n",
      "0.733 computing\n",
      "0.718 networking\n",
      "0.712 sunnyvale\n",
      "0.689 receive\n",
      "0.686 defined\n",
      "0.682 code\n",
      "0.680 calls\n",
      "0.657 web\n"
     ]
    }
   ],
   "source": [
    "token = 'cloud'\n",
    "word_idx = vocab[token]\n",
    "for idx, row in pd.Series(sim_mat[word_idx]).sort_values(ascending=False).head(10).iteritems():\n",
    "    print('%.3f' % row, inv_vocab[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change our corpus to larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df = pd.read_csv('./kdwd_featured_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=3, max_df=1.0)\n",
    "count_matrix = vectorizer.fit_transform(wiki_df['intro_text'].tolist());\n",
    "vocab = {token: n for n, token in enumerate(pd.Series(vectorizer.vocabulary_).sort_values().index)}\n",
    "inv_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=64, random_state=6006)\n",
    "trafo_count_matrix = svd.fit_transform(count_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = cosine_similarity(trafo_count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 microsoft\n",
      "0.982 developers\n",
      "0.970 xbox\n",
      "0.969 windows\n",
      "0.968 interactive\n",
      "0.967 gameplay\n",
      "0.966 downloadable\n",
      "0.966 graphics\n",
      "0.959 ios\n",
      "0.958 multiplayer\n"
     ]
    }
   ],
   "source": [
    "token = 'microsoft'\n",
    "word_idx = vocab[token]\n",
    "for idx, row in pd.Series(sim_mat[word_idx]).sort_values(ascending=False).head(10).iteritems():\n",
    "    print('%.3f' % row, inv_vocab[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 oil\n",
      "0.961 painting\n",
      "0.955 canvas\n",
      "0.944 paintings\n",
      "0.935 reproductions\n",
      "0.935 nude\n",
      "0.933 exhibited\n",
      "0.928 etty\n",
      "0.927 painterly\n",
      "0.926 indecency\n"
     ]
    }
   ],
   "source": [
    "token = 'oil'\n",
    "word_idx = vocab[token]\n",
    "for idx, row in pd.Series(sim_mat[word_idx]).sort_values(ascending=False).head(10).iteritems():\n",
    "    print('%.3f' % row, inv_vocab[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 ceo\n",
      "0.662 abortion\n",
      "0.648 tuesday\n",
      "0.646 inc\n",
      "0.640 grammys\n",
      "0.634 pendleton\n",
      "0.633 licensing\n",
      "0.628 eli\n",
      "0.615 guinness\n",
      "0.614 nomination\n"
     ]
    }
   ],
   "source": [
    "token = 'ceo'\n",
    "word_idx = vocab[token]\n",
    "for idx, row in pd.Series(sim_mat[word_idx]).sort_values(ascending=False).head(10).iteritems():\n",
    "    print('%.3f' % row, inv_vocab[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000 cloud\n",
      "0.932 velocity\n",
      "0.925 sun\n",
      "0.924 dust\n",
      "0.919 gravitationally\n",
      "0.919 astronomers\n",
      "0.915 massive\n",
      "0.914 planets\n",
      "0.911 objects\n",
      "0.910 disk\n"
     ]
    }
   ],
   "source": [
    "token = 'cloud'\n",
    "word_idx = vocab[token]\n",
    "for idx, row in pd.Series(sim_mat[word_idx]).sort_values(ascending=False).head(10).iteritems():\n",
    "    print('%.3f' % row, inv_vocab[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Think about how we can evaluate our embeddings and compare them to simpler techniques covered in class. (Hint: some basic word statistics and examples can go a long way here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GloVe corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(\n",
    "    './glove.6B.50d.txt', \n",
    "    binary=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('google', 0.8450709581375122),\n",
       " ('ibm', 0.841211199760437),\n",
       " ('software', 0.8281513452529907),\n",
       " ('intel', 0.8273229598999023),\n",
       " ('netscape', 0.8212510347366333),\n",
       " ('yahoo', 0.8106437921524048),\n",
       " ('aol', 0.8012619018554688),\n",
       " ('oracle', 0.7997608184814453),\n",
       " ('cisco', 0.7832400798797607),\n",
       " ('compaq', 0.746559739112854)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('microsoft', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crude', 0.8536815047264099),\n",
       " ('petroleum', 0.8413494229316711),\n",
       " ('gas', 0.8313400745391846),\n",
       " ('energy', 0.8022806644439697),\n",
       " ('supply', 0.7696146965026855),\n",
       " ('fuel', 0.7656652927398682),\n",
       " ('grain', 0.7601252794265747),\n",
       " ('exports', 0.7591190338134766),\n",
       " ('supplies', 0.7502518892288208),\n",
       " ('prices', 0.7354471683502197)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('oil', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('executive', 0.911155641078949),\n",
       " ('chairman', 0.84330815076828),\n",
       " ('chief', 0.7824074029922485),\n",
       " ('vice', 0.7623285055160522),\n",
       " ('managing', 0.7437090873718262),\n",
       " ('manager', 0.729428231716156),\n",
       " ('cfo', 0.7279371023178101),\n",
       " ('co', 0.7262787222862244),\n",
       " ('announced', 0.6999826431274414),\n",
       " ('company', 0.6858142018318176)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('ceo', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clouds', 0.821401059627533),\n",
       " ('dust', 0.7634189128875732),\n",
       " ('horizon', 0.7331849336624146),\n",
       " ('beneath', 0.720876157283783),\n",
       " ('surface', 0.719190239906311),\n",
       " ('sky', 0.7017689943313599),\n",
       " ('visible', 0.7004544138908386),\n",
       " ('ash', 0.6955233812332153),\n",
       " ('sunlight', 0.6899121999740601),\n",
       " ('ocean', 0.686274528503418)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('cloud', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('job', 0.8104128241539001),\n",
       " ('hired', 0.7492485046386719),\n",
       " ('bob', 0.7301130294799805),\n",
       " ('spent', 0.7180465459823608),\n",
       " ('working', 0.7085384726524353)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['steve', 'jobs'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8523603677749634),\n",
       " ('throne', 0.7664333581924438),\n",
       " ('prince', 0.7592144012451172),\n",
       " ('daughter', 0.7473883032798767),\n",
       " ('elizabeth', 0.7460219860076904)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('google', 0.7364129424095154),\n",
       " ('xbox', 0.7173627614974976),\n",
       " ('yahoo', 0.7048122882843018),\n",
       " ('smartphone', 0.6997253894805908),\n",
       " ('xp', 0.6943330764770508)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['microsoft', 'iphone'], negative=['apple'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Tomorrow, Apple will release its new iPhone.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = re.findall('[a-z]+', sentence.lower())\n",
    "token_embed_vectors = [model[t] for t in tokens if t in model]\n",
    "sentence_embed_vector = np.mean(token_embed_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.605 - Fourteen tomatos are heavier than six mushrooms.\n",
      "0.879 - Microsoft is launching Windows 7.\n",
      "0.764 - Rabbits that get released into the wild eat carrots.\n"
     ]
    }
   ],
   "source": [
    "sentence_options = [\n",
    "    \"Fourteen tomatos are heavier than six mushrooms.\",\n",
    "    \"Microsoft is launching Windows 7.\",\n",
    "    \"Rabbits that get released into the wild eat carrots.\",\n",
    "]\n",
    "\n",
    "def get_doc_embed(s):\n",
    "    tokens = re.findall('[a-z]+', s.lower())\n",
    "    return np.mean([model[t] for t in tokens if t in model], axis=0)\n",
    "\n",
    "v1 = get_doc_embed(sentence)\n",
    "for s in sentence_options:\n",
    "    v2 = get_doc_embed(s)\n",
    "    print(round(cosine_similarity([v1], [v2])[0][0], 3), '-', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Metrics vs Loss functions (10%)\n",
    "### As we said in lecture, we optimize (minimize) the loss function, but monitor metrics without letting the model know about their value.\n",
    "\n",
    "### A very common loss/metric pair is crossentropy for the loss and accuracy for the metric. Often, these two numbers correlate, but they are not guaranteed to do some. \n",
    "\n",
    "### In this problem, you will come up with 3 sets of 20 synthetic values for predicted probability in a binary classification problem where the crossentropy and accuracy are anti correlated. Plot the loss vs the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# safe to restart\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)  # 将数值限定在min_value, max_value的范围内\n",
    "    num_datapoints = predictions.shape[0]\n",
    "    ce = -1 * np.sum(targets*np.log(predictions + 1e-9)) / num_datapoints\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hint: not all right and wrong answers are created equally\n",
    "\n",
    "y_true = np.array([1]*20)\n",
    "predictions_1 = np.array([1]*10+[0]*10)\n",
    "predictions_2 = np.array([1]*9+[0.25]*11)\n",
    "predictions_3 = np.array([1]*8+[0.49]*12)\n",
    "\n",
    "preds = [predictions_1, predictions_2, predictions_3]\n",
    "\n",
    "cross_entropies = [cross_entropy(y_pred, y_true) for y_pred in preds]\n",
    "accuracies = [accuracy_score(y_true, y_pred > 0.5) for y_pred in preds]\n",
    "pd.Series(accuracies, index=cross_entropies).plot(\n",
    "    style='-x', figsize=(12,7), fontsize=17,\n",
    "    markersize=18, linewidth=4\n",
    ")\n",
    "plt.xlabel('Cross Entropy', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Cross Validation (10%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Generate some data\n",
    " - Generate data with 1024 points and 10 features. \n",
    " - Create targets that are linear combinations of the 10 features and add some noise. \n",
    " - Split the data into train/test 80/20 percent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "\n",
    "num_observations = 1024\n",
    "coefs = np.array([-1.2, 5, 0, .22, 0, 2, 0, 4, 0, 0])  # notice, there are zeros!\n",
    "noise_amplitude = 5\n",
    "\n",
    "num_variables = coefs.shape[0]\n",
    "\n",
    "x = np.random.randn(num_observations, len(coefs))*noise_amplitude+2\n",
    "y = (np.matmul(x,coefs)+ np.random.randn(1,num_observations)*noise_amplitude).T\n",
    "\n",
    "train_size = int(num_observations*0.8)\n",
    "x_train, x_test = x[:train_size,:], x[train_size:,:]\n",
    "y_train, y_test = y[:train_size,:], y[train_size:,:]\n",
    "\n",
    "del x, y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: cross validation\n",
    "Here we will cross validate our data to find the best set of parameters for `Lasso` regression. We will tune the regularization strength (`alpha`) and whether we fit an intercept to the data. \n",
    " - For each set of parameters find fit an estimator\n",
    " - Calculate the mean squared error on the test set\n",
    " - store the results (both the MSE and the parameters) so we can do analysis later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.1*i for i in range(1,10)]\n",
    "fit_intercepts = [True, False]\n",
    "\n",
    "# everything below to be removed\n",
    "# for all possible combinations, compute necessary metrics\n",
    "results = [[],[]]\n",
    "\n",
    "for a in alphas:\n",
    "    for f in fit_intercepts:\n",
    "        clf = Lasso(alpha=a, fit_intercept=f)      # 一类有约束的回规，通过损失函数可以让部分参数为0\n",
    "        clf.fit(x_train, y_train)\n",
    "        r2 = r2_score(y_test, clf.predict(x_test))\n",
    "        mse = mean_squared_error(y_test, clf.predict(x_test))\n",
    "        results[int(f)].append((a,r2,mse))\n",
    "        \n",
    "# put code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Analysis\n",
    " - for each value of `fit_intercept` plot the `MSE` and `r2` scores as a function of alpha. \n",
    " - What is the overall best combination of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "\n",
    "# your plotting code here\n",
    "for f in fit_intercepts:\n",
    "    alphas = [t[0] for t in results[int(f)]]\n",
    "    r2 = [t[1] for t in results[int(f)]]\n",
    "    mse = [t[2] for t in results[int(f)]]\n",
    "    axs[0, int(f)].plot(alphas, r2)\n",
    "    axs[0, int(f)].set_title('fit_intercept={}'.format(f))\n",
    "    axs[0, int(f)].set(xlabel='alphas', ylabel='r2')\n",
    "    axs[1, int(f)].plot(alphas, mse)\n",
    "    axs[1, int(f)].set(xlabel='alphas', ylabel='mse')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best combination is when alpha is small and we set `fit_intercept = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Linear regression in Keras (40%)\n",
    "\n",
    "#### We'd like to use keras to perform linear regression and compare it to another tool (scikit-learn)\n",
    "#### We'll compare OLS, ridge ($L2$ regularization) and LASSO ($L1$ regularization) using both keras and scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 1)\n"
     ]
    }
   ],
   "source": [
    "# Generate some data\n",
    "\n",
    "num_observations = 1024\n",
    "coefs = np.array([-1.2, 5, 0, .22, 2, 0, 4])  # notice, there are zeros!\n",
    "noise_amplitude = .05\n",
    "\n",
    "num_variables = coefs.shape[0]\n",
    "\n",
    "x = np.random.rand(num_observations, num_variables)\n",
    "y = (np.matmul(x, coefs) + (noise_amplitude * np.random.rand(1, num_observations))).T\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "# your code here, \n",
    "# split data into x_train, x_test, y_train, y_test\n",
    "train_size = int(num_observations*0.8)\n",
    "x_train, x_test = x[:train_size,:], x[train_size:,:]\n",
    "y_train, y_test = y[:train_size,], y[train_size:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape\n",
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:0.000196  r2_score:0.999944\n"
     ]
    }
   ],
   "source": [
    "# fit linear regression here and calculate MSE\n",
    "# put code here\n",
    "\n",
    "model = LinearRegression()\n",
    "coefs_reg = model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "R2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"MSE:%f  r2_score:%f\" % (MSE, R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real coefficients: [-1.2   5.    0.    0.22  2.    0.    4.  ]\n",
      "Predicted coefficients: [-1.19888194e+00  5.00173143e+00  1.37289279e-03  2.18767048e-01\n",
      "  1.99612945e+00  1.11601372e-03  4.00104707e+00]\n"
     ]
    }
   ],
   "source": [
    "# Show that the coefficients are all close the \"real\" ones used to generate the data\n",
    "# put code here\n",
    "coefs_reg = model.coef_[0]\n",
    "print(\"Real coefficients:\", coefs)\n",
    "print(\"Predicted coefficients:\", coefs_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras.layers as KL\n",
    "import keras.backend as K\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "def plot_model_in_notebook(model):\n",
    "    return SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will use keras to solve the same problem \n",
    "input_data = KL.Input(shape=x_train.shape)\n",
    "# add model definition here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"128pt\" viewBox=\"0.00 0.00 226.00 128.00\" width=\"226pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 124)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-124 222,-124 222,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2253851205528 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2253851205528</title>\n",
       "<polygon fill=\"none\" points=\"0,-0.5 0,-46.5 218,-46.5 218,-0.5 0,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"46\" y=\"-19.8\">Dense: Dense</text>\n",
       "<polyline fill=\"none\" points=\"92,-0.5 92,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"92,-23.5 148,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"148,-0.5 148,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183\" y=\"-31.3\">(None, 7)</text>\n",
       "<polyline fill=\"none\" points=\"148,-23.5 218,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183\" y=\"-8.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 2253851366064 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2253851366064</title>\n",
       "<polygon fill=\"none\" points=\"57,-83.5 57,-119.5 161,-119.5 161,-83.5 57,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"109\" y=\"-97.8\">2253851366064</text>\n",
       "</g>\n",
       "<!-- 2253851366064&#45;&gt;2253851205528 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2253851366064-&gt;2253851205528</title>\n",
       "<path d=\"M109,-83.2544C109,-75.3631 109,-65.7491 109,-56.6025\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"112.5,-56.5908 109,-46.5908 105.5,-56.5909 112.5,-56.5908\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "# build model here\n",
    "keras_lin_reg = Sequential()\n",
    "keras_lin_reg.add(Dense(1, input_dim=coefs.shape[0], name=\"Dense\"))\n",
    "# don't forget to compile model here\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "keras_lin_reg.compile(loss='mse', optimizer=sgd)\n",
    "\n",
    "# plot the model\n",
    "plot_model_in_notebook(keras_lin_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many parameters does the model have? \n",
    "### Explicitly show the calculation, explain it, and verify that it agrees with `model.count_params()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# ADD CODE HERE\n",
    "print(keras_lin_reg.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model and calculate the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 epochs, the loss is: 27.922493\n",
      "After 300 epochs, the loss is: 0.003135\n",
      "After 600 epochs, the loss is: 0.000272\n",
      "After 900 epochs, the loss is: 0.000208\n",
      "After 1200 epochs, the loss is: 0.000207\n",
      "(205, 1)\n",
      "MSE: 0.000188\n"
     ]
    }
   ],
   "source": [
    "# fit the model here\n",
    "\n",
    "train_epoch = 1500\n",
    "show_epoch = 300\n",
    "\n",
    "for epoch in range(train_epoch):\n",
    "    loss = keras_lin_reg.train_on_batch(x_train, y_train)\n",
    "    if epoch % show_epoch == 0:\n",
    "        print(\"After %d epochs, the loss is: %f\" % (epoch, loss))\n",
    "        \n",
    "# calculate mse\n",
    "y_pred = keras_lin_reg.predict(x_test)\n",
    "print(y_pred.shape)\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE: %f\" % MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the coefficients and compare them to the real coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real coefficients: [-1.2   5.    0.    0.22  2.    0.    4.  ]\n",
      "Predicted coefficients: [-1.2010928e+00  5.0014496e+00 -6.3487259e-04  2.2029732e-01\n",
      "  2.0003860e+00  1.5838676e-03  3.9992833e+00]\n"
     ]
    }
   ],
   "source": [
    "# add code here\n",
    "lin_reg_coefs, _ = keras_lin_reg.get_layer('Dense').get_weights()\n",
    "lin_reg_coefs = np.squeeze(lin_reg_coefs)\n",
    "print(\"Real coefficients:\", coefs)\n",
    "print(\"Predicted coefficients:\", lin_reg_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will add some regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1_l2\n",
    "regularizer = l1_l2(l1=0, l2=.1)\n",
    " # Dense(...) -> Dense(..., kernel_regularizer=regularizer)\n",
    "\n",
    "keras_ridge_model = Sequential()\n",
    "keras_ridge_model.add(Dense(1, input_dim=coefs.shape[0], kernel_regularizer=regularizer, name=\"Dense\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "keras_ridge_model.compile(loss='mse', optimizer=sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 epochs, the loss is: 26.748163\n",
      "After 300 epochs, the loss is: 2.183572\n",
      "After 600 epochs, the loss is: 2.183549\n",
      "After 900 epochs, the loss is: 2.183549\n",
      "After 1200 epochs, the loss is: 2.183549\n",
      "MSE: 1.231391\n"
     ]
    }
   ],
   "source": [
    "# fit the model and calculate the MSE\n",
    "\n",
    "train_epoch = 1500\n",
    "show_epoch = 300\n",
    "\n",
    "for epoch in range(train_epoch):\n",
    "    loss = keras_ridge_model.train_on_batch(x_train, y_train)\n",
    "    if epoch % show_epoch == 0:\n",
    "        print(\"After %d epochs, the loss is: %f\" % (epoch, loss))\n",
    "        \n",
    "\n",
    "y_pred = keras_ridge_model.predict(x_test)\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE: %f\" % MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keras ridge coefficients</th>\n",
       "      <th>real coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.201093</td>\n",
       "      <td>-1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.001450</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000635</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.220297</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.000386</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.999283</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keras ridge coefficients  real coefficients\n",
       "0                 -1.201093              -1.20\n",
       "1                  5.001450               5.00\n",
       "2                 -0.000635               0.00\n",
       "3                  0.220297               0.22\n",
       "4                  2.000386               2.00\n",
       "5                  0.001584               0.00\n",
       "6                  3.999283               4.00"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the coefficients and compare them to the real ones\n",
    "keras_ridge_coefs, _ = keras_lin_reg.get_layer('Dense').get_weights()\n",
    "keras_ridge_coefs = np.squeeze(keras_ridge_coefs)\n",
    "pd.Series(keras_ridge_coefs, name='keras ridge coefficients').to_frame().join(pd.Series(coefs, name='real coefficients'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ridge coefficients</th>\n",
       "      <th>real coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.449921</td>\n",
       "      <td>-1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.085208</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.066158</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.033747</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.805772</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.052516</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.743972</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ridge coefficients  real coefficients\n",
       "0           -0.449921              -1.20\n",
       "1            2.085208               5.00\n",
       "2            0.066158               0.00\n",
       "3           -0.033747               0.22\n",
       "4            0.805772               2.00\n",
       "5            0.052516               0.00\n",
       "6            1.743972               4.00"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=100)\n",
    "ridge.fit(x_train, y_train)\n",
    "pd.Series(np.squeeze(ridge.coef_), name='ridge coefficients').to_frame().join(pd.Series(coefs, name='real coefficients'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ridge coefs</th>\n",
       "      <th>keras L2 coefs</th>\n",
       "      <th>real coefs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.449921</td>\n",
       "      <td>-1.201093</td>\n",
       "      <td>-1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.085208</td>\n",
       "      <td>5.001450</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.066158</td>\n",
       "      <td>-0.000635</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.033747</td>\n",
       "      <td>0.220297</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.805772</td>\n",
       "      <td>2.000386</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.052516</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.743972</td>\n",
       "      <td>3.999283</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ridge coefs  keras L2 coefs  real coefs\n",
       "0    -0.449921       -1.201093       -1.20\n",
       "1     2.085208        5.001450        5.00\n",
       "2     0.066158       -0.000635        0.00\n",
       "3    -0.033747        0.220297        0.22\n",
       "4     0.805772        2.000386        2.00\n",
       "5     0.052516        0.001584        0.00\n",
       "6     1.743972        3.999283        4.00"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare coefficients from various methods\n",
    "pd.concat([\n",
    "    pd.Series(np.squeeze(ridge.coef_), name='ridge coefs'),\n",
    "    pd.Series(keras_ridge_coefs, name='keras L2 coefs'),\n",
    "    pd.Series(coefs, name='real coefs')\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In fact, given the zero coefficients, LASSO might have been a better model. \n",
    "## LASSO uses $L_{1}$ regularization which will make sparse coefficients (some are zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lasso coefficients</th>\n",
       "      <th>real coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.872086</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.768193</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.901648</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lasso coefficients  real coefficients\n",
       "0           -0.000000              -1.20\n",
       "1            3.872086               5.00\n",
       "2            0.000000               0.00\n",
       "3            0.000000               0.22\n",
       "4            0.768193               2.00\n",
       "5            0.000000               0.00\n",
       "6            2.901648               4.00"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.1, fit_intercept=True)      # constrained regression\n",
    "lasso.fit(x_train, y_train)\n",
    "\n",
    "lasso_coefs = lasso.coef_\n",
    "pd.Series(lasso_coefs, name='lasso coefficients').to_frame().join(pd.Series(coefs, name='real coefficients'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizer = l1_l2(l1=0.1, l2=0)  # to be removed\n",
    "\n",
    "keras_lasso_model = Sequential()\n",
    "keras_lasso_model.add(Dense(1, input_dim=coefs.shape[0], kernel_regularizer=regularizer, name=\"Dense\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "keras_lasso_model.compile(loss='mse', optimizer=sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 epochs, the loss is: 2.183549\n",
      "After 300 epochs, the loss is: 2.183549\n",
      "After 600 epochs, the loss is: 2.183549\n",
      "After 900 epochs, the loss is: 2.183549\n",
      "After 1200 epochs, the loss is: 2.183549\n"
     ]
    }
   ],
   "source": [
    "train_epoch = 1500\n",
    "show_epoch = 300\n",
    "\n",
    "for epoch in range(train_epoch):\n",
    "    loss = keras_ridge_model.train_on_batch(x_train, y_train)\n",
    "    if epoch % show_epoch == 0:\n",
    "        print(\"After %d epochs, the loss is: %f\" % (epoch, loss))\n",
    "        \n",
    "keras_lasso_coefs, _ = keras_lasso_model.get_layer('Dense').get_weights()\n",
    "keras_lasso_coefs = np.squeeze(keras_lasso_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ridge coefs</th>\n",
       "      <th>keras L2 coefs</th>\n",
       "      <th>lasso coefs</th>\n",
       "      <th>keras L1 coefs</th>\n",
       "      <th>ols coefs</th>\n",
       "      <th>real coefs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.449921</td>\n",
       "      <td>-1.201093</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.161360</td>\n",
       "      <td>-1.201093</td>\n",
       "      <td>-1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.085208</td>\n",
       "      <td>5.001450</td>\n",
       "      <td>3.872086</td>\n",
       "      <td>0.143780</td>\n",
       "      <td>5.001450</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.066158</td>\n",
       "      <td>-0.000635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623117</td>\n",
       "      <td>-0.000635</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.033747</td>\n",
       "      <td>0.220297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.535114</td>\n",
       "      <td>0.220297</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.805772</td>\n",
       "      <td>2.000386</td>\n",
       "      <td>0.768193</td>\n",
       "      <td>0.335389</td>\n",
       "      <td>2.000386</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.052516</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300296</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.743972</td>\n",
       "      <td>3.999283</td>\n",
       "      <td>2.901648</td>\n",
       "      <td>-0.252812</td>\n",
       "      <td>3.999283</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ridge coefs  keras L2 coefs  lasso coefs  keras L1 coefs  ols coefs  \\\n",
       "0    -0.449921       -1.201093    -0.000000       -0.161360  -1.201093   \n",
       "1     2.085208        5.001450     3.872086        0.143780   5.001450   \n",
       "2     0.066158       -0.000635     0.000000        0.623117  -0.000635   \n",
       "3    -0.033747        0.220297     0.000000        0.535114   0.220297   \n",
       "4     0.805772        2.000386     0.768193        0.335389   2.000386   \n",
       "5     0.052516        0.001584     0.000000       -0.300296   0.001584   \n",
       "6     1.743972        3.999283     2.901648       -0.252812   3.999283   \n",
       "\n",
       "   real coefs  \n",
       "0       -1.20  \n",
       "1        5.00  \n",
       "2        0.00  \n",
       "3        0.22  \n",
       "4        2.00  \n",
       "5        0.00  \n",
       "6        4.00  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare all the coefficients\n",
    "pd.concat([\n",
    "    pd.Series(np.squeeze(ridge.coef_), name='ridge coefs'),\n",
    "    pd.Series(keras_ridge_coefs, name='keras L2 coefs'),\n",
    "    pd.Series(lasso.coef_, name='lasso coefs'),\n",
    "    pd.Series(keras_lasso_coefs, name='keras L1 coefs'),\n",
    "    pd.Series(lin_reg_coefs, name='ols coefs'),\n",
    "    pd.Series(coefs, name='real coefs'),\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS coefficients are the closest to real coefficients.However,Lasso regression generally gets better result compared to OLS ridge regression when the coefficients are 0.The results from keras and scikit learn are different mainly because of divergent optimization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe to restart here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Custom Loss Functions (10%)\n",
    "### In this problem we will explore the use of custom loss function to better capture desired behavior in a model.\n",
    "### In particular, we will examine the tradeoff between false positives and true positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe to restart here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras.layers as KL\n",
    "import keras.backend as K\n",
    "from keras.datasets import mnist\n",
    "%pylab inline\n",
    "\n",
    "# many of these to be removed\n",
    "# Insert necessary imports here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Data preparation\n",
    " - ### Load the `mnist` data and subset the data only to the classes `4` and `9`\n",
    " - ### This way we have a binary classification task for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "def preprocess_training_data(data):\n",
    "    data = data.reshape(data.shape[0], data.shape[1] * data.shape[2])\n",
    "    data = data.astype('float32') / 255\n",
    "    return data\n",
    "\n",
    "def preprocess_targets(target, num_classes):\n",
    "    return to_categorical(target, num_classes)\n",
    "\n",
    "def subset_to_9_and_4(x, y):  # this is a new function\n",
    "    # insert code here:\n",
    "    new_x = np.full((x.shape),0.0)\n",
    "    new_y = np.full((y.shape),False)\n",
    "    for idx in range(len(x)):\n",
    "        if y[idx] == 9 or y[idx] == 4:\n",
    "            new_x[idx] = x[idx]\n",
    "        if y[idx] == 4:\n",
    "            new_y[idx] = True\n",
    "        else:\n",
    "            new_y[idx] = False\n",
    "    return new_x, new_y\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()  # x_train[60000:784], y_train[60000]\n",
    "\n",
    "x_train = preprocess_training_data(x_train)\n",
    "x_test = preprocess_training_data(x_test)\n",
    "\n",
    "num_classes = np.unique(y_train).shape[0]\n",
    "\n",
    "y_train_ohe = preprocess_targets(y_train, num_classes)\n",
    "y_test_ohe = preprocess_targets(y_test, num_classes)\n",
    "\n",
    "x_train, y_train = subset_to_9_and_4(x_train, y_train)\n",
    "x_test, y_test = subset_to_9_and_4(x_test, y_test)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: fit logistic regression in `scikit-learn` and compute the true positive and false positive rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.74 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# insert code here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model.predict_proba(x_test)\n",
    "y_pred = [True if y_pred[x][1]>y_pred[x][0] else False for x in range(len(y_pred))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive rate 0.004\n",
      "false negative rate 0.037\n"
     ]
    }
   ],
   "source": [
    "# put your code here\n",
    "total_predicted_pos = sum(y_pred)\n",
    "total_true_pos = sum(y_test)\n",
    "correct_predicted_pos = sum([1 if y_pred[x]==True and y_test[x]==True else 0 for x in range(len(y_test))])\n",
    "\n",
    "tpr =  correct_predicted_pos / total_true_pos\n",
    "fpr =  (total_predicted_pos - correct_predicted_pos)/ (len(y_test) - total_true_pos)\n",
    "fnr =  1 - tpr\n",
    "print('false positive rate {:.3f}'.format(fpr))\n",
    "print('false negative rate {:.3f}'.format(fnr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336\n",
      "575\n",
      "707\n",
      "740\n",
      "760\n",
      "874\n",
      "959\n",
      "969\n",
      "1192\n",
      "1232\n",
      "1242\n",
      "1357\n",
      "1413\n",
      "1422\n",
      "1429\n",
      "1440\n",
      "1634\n",
      "1901\n",
      "1963\n",
      "2043\n",
      "2053\n",
      "2129\n",
      "2161\n",
      "2232\n",
      "2414\n",
      "2514\n",
      "2548\n",
      "2720\n",
      "2771\n",
      "2863\n",
      "2877\n",
      "3066\n",
      "3288\n",
      "3490\n",
      "3533\n",
      "3718\n",
      "3758\n",
      "3798\n",
      "3821\n",
      "3869\n",
      "3970\n",
      "3985\n",
      "4000\n",
      "4017\n",
      "4093\n",
      "4154\n",
      "4174\n",
      "4265\n",
      "4405\n",
      "4425\n",
      "4444\n",
      "4567\n",
      "4575\n",
      "4721\n",
      "4735\n",
      "4823\n",
      "4893\n",
      "4910\n",
      "4929\n",
      "5926\n",
      "6568\n",
      "6817\n",
      "7081\n",
      "7089\n",
      "7434\n",
      "8406\n",
      "8426\n",
      "8520\n",
      "9170\n",
      "9587\n",
      "9764\n",
      "9792\n",
      "9808\n"
     ]
    }
   ],
   "source": [
    "for x in range(len(y_test)):\n",
    "    if y_test[x] != y_pred[x]:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: A custom loss function\n",
    "### Write a function that returns a custom loss function\n",
    "### As a shortcut, we can modify the loss function to be\n",
    "### $J(\\theta )$=`regular crossentropy `  $\\times$  $\\alpha$ \n",
    "\n",
    "### where \n",
    " - ### $\\alpha=$ 1 + `weight_factor` (a number greater than 0) for the positive class (`y=1`)\n",
    " - ### $\\alpha=$ `1` for the negative class (`y=0`)\n",
    "\n",
    "### Like this we can over weight how much we care about positive examples. \n",
    "\n",
    "# $ \\\\ $\n",
    "## Function\n",
    "### Inputs:\n",
    " - ### `weight_factor`: the multiplicative factor for how much to weight errors in the `1` class over `0`\n",
    "\n",
    "### Output:\n",
    " - ### a function that has inputs `(y_true, y_pred)` that is a viable keras loss function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_function(weight_factor):\n",
    "    def my_cross_entropy(y_true, y_pred):\n",
    "        y_pred = K.clip(y_pred, 1e-7, 1. - 1e-7)\n",
    "        data_size = y_pred.shape[0]\n",
    "        y_label = K.cast(y_true, dtype=\"float32\")\n",
    "        ce = -1 * K.sum(K.log(y_pred) * (y_label * (1.0+weight_factor)) + K.log(1.0 - y_pred) * (1 - y_label))\n",
    "        ce = ce / data_size\n",
    "        return ce\n",
    "    return my_cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Fit Models with Different `weight_factors`\n",
    "## Steps\n",
    " - ### For values in `np.linspace(0, 20, 50)` fit logistic regression in keras.\n",
    " - ### For each model compute the true positive and false positive rate\n",
    " - ### Make a plot of true positives and false positives as a function of `weight_factor`\n",
    " - ### Make a plot of true positives as a function of false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as KL\n",
    "import keras.backend as K\n",
    "from keras import optimizers, Model\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "[False False  True False False False False False False  True]\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train), type(y_train))\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-cd41183e1f60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0msgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_loss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;31m#                   loss_weight_2 * output_2_loss_fn(...) +\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;31m#                   layer losses.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_total_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;31m# Functions for train, test and predict will\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_prepare_total_loss\u001b[1;34m(self, masks)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                     output_loss = loss_fn(\n\u001b[1;32m--> 692\u001b[1;33m                         y_true, y_pred, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\losses.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mscope_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'lambda'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'<lambda>'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             return losses_utils.compute_weighted_loss(\n\u001b[0;32m     73\u001b[0m                 losses, sample_weight, reduction=self.reduction)\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\losses.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mLoss\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \"\"\"\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-24447df08490>\u001b[0m in \u001b[0;36mmy_cross_entropy\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0my_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_label\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mweight_factor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mce\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mce\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmy_cross_entropy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    904\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m           y = ops.convert_to_tensor_v2(\n\u001b[1;32m--> 906\u001b[1;33m               y, dtype_hint=x.dtype.base_dtype, name=\"y\")\n\u001b[0m\u001b[0;32m    907\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m           \u001b[1;31m# If the RHS is not a tensor, it might be a tensor aware object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1254\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1256\u001b[1;33m       as_ref=False)\n\u001b[0m\u001b[0;32m   1257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1314\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    315\u001b[0m                                          as_ref=False):\n\u001b[0;32m    316\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    256\u001b[0m   \"\"\"\n\u001b[0;32m    257\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[1;32m--> 258\u001b[1;33m                         allow_broadcast=True)\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    294\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m    295\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[0;32m    297\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m   const_tensor = g._create_op_internal(  # pylint: disable=protected-access\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    437\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"None values not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m     \u001b[1;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[1;31m# provided if possible.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": [
    "res = {}\n",
    "\n",
    "train_epoch = 1500\n",
    "show_epoch = 30\n",
    "\n",
    "for wt in np.linspace(0, 20, 50):\n",
    "    K.clear_session()\n",
    "    digit_input = KL.Input(shape=(x_train.shape[1],), name='digit_input')\n",
    "    \n",
    "    output = KL.Dense(1, name=\"Dense\")(digit_input) #, activation='sigmoid'\n",
    "    model = Model(inputs=[digit_input], outputs=[output])\n",
    "    \n",
    "    sgd = optimizers.SGD(lr=0.001)\n",
    "    model.compile(optimizer=sgd, loss=[get_loss_function(wt)])\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred = [True if y_pred[x] > 0.5 else False for x in range(len(y_pred))]\n",
    "    \n",
    "    total_predicted_pos = sum(y_pred)\n",
    "    total_true_pos = sum(y_test)\n",
    "    correct_predicted_pos = sum([1 if y_pred[x] and y_test[x] else 0 for x in range(len(y_test))])\n",
    "    \n",
    "    tpr =  correct_predicted_pos / total_true_pos\n",
    "    fpr =  (total_predicted_pos - correct_predicted_pos)/ (len(y_test) - total_true_pos)\n",
    "    \n",
    "    # print(total_predicted_pos, correct_predicted_pos)\n",
    "    # print(tpr, fpr)\n",
    "\n",
    "    # calculate metrics here and save them in `res`\n",
    "    res[wt] = [tpr, fpr]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# put plotting code here\n",
    "# true positive and false positives as a function of wt\n",
    "wt_list = []\n",
    "tpr_list, fpr_list = [], []\n",
    "for key, value in res.items():\n",
    "    wt_list.append(key)\n",
    "    tpr, fpr = value\n",
    "    tpr_list.append(tpr)\n",
    "    fpr_list.append(fpr)\n",
    "\n",
    "plt.plot(wt_list, tpr_list, label=\"tpr\", color=\"r\") \n",
    "plt.plot(wt_list, fpr_list, label=\"fpr\", color=\"b\") \n",
    "plt.xlabel('wt value') \n",
    "plt.ylabel('tpr / fpr') \n",
    "plt.legend() \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# true positives as a function of false positives\n",
    "plt.plot(fpr_list, tpr_list, label=\"fpr\", color=\"b\") \n",
    "plt.xlabel('fpr') \n",
    "plt.ylabel('tpr') \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Thresholding vs Loss Function\n",
    "### What is the difference between using regular crossentropy and changing thresholds vs changing the loss function? \n",
    "### Typically, we consider anything with a predicted probability of 0.5 or above to be a positive example. What if we change this number?\n",
    "\n",
    "## Steps:\n",
    " - ### Fit logistic regression in keras with the regular (crossentropy) loss function\n",
    " - ### Find the false positive and true positive rates as a function of the `threshold` for which we consider a positive example. \n",
    " - ### Make a plot of false positives as a function of true positives for this thresholding method.\n",
    " - ### On the same axes, plot the old curve of false positives as a function of true positives\n",
    " - ### Comment on what you find- what are the advantages and disadvantages of these two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_true, y_pred):\n",
    "    y_pred = K.clip(y_pred, 1e-7, 1. - 1e-7)\n",
    "    data_size = y_pred.shape[0]\n",
    "    y_label = K.cast(y_true, dtype=\"float32\")\n",
    "    ce = -1 * K.sum(K.log(y_pred) * y_label + K.log(1.0 - y_pred) * (1 - y_label))\n",
    "    ce = ce / data_size\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 1s 395us/step - loss: 0.2497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e55f2c8af0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "digit_input = KL.Input(shape=(x_train.shape[1],), name='digit_input')\n",
    "# your code goes here\n",
    "\n",
    "output = KL.Dense(1, name=\"Dense\", activation='sigmoid')(digit_input)\n",
    "model = Model(inputs=[digit_input], outputs=[output])\n",
    "    \n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd, loss=cross_entropy)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0739533 ]\n",
      " [0.0739533 ]\n",
      " [0.0739533 ]\n",
      " [0.0739533 ]\n",
      " [0.5979102 ]\n",
      " [0.0739533 ]\n",
      " [0.63975406]\n",
      " [0.2939192 ]\n",
      " [0.0739533 ]\n",
      " [0.28111202]]\n"
     ]
    }
   ],
   "source": [
    "pred_proba = model.predict(x_test)\n",
    "print(pred_proba[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "threshold_list, fpr_list, tpr_list = [], [], []\n",
    "\n",
    "for threshold in np.linspace(0.4, 0.8, 20):\n",
    "    y_pred = [True if pred_proba[i] > threshold else False for i in range(len(pred_proba))]\n",
    "    \n",
    "    total_predicted_pos = sum(y_pred)\n",
    "    total_true_pos = sum(y_test)\n",
    "    correct_predicted_pos = sum([1 if y_pred[x] and y_test[x] else 0 for x in range(len(y_test))])\n",
    "    \n",
    "    tpr =  correct_predicted_pos / total_true_pos\n",
    "    fpr =  (total_predicted_pos - correct_predicted_pos)/ (len(y_test) - total_true_pos)\n",
    "    \n",
    "    threshold_list.append(threshold)\n",
    "    fpr_list.append(fpr)\n",
    "    tpr_list.append(tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(threshold_list, tpr_list, label=\"tpr\", color=\"r\") \n",
    "plt.plot(threshold_list, fpr_list, label=\"fpr\", color=\"b\") \n",
    "plt.xlabel('threshold') \n",
    "plt.ylabel('tpr / fpr') \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a plot of false positives as a function of true positives for this thresholding method.\n",
    "plt.plot(tpr_list, fpr_list, label=\"tpr\", color=\"r\") \n",
    "plt.xlabel('tpr') \n",
    "plt.ylabel('fpr') \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments here \n",
    "# Increasing threshold would decrease tpr and fpr accordingly, decreasing threshold would decrease tnr and fnr accordingly. In ideal conditions, TPR would be close to 1 and FPR would be close to zero. Every point on ROC corresponds to a threshold, and every threshold would have one TPR and FPR. \n",
    "# When threshold reaches maximum, TP=FP=0, which is the original point; when threshold reaches minimum, TN=FN=0, which is the point(1,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: Custom Regularization (15%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "\n",
    "# many of these to be removed\n",
    "# necessary imports here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "n_points = 2048\n",
    "\n",
    "x = np.vstack([np.random.randn(n_points) + ii for ii in np.arange(-3, 3)]).T\n",
    "real_coefs = np.array([1.4, 0.9, .1, -3, 2.2, -1.3])\n",
    "y = x.dot(real_coefs) + .5 * np.random.randn(n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 6)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'counts')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(y).hist(bins=30, figsize=(12,8))\n",
    "plt.xlabel('y', fontsize=24)\n",
    "plt.ylabel('counts', fontsize=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A new regularization\n",
    "Sometimes, in the social science and in low-data situations a good rule of thumb is that instead of doing linear regression to find the coefficients, we can assume that all the coefficients are the same!\n",
    "\n",
    "We will implement a variation on this by penalizing the `variance` of the coefficients, instead of the size.\n",
    "\n",
    "__NOTE__: this is not a good thing to do in this situation- we have coefficients of different sign!\n",
    "\n",
    "In this problem we will first solve linear regression with keras. Then we will add a custom regularizer to penalize the variance, and see what the effect are. \n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## Part 0: Solve the problem with no regularization\n",
    "### Steps:\n",
    " - ### Partition the data into train/test\n",
    " - ### Fit linear regression in keras with no regularization\n",
    " - ### Find the `mse` for the test set this model\n",
    " - ### Examine the coefficients and make sure they look correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the data\n",
    "ratio = 0.8\n",
    "y = np.array([y]).T\n",
    "x_train = x[:int(x.shape[0]*ratio),:]\n",
    "y_train = y[:int(y.shape[0]*ratio),:]\n",
    "x_test = x[int(x.shape[0]*ratio):,:]\n",
    "y_test = y[int(y.shape[0]*ratio):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "11/11 - 0s - loss: 59.7695 - mse: 59.7695\n",
      "Epoch 2/1000\n",
      "11/11 - 0s - loss: 58.3714 - mse: 58.3714\n",
      "Epoch 3/1000\n",
      "11/11 - 0s - loss: 57.0080 - mse: 57.0080\n",
      "Epoch 4/1000\n",
      "11/11 - 0s - loss: 55.6782 - mse: 55.6782\n",
      "Epoch 5/1000\n",
      "11/11 - 0s - loss: 54.3831 - mse: 54.3831\n",
      "Epoch 6/1000\n",
      "11/11 - 0s - loss: 53.1228 - mse: 53.1228\n",
      "Epoch 7/1000\n",
      "11/11 - 0s - loss: 51.8973 - mse: 51.8973\n",
      "Epoch 8/1000\n",
      "11/11 - 0s - loss: 50.7059 - mse: 50.7059\n",
      "Epoch 9/1000\n",
      "11/11 - 0s - loss: 49.5483 - mse: 49.5483\n",
      "Epoch 10/1000\n",
      "11/11 - 0s - loss: 48.4238 - mse: 48.4238\n",
      "Epoch 11/1000\n",
      "11/11 - 0s - loss: 47.3318 - mse: 47.3318\n",
      "Epoch 12/1000\n",
      "11/11 - 0s - loss: 46.2716 - mse: 46.2716\n",
      "Epoch 13/1000\n",
      "11/11 - 0s - loss: 45.2427 - mse: 45.2427\n",
      "Epoch 14/1000\n",
      "11/11 - 0s - loss: 44.2443 - mse: 44.2443\n",
      "Epoch 15/1000\n",
      "11/11 - 0s - loss: 43.2758 - mse: 43.2758\n",
      "Epoch 16/1000\n",
      "11/11 - 0s - loss: 42.3366 - mse: 42.3366\n",
      "Epoch 17/1000\n",
      "11/11 - 0s - loss: 41.4260 - mse: 41.4260\n",
      "Epoch 18/1000\n",
      "11/11 - 0s - loss: 40.5434 - mse: 40.5434\n",
      "Epoch 19/1000\n",
      "11/11 - 0s - loss: 39.6882 - mse: 39.6882\n",
      "Epoch 20/1000\n",
      "11/11 - 0s - loss: 38.8596 - mse: 38.8596\n",
      "Epoch 21/1000\n",
      "11/11 - 0s - loss: 38.0571 - mse: 38.0571\n",
      "Epoch 22/1000\n",
      "11/11 - 0s - loss: 37.2800 - mse: 37.2800\n",
      "Epoch 23/1000\n",
      "11/11 - 0s - loss: 36.5277 - mse: 36.5277\n",
      "Epoch 24/1000\n",
      "11/11 - 0s - loss: 35.7996 - mse: 35.7996\n",
      "Epoch 25/1000\n",
      "11/11 - 0s - loss: 35.0949 - mse: 35.0949\n",
      "Epoch 26/1000\n",
      "11/11 - 0s - loss: 34.4132 - mse: 34.4132\n",
      "Epoch 27/1000\n",
      "11/11 - 0s - loss: 33.7537 - mse: 33.7537\n",
      "Epoch 28/1000\n",
      "11/11 - 0s - loss: 33.1159 - mse: 33.1159\n",
      "Epoch 29/1000\n",
      "11/11 - 0s - loss: 32.4991 - mse: 32.4991\n",
      "Epoch 30/1000\n",
      "11/11 - 0s - loss: 31.9027 - mse: 31.9027\n",
      "Epoch 31/1000\n",
      "11/11 - 0s - loss: 31.3261 - mse: 31.3261\n",
      "Epoch 32/1000\n",
      "11/11 - 0s - loss: 30.7688 - mse: 30.7688\n",
      "Epoch 33/1000\n",
      "11/11 - 0s - loss: 30.2301 - mse: 30.2301\n",
      "Epoch 34/1000\n",
      "11/11 - 0s - loss: 29.7094 - mse: 29.7094\n",
      "Epoch 35/1000\n",
      "11/11 - 0s - loss: 29.2061 - mse: 29.2061\n",
      "Epoch 36/1000\n",
      "11/11 - 0s - loss: 28.7198 - mse: 28.7198\n",
      "Epoch 37/1000\n",
      "11/11 - 0s - loss: 28.2497 - mse: 28.2497\n",
      "Epoch 38/1000\n",
      "11/11 - 0s - loss: 27.7954 - mse: 27.7954\n",
      "Epoch 39/1000\n",
      "11/11 - 0s - loss: 27.3563 - mse: 27.3563\n",
      "Epoch 40/1000\n",
      "11/11 - 0s - loss: 26.9318 - mse: 26.9318\n",
      "Epoch 41/1000\n",
      "11/11 - 0s - loss: 26.5214 - mse: 26.5214\n",
      "Epoch 42/1000\n",
      "11/11 - 0s - loss: 26.1247 - mse: 26.1247\n",
      "Epoch 43/1000\n",
      "11/11 - 0s - loss: 25.7410 - mse: 25.7410\n",
      "Epoch 44/1000\n",
      "11/11 - 0s - loss: 25.3699 - mse: 25.3699\n",
      "Epoch 45/1000\n",
      "11/11 - 0s - loss: 25.0109 - mse: 25.0109\n",
      "Epoch 46/1000\n",
      "11/11 - 0s - loss: 24.6635 - mse: 24.6635\n",
      "Epoch 47/1000\n",
      "11/11 - 0s - loss: 24.3272 - mse: 24.3272\n",
      "Epoch 48/1000\n",
      "11/11 - 0s - loss: 24.0016 - mse: 24.0016\n",
      "Epoch 49/1000\n",
      "11/11 - 0s - loss: 23.6862 - mse: 23.6862\n",
      "Epoch 50/1000\n",
      "11/11 - 0s - loss: 23.3806 - mse: 23.3806\n",
      "Epoch 51/1000\n",
      "11/11 - 0s - loss: 23.0844 - mse: 23.0844\n",
      "Epoch 52/1000\n",
      "11/11 - 0s - loss: 22.7971 - mse: 22.7971\n",
      "Epoch 53/1000\n",
      "11/11 - 0s - loss: 22.5184 - mse: 22.5184\n",
      "Epoch 54/1000\n",
      "11/11 - 0s - loss: 22.2479 - mse: 22.2479\n",
      "Epoch 55/1000\n",
      "11/11 - 0s - loss: 21.9852 - mse: 21.9852\n",
      "Epoch 56/1000\n",
      "11/11 - 0s - loss: 21.7300 - mse: 21.7300\n",
      "Epoch 57/1000\n",
      "11/11 - 0s - loss: 21.4818 - mse: 21.4818\n",
      "Epoch 58/1000\n",
      "11/11 - 0s - loss: 21.2405 - mse: 21.2405\n",
      "Epoch 59/1000\n",
      "11/11 - 0s - loss: 21.0056 - mse: 21.0056\n",
      "Epoch 60/1000\n",
      "11/11 - 0s - loss: 20.7769 - mse: 20.7769\n",
      "Epoch 61/1000\n",
      "11/11 - 0s - loss: 20.5541 - mse: 20.5541\n",
      "Epoch 62/1000\n",
      "11/11 - 0s - loss: 20.3368 - mse: 20.3368\n",
      "Epoch 63/1000\n",
      "11/11 - 0s - loss: 20.1250 - mse: 20.1250\n",
      "Epoch 64/1000\n",
      "11/11 - 0s - loss: 19.9181 - mse: 19.9181\n",
      "Epoch 65/1000\n",
      "11/11 - 0s - loss: 19.7162 - mse: 19.7162\n",
      "Epoch 66/1000\n",
      "11/11 - 0s - loss: 19.5188 - mse: 19.5188\n",
      "Epoch 67/1000\n",
      "11/11 - 0s - loss: 19.3258 - mse: 19.3258\n",
      "Epoch 68/1000\n",
      "11/11 - 0s - loss: 19.1371 - mse: 19.1371\n",
      "Epoch 69/1000\n",
      "11/11 - 0s - loss: 18.9523 - mse: 18.9523\n",
      "Epoch 70/1000\n",
      "11/11 - 0s - loss: 18.7713 - mse: 18.7713\n",
      "Epoch 71/1000\n",
      "11/11 - 0s - loss: 18.5940 - mse: 18.5940\n",
      "Epoch 72/1000\n",
      "11/11 - 0s - loss: 18.4201 - mse: 18.4201\n",
      "Epoch 73/1000\n",
      "11/11 - 0s - loss: 18.2496 - mse: 18.2496\n",
      "Epoch 74/1000\n",
      "11/11 - 0s - loss: 18.0822 - mse: 18.0822\n",
      "Epoch 75/1000\n",
      "11/11 - 0s - loss: 17.9178 - mse: 17.9178\n",
      "Epoch 76/1000\n",
      "11/11 - 0s - loss: 17.7563 - mse: 17.7563\n",
      "Epoch 77/1000\n",
      "11/11 - 0s - loss: 17.5976 - mse: 17.5976\n",
      "Epoch 78/1000\n",
      "11/11 - 0s - loss: 17.4415 - mse: 17.4415\n",
      "Epoch 79/1000\n",
      "11/11 - 0s - loss: 17.2880 - mse: 17.2880\n",
      "Epoch 80/1000\n",
      "11/11 - 0s - loss: 17.1368 - mse: 17.1368\n",
      "Epoch 81/1000\n",
      "11/11 - 0s - loss: 16.9880 - mse: 16.9880\n",
      "Epoch 82/1000\n",
      "11/11 - 0s - loss: 16.8415 - mse: 16.8415\n",
      "Epoch 83/1000\n",
      "11/11 - 0s - loss: 16.6971 - mse: 16.6971\n",
      "Epoch 84/1000\n",
      "11/11 - 0s - loss: 16.5547 - mse: 16.5547\n",
      "Epoch 85/1000\n",
      "11/11 - 0s - loss: 16.4143 - mse: 16.4143\n",
      "Epoch 86/1000\n",
      "11/11 - 0s - loss: 16.2759 - mse: 16.2759\n",
      "Epoch 87/1000\n",
      "11/11 - 0s - loss: 16.1392 - mse: 16.1392\n",
      "Epoch 88/1000\n",
      "11/11 - 0s - loss: 16.0044 - mse: 16.0044\n",
      "Epoch 89/1000\n",
      "11/11 - 0s - loss: 15.8712 - mse: 15.8712\n",
      "Epoch 90/1000\n",
      "11/11 - 0s - loss: 15.7397 - mse: 15.7397\n",
      "Epoch 91/1000\n",
      "11/11 - 0s - loss: 15.6098 - mse: 15.6098\n",
      "Epoch 92/1000\n",
      "11/11 - 0s - loss: 15.4815 - mse: 15.4815\n",
      "Epoch 93/1000\n",
      "11/11 - 0s - loss: 15.3546 - mse: 15.3546\n",
      "Epoch 94/1000\n",
      "11/11 - 0s - loss: 15.2292 - mse: 15.2292\n",
      "Epoch 95/1000\n",
      "11/11 - 0s - loss: 15.1051 - mse: 15.1051\n",
      "Epoch 96/1000\n",
      "11/11 - 0s - loss: 14.9824 - mse: 14.9824\n",
      "Epoch 97/1000\n",
      "11/11 - 0s - loss: 14.8611 - mse: 14.8611\n",
      "Epoch 98/1000\n",
      "11/11 - 0s - loss: 14.7410 - mse: 14.7410\n",
      "Epoch 99/1000\n",
      "11/11 - 0s - loss: 14.6221 - mse: 14.6221\n",
      "Epoch 100/1000\n",
      "11/11 - 0s - loss: 14.5045 - mse: 14.5045\n",
      "Epoch 101/1000\n",
      "11/11 - 0s - loss: 14.3880 - mse: 14.3880\n",
      "Epoch 102/1000\n",
      "11/11 - 0s - loss: 14.2727 - mse: 14.2727\n",
      "Epoch 103/1000\n",
      "11/11 - 0s - loss: 14.1585 - mse: 14.1585\n",
      "Epoch 104/1000\n",
      "11/11 - 0s - loss: 14.0453 - mse: 14.0453\n",
      "Epoch 105/1000\n",
      "11/11 - 0s - loss: 13.9332 - mse: 13.9332\n",
      "Epoch 106/1000\n",
      "11/11 - 0s - loss: 13.8222 - mse: 13.8222\n",
      "Epoch 107/1000\n",
      "11/11 - 0s - loss: 13.7121 - mse: 13.7121\n",
      "Epoch 108/1000\n",
      "11/11 - 0s - loss: 13.6031 - mse: 13.6031\n",
      "Epoch 109/1000\n",
      "11/11 - 0s - loss: 13.4949 - mse: 13.4949\n",
      "Epoch 110/1000\n",
      "11/11 - 0s - loss: 13.3878 - mse: 13.3878\n",
      "Epoch 111/1000\n",
      "11/11 - 0s - loss: 13.2815 - mse: 13.2815\n",
      "Epoch 112/1000\n",
      "11/11 - 0s - loss: 13.1762 - mse: 13.1762\n",
      "Epoch 113/1000\n",
      "11/11 - 0s - loss: 13.0717 - mse: 13.0717\n",
      "Epoch 114/1000\n",
      "11/11 - 0s - loss: 12.9681 - mse: 12.9681\n",
      "Epoch 115/1000\n",
      "11/11 - 0s - loss: 12.8654 - mse: 12.8654\n",
      "Epoch 116/1000\n",
      "11/11 - 0s - loss: 12.7634 - mse: 12.7634\n",
      "Epoch 117/1000\n",
      "11/11 - 0s - loss: 12.6623 - mse: 12.6623\n",
      "Epoch 118/1000\n",
      "11/11 - 0s - loss: 12.5620 - mse: 12.5620\n",
      "Epoch 119/1000\n",
      "11/11 - 0s - loss: 12.4625 - mse: 12.4625\n",
      "Epoch 120/1000\n",
      "11/11 - 0s - loss: 12.3638 - mse: 12.3638\n",
      "Epoch 121/1000\n",
      "11/11 - 0s - loss: 12.2658 - mse: 12.2658\n",
      "Epoch 122/1000\n",
      "11/11 - 0s - loss: 12.1685 - mse: 12.1685\n",
      "Epoch 123/1000\n",
      "11/11 - 0s - loss: 12.0720 - mse: 12.0720\n",
      "Epoch 124/1000\n",
      "11/11 - 0s - loss: 11.9763 - mse: 11.9763\n",
      "Epoch 125/1000\n",
      "11/11 - 0s - loss: 11.8812 - mse: 11.8812\n",
      "Epoch 126/1000\n",
      "11/11 - 0s - loss: 11.7868 - mse: 11.7868\n",
      "Epoch 127/1000\n",
      "11/11 - 0s - loss: 11.6932 - mse: 11.6932\n",
      "Epoch 128/1000\n",
      "11/11 - 0s - loss: 11.6002 - mse: 11.6002\n",
      "Epoch 129/1000\n",
      "11/11 - 0s - loss: 11.5079 - mse: 11.5079\n",
      "Epoch 130/1000\n",
      "11/11 - 0s - loss: 11.4162 - mse: 11.4162\n",
      "Epoch 131/1000\n",
      "11/11 - 0s - loss: 11.3253 - mse: 11.3253\n",
      "Epoch 132/1000\n",
      "11/11 - 0s - loss: 11.2349 - mse: 11.2349\n",
      "Epoch 133/1000\n",
      "11/11 - 0s - loss: 11.1452 - mse: 11.1452\n",
      "Epoch 134/1000\n",
      "11/11 - 0s - loss: 11.0561 - mse: 11.0561\n",
      "Epoch 135/1000\n",
      "11/11 - 0s - loss: 10.9677 - mse: 10.9677\n",
      "Epoch 136/1000\n",
      "11/11 - 0s - loss: 10.8799 - mse: 10.8799\n",
      "Epoch 137/1000\n",
      "11/11 - 0s - loss: 10.7926 - mse: 10.7926\n",
      "Epoch 138/1000\n",
      "11/11 - 0s - loss: 10.7060 - mse: 10.7060\n",
      "Epoch 139/1000\n",
      "11/11 - 0s - loss: 10.6200 - mse: 10.6200\n",
      "Epoch 140/1000\n",
      "11/11 - 0s - loss: 10.5345 - mse: 10.5345\n",
      "Epoch 141/1000\n",
      "11/11 - 0s - loss: 10.4497 - mse: 10.4497\n",
      "Epoch 142/1000\n",
      "11/11 - 0s - loss: 10.3654 - mse: 10.3654\n",
      "Epoch 143/1000\n",
      "11/11 - 0s - loss: 10.2817 - mse: 10.2817\n",
      "Epoch 144/1000\n",
      "11/11 - 0s - loss: 10.1985 - mse: 10.1985\n",
      "Epoch 145/1000\n",
      "11/11 - 0s - loss: 10.1160 - mse: 10.1160\n",
      "Epoch 146/1000\n",
      "11/11 - 0s - loss: 10.0339 - mse: 10.0339\n",
      "Epoch 147/1000\n",
      "11/11 - 0s - loss: 9.9524 - mse: 9.9524\n",
      "Epoch 148/1000\n",
      "11/11 - 0s - loss: 9.8715 - mse: 9.8715\n",
      "Epoch 149/1000\n",
      "11/11 - 0s - loss: 9.7911 - mse: 9.7911\n",
      "Epoch 150/1000\n",
      "11/11 - 0s - loss: 9.7112 - mse: 9.7112\n",
      "Epoch 151/1000\n",
      "11/11 - 0s - loss: 9.6319 - mse: 9.6319\n",
      "Epoch 152/1000\n",
      "11/11 - 0s - loss: 9.5531 - mse: 9.5531\n",
      "Epoch 153/1000\n",
      "11/11 - 0s - loss: 9.4748 - mse: 9.4748\n",
      "Epoch 154/1000\n",
      "11/11 - 0s - loss: 9.3970 - mse: 9.3970\n",
      "Epoch 155/1000\n",
      "11/11 - 0s - loss: 9.3197 - mse: 9.3197\n",
      "Epoch 156/1000\n",
      "11/11 - 0s - loss: 9.2430 - mse: 9.2430\n",
      "Epoch 157/1000\n",
      "11/11 - 0s - loss: 9.1667 - mse: 9.1667\n",
      "Epoch 158/1000\n",
      "11/11 - 0s - loss: 9.0910 - mse: 9.0910\n",
      "Epoch 159/1000\n",
      "11/11 - 0s - loss: 9.0157 - mse: 9.0157\n",
      "Epoch 160/1000\n",
      "11/11 - 0s - loss: 8.9409 - mse: 8.9409\n",
      "Epoch 161/1000\n",
      "11/11 - 0s - loss: 8.8667 - mse: 8.8667\n",
      "Epoch 162/1000\n",
      "11/11 - 0s - loss: 8.7929 - mse: 8.7929\n",
      "Epoch 163/1000\n",
      "11/11 - 0s - loss: 8.7195 - mse: 8.7195\n",
      "Epoch 164/1000\n",
      "11/11 - 0s - loss: 8.6467 - mse: 8.6467\n",
      "Epoch 165/1000\n",
      "11/11 - 0s - loss: 8.5743 - mse: 8.5743\n",
      "Epoch 166/1000\n",
      "11/11 - 0s - loss: 8.5024 - mse: 8.5024\n",
      "Epoch 167/1000\n",
      "11/11 - 0s - loss: 8.4310 - mse: 8.4310\n",
      "Epoch 168/1000\n",
      "11/11 - 0s - loss: 8.3600 - mse: 8.3600\n",
      "Epoch 169/1000\n",
      "11/11 - 0s - loss: 8.2895 - mse: 8.2895\n",
      "Epoch 170/1000\n",
      "11/11 - 0s - loss: 8.2195 - mse: 8.2195\n",
      "Epoch 171/1000\n",
      "11/11 - 0s - loss: 8.1499 - mse: 8.1499\n",
      "Epoch 172/1000\n",
      "11/11 - 0s - loss: 8.0808 - mse: 8.0808\n",
      "Epoch 173/1000\n",
      "11/11 - 0s - loss: 8.0121 - mse: 8.0121\n",
      "Epoch 174/1000\n",
      "11/11 - 0s - loss: 7.9438 - mse: 7.9438\n",
      "Epoch 175/1000\n",
      "11/11 - 0s - loss: 7.8760 - mse: 7.8760\n",
      "Epoch 176/1000\n",
      "11/11 - 0s - loss: 7.8087 - mse: 7.8087\n",
      "Epoch 177/1000\n",
      "11/11 - 0s - loss: 7.7418 - mse: 7.7418\n",
      "Epoch 178/1000\n",
      "11/11 - 0s - loss: 7.6753 - mse: 7.6753\n",
      "Epoch 179/1000\n",
      "11/11 - 0s - loss: 7.6092 - mse: 7.6092\n",
      "Epoch 180/1000\n",
      "11/11 - 0s - loss: 7.5436 - mse: 7.5436\n",
      "Epoch 181/1000\n",
      "11/11 - 0s - loss: 7.4784 - mse: 7.4784\n",
      "Epoch 182/1000\n",
      "11/11 - 0s - loss: 7.4136 - mse: 7.4136\n",
      "Epoch 183/1000\n",
      "11/11 - 0s - loss: 7.3493 - mse: 7.3493\n",
      "Epoch 184/1000\n",
      "11/11 - 0s - loss: 7.2854 - mse: 7.2854\n",
      "Epoch 185/1000\n",
      "11/11 - 0s - loss: 7.2219 - mse: 7.2219\n",
      "Epoch 186/1000\n",
      "11/11 - 0s - loss: 7.1588 - mse: 7.1588\n",
      "Epoch 187/1000\n",
      "11/11 - 0s - loss: 7.0961 - mse: 7.0961\n",
      "Epoch 188/1000\n",
      "11/11 - 0s - loss: 7.0338 - mse: 7.0338\n",
      "Epoch 189/1000\n",
      "11/11 - 0s - loss: 6.9720 - mse: 6.9720\n",
      "Epoch 190/1000\n",
      "11/11 - 0s - loss: 6.9105 - mse: 6.9105\n",
      "Epoch 191/1000\n",
      "11/11 - 0s - loss: 6.8495 - mse: 6.8495\n",
      "Epoch 192/1000\n",
      "11/11 - 0s - loss: 6.7889 - mse: 6.7889\n",
      "Epoch 193/1000\n",
      "11/11 - 0s - loss: 6.7286 - mse: 6.7286\n",
      "Epoch 194/1000\n",
      "11/11 - 0s - loss: 6.6688 - mse: 6.6688\n",
      "Epoch 195/1000\n",
      "11/11 - 0s - loss: 6.6093 - mse: 6.6093\n",
      "Epoch 196/1000\n",
      "11/11 - 0s - loss: 6.5503 - mse: 6.5503\n",
      "Epoch 197/1000\n",
      "11/11 - 0s - loss: 6.4916 - mse: 6.4916\n",
      "Epoch 198/1000\n",
      "11/11 - 0s - loss: 6.4334 - mse: 6.4334\n",
      "Epoch 199/1000\n",
      "11/11 - 0s - loss: 6.3755 - mse: 6.3755\n",
      "Epoch 200/1000\n",
      "11/11 - 0s - loss: 6.3180 - mse: 6.3180\n",
      "Epoch 201/1000\n",
      "11/11 - 0s - loss: 6.2609 - mse: 6.2609\n",
      "Epoch 202/1000\n",
      "11/11 - 0s - loss: 6.2042 - mse: 6.2042\n",
      "Epoch 203/1000\n",
      "11/11 - 0s - loss: 6.1479 - mse: 6.1479\n",
      "Epoch 204/1000\n",
      "11/11 - 0s - loss: 6.0919 - mse: 6.0919\n",
      "Epoch 205/1000\n",
      "11/11 - 0s - loss: 6.0363 - mse: 6.0363\n",
      "Epoch 206/1000\n",
      "11/11 - 0s - loss: 5.9811 - mse: 5.9811\n",
      "Epoch 207/1000\n",
      "11/11 - 0s - loss: 5.9263 - mse: 5.9263\n",
      "Epoch 208/1000\n",
      "11/11 - 0s - loss: 5.8718 - mse: 5.8718\n",
      "Epoch 209/1000\n",
      "11/11 - 0s - loss: 5.8177 - mse: 5.8177\n",
      "Epoch 210/1000\n",
      "11/11 - 0s - loss: 5.7640 - mse: 5.7640\n",
      "Epoch 211/1000\n",
      "11/11 - 0s - loss: 5.7107 - mse: 5.7107\n",
      "Epoch 212/1000\n",
      "11/11 - 0s - loss: 5.6577 - mse: 5.6577\n",
      "Epoch 213/1000\n",
      "11/11 - 0s - loss: 5.6051 - mse: 5.6051\n",
      "Epoch 214/1000\n",
      "11/11 - 0s - loss: 5.5528 - mse: 5.5528\n",
      "Epoch 215/1000\n",
      "11/11 - 0s - loss: 5.5009 - mse: 5.5009\n",
      "Epoch 216/1000\n",
      "11/11 - 0s - loss: 5.4494 - mse: 5.4494\n",
      "Epoch 217/1000\n",
      "11/11 - 0s - loss: 5.3982 - mse: 5.3982\n",
      "Epoch 218/1000\n",
      "11/11 - 0s - loss: 5.3474 - mse: 5.3474\n",
      "Epoch 219/1000\n",
      "11/11 - 0s - loss: 5.2969 - mse: 5.2969\n",
      "Epoch 220/1000\n",
      "11/11 - 0s - loss: 5.2468 - mse: 5.2468\n",
      "Epoch 221/1000\n",
      "11/11 - 0s - loss: 5.1970 - mse: 5.1970\n",
      "Epoch 222/1000\n",
      "11/11 - 0s - loss: 5.1476 - mse: 5.1476\n",
      "Epoch 223/1000\n",
      "11/11 - 0s - loss: 5.0986 - mse: 5.0986\n",
      "Epoch 224/1000\n",
      "11/11 - 0s - loss: 5.0498 - mse: 5.0498\n",
      "Epoch 225/1000\n",
      "11/11 - 0s - loss: 5.0015 - mse: 5.0015\n",
      "Epoch 226/1000\n",
      "11/11 - 0s - loss: 4.9534 - mse: 4.9534\n",
      "Epoch 227/1000\n",
      "11/11 - 0s - loss: 4.9057 - mse: 4.9057\n",
      "Epoch 228/1000\n",
      "11/11 - 0s - loss: 4.8584 - mse: 4.8584\n",
      "Epoch 229/1000\n",
      "11/11 - 0s - loss: 4.8114 - mse: 4.8114\n",
      "Epoch 230/1000\n",
      "11/11 - 0s - loss: 4.7647 - mse: 4.7647\n",
      "Epoch 231/1000\n",
      "11/11 - 0s - loss: 4.7184 - mse: 4.7184\n",
      "Epoch 232/1000\n",
      "11/11 - 0s - loss: 4.6724 - mse: 4.6724\n",
      "Epoch 233/1000\n",
      "11/11 - 0s - loss: 4.6267 - mse: 4.6267\n",
      "Epoch 234/1000\n",
      "11/11 - 0s - loss: 4.5814 - mse: 4.5814\n",
      "Epoch 235/1000\n",
      "11/11 - 0s - loss: 4.5364 - mse: 4.5364\n",
      "Epoch 236/1000\n",
      "11/11 - 0s - loss: 4.4917 - mse: 4.4917\n",
      "Epoch 237/1000\n",
      "11/11 - 0s - loss: 4.4474 - mse: 4.4474\n",
      "Epoch 238/1000\n",
      "11/11 - 0s - loss: 4.4034 - mse: 4.4034\n",
      "Epoch 239/1000\n",
      "11/11 - 0s - loss: 4.3597 - mse: 4.3597\n",
      "Epoch 240/1000\n",
      "11/11 - 0s - loss: 4.3163 - mse: 4.3163\n",
      "Epoch 241/1000\n",
      "11/11 - 0s - loss: 4.2733 - mse: 4.2733\n",
      "Epoch 242/1000\n",
      "11/11 - 0s - loss: 4.2306 - mse: 4.2306\n",
      "Epoch 243/1000\n",
      "11/11 - 0s - loss: 4.1882 - mse: 4.1882\n",
      "Epoch 244/1000\n",
      "11/11 - 0s - loss: 4.1461 - mse: 4.1461\n",
      "Epoch 245/1000\n",
      "11/11 - 0s - loss: 4.1043 - mse: 4.1043\n",
      "Epoch 246/1000\n",
      "11/11 - 0s - loss: 4.0629 - mse: 4.0629\n",
      "Epoch 247/1000\n",
      "11/11 - 0s - loss: 4.0217 - mse: 4.0217\n",
      "Epoch 248/1000\n",
      "11/11 - 0s - loss: 3.9809 - mse: 3.9809\n",
      "Epoch 249/1000\n",
      "11/11 - 0s - loss: 3.9404 - mse: 3.9404\n",
      "Epoch 250/1000\n",
      "11/11 - 0s - loss: 3.9002 - mse: 3.9002\n",
      "Epoch 251/1000\n",
      "11/11 - 0s - loss: 3.8603 - mse: 3.8603\n",
      "Epoch 252/1000\n",
      "11/11 - 0s - loss: 3.8207 - mse: 3.8207\n",
      "Epoch 253/1000\n",
      "11/11 - 0s - loss: 3.7815 - mse: 3.7815\n",
      "Epoch 254/1000\n",
      "11/11 - 0s - loss: 3.7425 - mse: 3.7425\n",
      "Epoch 255/1000\n",
      "11/11 - 0s - loss: 3.7038 - mse: 3.7038\n",
      "Epoch 256/1000\n",
      "11/11 - 0s - loss: 3.6655 - mse: 3.6655\n",
      "Epoch 257/1000\n",
      "11/11 - 0s - loss: 3.6274 - mse: 3.6274\n",
      "Epoch 258/1000\n",
      "11/11 - 0s - loss: 3.5897 - mse: 3.5897\n",
      "Epoch 259/1000\n",
      "11/11 - 0s - loss: 3.5522 - mse: 3.5522\n",
      "Epoch 260/1000\n",
      "11/11 - 0s - loss: 3.5150 - mse: 3.5150\n",
      "Epoch 261/1000\n",
      "11/11 - 0s - loss: 3.4782 - mse: 3.4782\n",
      "Epoch 262/1000\n",
      "11/11 - 0s - loss: 3.4416 - mse: 3.4416\n",
      "Epoch 263/1000\n",
      "11/11 - 0s - loss: 3.4053 - mse: 3.4053\n",
      "Epoch 264/1000\n",
      "11/11 - 0s - loss: 3.3694 - mse: 3.3694\n",
      "Epoch 265/1000\n",
      "11/11 - 0s - loss: 3.3337 - mse: 3.3337\n",
      "Epoch 266/1000\n",
      "11/11 - 0s - loss: 3.2983 - mse: 3.2983\n",
      "Epoch 267/1000\n",
      "11/11 - 0s - loss: 3.2632 - mse: 3.2632\n",
      "Epoch 268/1000\n",
      "11/11 - 0s - loss: 3.2284 - mse: 3.2284\n",
      "Epoch 269/1000\n",
      "11/11 - 0s - loss: 3.1938 - mse: 3.1938\n",
      "Epoch 270/1000\n",
      "11/11 - 0s - loss: 3.1596 - mse: 3.1596\n",
      "Epoch 271/1000\n",
      "11/11 - 0s - loss: 3.1256 - mse: 3.1256\n",
      "Epoch 272/1000\n",
      "11/11 - 0s - loss: 3.0919 - mse: 3.0919\n",
      "Epoch 273/1000\n",
      "11/11 - 0s - loss: 3.0585 - mse: 3.0585\n",
      "Epoch 274/1000\n",
      "11/11 - 0s - loss: 3.0254 - mse: 3.0254\n",
      "Epoch 275/1000\n",
      "11/11 - 0s - loss: 2.9926 - mse: 2.9926\n",
      "Epoch 276/1000\n",
      "11/11 - 0s - loss: 2.9600 - mse: 2.9600\n",
      "Epoch 277/1000\n",
      "11/11 - 0s - loss: 2.9277 - mse: 2.9277\n",
      "Epoch 278/1000\n",
      "11/11 - 0s - loss: 2.8957 - mse: 2.8957\n",
      "Epoch 279/1000\n",
      "11/11 - 0s - loss: 2.8640 - mse: 2.8640\n",
      "Epoch 280/1000\n",
      "11/11 - 0s - loss: 2.8325 - mse: 2.8325\n",
      "Epoch 281/1000\n",
      "11/11 - 0s - loss: 2.8013 - mse: 2.8013\n",
      "Epoch 282/1000\n",
      "11/11 - 0s - loss: 2.7704 - mse: 2.7704\n",
      "Epoch 283/1000\n",
      "11/11 - 0s - loss: 2.7398 - mse: 2.7398\n",
      "Epoch 284/1000\n",
      "11/11 - 0s - loss: 2.7094 - mse: 2.7094\n",
      "Epoch 285/1000\n",
      "11/11 - 0s - loss: 2.6793 - mse: 2.6793\n",
      "Epoch 286/1000\n",
      "11/11 - 0s - loss: 2.6494 - mse: 2.6494\n",
      "Epoch 287/1000\n",
      "11/11 - 0s - loss: 2.6198 - mse: 2.6198\n",
      "Epoch 288/1000\n",
      "11/11 - 0s - loss: 2.5905 - mse: 2.5905\n",
      "Epoch 289/1000\n",
      "11/11 - 0s - loss: 2.5615 - mse: 2.5615\n",
      "Epoch 290/1000\n",
      "11/11 - 0s - loss: 2.5327 - mse: 2.5327\n",
      "Epoch 291/1000\n",
      "11/11 - 0s - loss: 2.5041 - mse: 2.5041\n",
      "Epoch 292/1000\n",
      "11/11 - 0s - loss: 2.4758 - mse: 2.4758\n",
      "Epoch 293/1000\n",
      "11/11 - 0s - loss: 2.4478 - mse: 2.4478\n",
      "Epoch 294/1000\n",
      "11/11 - 0s - loss: 2.4200 - mse: 2.4200\n",
      "Epoch 295/1000\n",
      "11/11 - 0s - loss: 2.3925 - mse: 2.3925\n",
      "Epoch 296/1000\n",
      "11/11 - 0s - loss: 2.3653 - mse: 2.3653\n",
      "Epoch 297/1000\n",
      "11/11 - 0s - loss: 2.3383 - mse: 2.3383\n",
      "Epoch 298/1000\n",
      "11/11 - 0s - loss: 2.3115 - mse: 2.3115\n",
      "Epoch 299/1000\n",
      "11/11 - 0s - loss: 2.2850 - mse: 2.2850\n",
      "Epoch 300/1000\n",
      "11/11 - 0s - loss: 2.2587 - mse: 2.2587\n",
      "Epoch 301/1000\n",
      "11/11 - 0s - loss: 2.2327 - mse: 2.2327\n",
      "Epoch 302/1000\n",
      "11/11 - 0s - loss: 2.2070 - mse: 2.2070\n",
      "Epoch 303/1000\n",
      "11/11 - 0s - loss: 2.1814 - mse: 2.1814\n",
      "Epoch 304/1000\n",
      "11/11 - 0s - loss: 2.1562 - mse: 2.1562\n",
      "Epoch 305/1000\n",
      "11/11 - 0s - loss: 2.1311 - mse: 2.1311\n",
      "Epoch 306/1000\n",
      "11/11 - 0s - loss: 2.1063 - mse: 2.1063\n",
      "Epoch 307/1000\n",
      "11/11 - 0s - loss: 2.0818 - mse: 2.0818\n",
      "Epoch 308/1000\n",
      "11/11 - 0s - loss: 2.0574 - mse: 2.0574\n",
      "Epoch 309/1000\n",
      "11/11 - 0s - loss: 2.0334 - mse: 2.0334\n",
      "Epoch 310/1000\n",
      "11/11 - 0s - loss: 2.0095 - mse: 2.0095\n",
      "Epoch 311/1000\n",
      "11/11 - 0s - loss: 1.9859 - mse: 1.9859\n",
      "Epoch 312/1000\n",
      "11/11 - 0s - loss: 1.9625 - mse: 1.9625\n",
      "Epoch 313/1000\n",
      "11/11 - 0s - loss: 1.9394 - mse: 1.9394\n",
      "Epoch 314/1000\n",
      "11/11 - 0s - loss: 1.9165 - mse: 1.9165\n",
      "Epoch 315/1000\n",
      "11/11 - 0s - loss: 1.8938 - mse: 1.8938\n",
      "Epoch 316/1000\n",
      "11/11 - 0s - loss: 1.8713 - mse: 1.8713\n",
      "Epoch 317/1000\n",
      "11/11 - 0s - loss: 1.8491 - mse: 1.8491\n",
      "Epoch 318/1000\n",
      "11/11 - 0s - loss: 1.8271 - mse: 1.8271\n",
      "Epoch 319/1000\n",
      "11/11 - 0s - loss: 1.8053 - mse: 1.8053\n",
      "Epoch 320/1000\n",
      "11/11 - 0s - loss: 1.7838 - mse: 1.7838\n",
      "Epoch 321/1000\n",
      "11/11 - 0s - loss: 1.7625 - mse: 1.7625\n",
      "Epoch 322/1000\n",
      "11/11 - 0s - loss: 1.7413 - mse: 1.7413\n",
      "Epoch 323/1000\n",
      "11/11 - 0s - loss: 1.7204 - mse: 1.7204\n",
      "Epoch 324/1000\n",
      "11/11 - 0s - loss: 1.6998 - mse: 1.6998\n",
      "Epoch 325/1000\n",
      "11/11 - 0s - loss: 1.6793 - mse: 1.6793\n",
      "Epoch 326/1000\n",
      "11/11 - 0s - loss: 1.6591 - mse: 1.6591\n",
      "Epoch 327/1000\n",
      "11/11 - 0s - loss: 1.6391 - mse: 1.6391\n",
      "Epoch 328/1000\n",
      "11/11 - 0s - loss: 1.6192 - mse: 1.6192\n",
      "Epoch 329/1000\n",
      "11/11 - 0s - loss: 1.5996 - mse: 1.5996\n",
      "Epoch 330/1000\n",
      "11/11 - 0s - loss: 1.5803 - mse: 1.5803\n",
      "Epoch 331/1000\n",
      "11/11 - 0s - loss: 1.5611 - mse: 1.5611\n",
      "Epoch 332/1000\n",
      "11/11 - 0s - loss: 1.5421 - mse: 1.5421\n",
      "Epoch 333/1000\n",
      "11/11 - 0s - loss: 1.5233 - mse: 1.5233\n",
      "Epoch 334/1000\n",
      "11/11 - 0s - loss: 1.5048 - mse: 1.5048\n",
      "Epoch 335/1000\n",
      "11/11 - 0s - loss: 1.4864 - mse: 1.4864\n",
      "Epoch 336/1000\n",
      "11/11 - 0s - loss: 1.4683 - mse: 1.4683\n",
      "Epoch 337/1000\n",
      "11/11 - 0s - loss: 1.4503 - mse: 1.4503\n",
      "Epoch 338/1000\n",
      "11/11 - 0s - loss: 1.4326 - mse: 1.4326\n",
      "Epoch 339/1000\n",
      "11/11 - 0s - loss: 1.4150 - mse: 1.4150\n",
      "Epoch 340/1000\n",
      "11/11 - 0s - loss: 1.3977 - mse: 1.3977\n",
      "Epoch 341/1000\n",
      "11/11 - 0s - loss: 1.3805 - mse: 1.3805\n",
      "Epoch 342/1000\n",
      "11/11 - 0s - loss: 1.3636 - mse: 1.3636\n",
      "Epoch 343/1000\n",
      "11/11 - 0s - loss: 1.3468 - mse: 1.3468\n",
      "Epoch 344/1000\n",
      "11/11 - 0s - loss: 1.3302 - mse: 1.3302\n",
      "Epoch 345/1000\n",
      "11/11 - 0s - loss: 1.3138 - mse: 1.3138\n",
      "Epoch 346/1000\n",
      "11/11 - 0s - loss: 1.2976 - mse: 1.2976\n",
      "Epoch 347/1000\n",
      "11/11 - 0s - loss: 1.2816 - mse: 1.2816\n",
      "Epoch 348/1000\n",
      "11/11 - 0s - loss: 1.2658 - mse: 1.2658\n",
      "Epoch 349/1000\n",
      "11/11 - 0s - loss: 1.2502 - mse: 1.2502\n",
      "Epoch 350/1000\n",
      "11/11 - 0s - loss: 1.2348 - mse: 1.2348\n",
      "Epoch 351/1000\n",
      "11/11 - 0s - loss: 1.2195 - mse: 1.2195\n",
      "Epoch 352/1000\n",
      "11/11 - 0s - loss: 1.2044 - mse: 1.2044\n",
      "Epoch 353/1000\n",
      "11/11 - 0s - loss: 1.1895 - mse: 1.1895\n",
      "Epoch 354/1000\n",
      "11/11 - 0s - loss: 1.1748 - mse: 1.1748\n",
      "Epoch 355/1000\n",
      "11/11 - 0s - loss: 1.1603 - mse: 1.1603\n",
      "Epoch 356/1000\n",
      "11/11 - 0s - loss: 1.1459 - mse: 1.1459\n",
      "Epoch 357/1000\n",
      "11/11 - 0s - loss: 1.1317 - mse: 1.1317\n",
      "Epoch 358/1000\n",
      "11/11 - 0s - loss: 1.1177 - mse: 1.1177\n",
      "Epoch 359/1000\n",
      "11/11 - 0s - loss: 1.1039 - mse: 1.1039\n",
      "Epoch 360/1000\n",
      "11/11 - 0s - loss: 1.0902 - mse: 1.0902\n",
      "Epoch 361/1000\n",
      "11/11 - 0s - loss: 1.0767 - mse: 1.0767\n",
      "Epoch 362/1000\n",
      "11/11 - 0s - loss: 1.0634 - mse: 1.0634\n",
      "Epoch 363/1000\n",
      "11/11 - 0s - loss: 1.0503 - mse: 1.0503\n",
      "Epoch 364/1000\n",
      "11/11 - 0s - loss: 1.0373 - mse: 1.0373\n",
      "Epoch 365/1000\n",
      "11/11 - 0s - loss: 1.0245 - mse: 1.0245\n",
      "Epoch 366/1000\n",
      "11/11 - 0s - loss: 1.0118 - mse: 1.0118\n",
      "Epoch 367/1000\n",
      "11/11 - 0s - loss: 0.9993 - mse: 0.9993\n",
      "Epoch 368/1000\n",
      "11/11 - 0s - loss: 0.9870 - mse: 0.9870\n",
      "Epoch 369/1000\n",
      "11/11 - 0s - loss: 0.9748 - mse: 0.9748\n",
      "Epoch 370/1000\n",
      "11/11 - 0s - loss: 0.9628 - mse: 0.9628\n",
      "Epoch 371/1000\n",
      "11/11 - 0s - loss: 0.9510 - mse: 0.9510\n",
      "Epoch 372/1000\n",
      "11/11 - 0s - loss: 0.9393 - mse: 0.9393\n",
      "Epoch 373/1000\n",
      "11/11 - 0s - loss: 0.9277 - mse: 0.9277\n",
      "Epoch 374/1000\n",
      "11/11 - 0s - loss: 0.9163 - mse: 0.9163\n",
      "Epoch 375/1000\n",
      "11/11 - 0s - loss: 0.9051 - mse: 0.9051\n",
      "Epoch 376/1000\n",
      "11/11 - 0s - loss: 0.8940 - mse: 0.8940\n",
      "Epoch 377/1000\n",
      "11/11 - 0s - loss: 0.8831 - mse: 0.8831\n",
      "Epoch 378/1000\n",
      "11/11 - 0s - loss: 0.8723 - mse: 0.8723\n",
      "Epoch 379/1000\n",
      "11/11 - 0s - loss: 0.8617 - mse: 0.8617\n",
      "Epoch 380/1000\n",
      "11/11 - 0s - loss: 0.8512 - mse: 0.8512\n",
      "Epoch 381/1000\n",
      "11/11 - 0s - loss: 0.8409 - mse: 0.8409\n",
      "Epoch 382/1000\n",
      "11/11 - 0s - loss: 0.8307 - mse: 0.8307\n",
      "Epoch 383/1000\n",
      "11/11 - 0s - loss: 0.8206 - mse: 0.8206\n",
      "Epoch 384/1000\n",
      "11/11 - 0s - loss: 0.8107 - mse: 0.8107\n",
      "Epoch 385/1000\n",
      "11/11 - 0s - loss: 0.8009 - mse: 0.8009\n",
      "Epoch 386/1000\n",
      "11/11 - 0s - loss: 0.7913 - mse: 0.7913\n",
      "Epoch 387/1000\n",
      "11/11 - 0s - loss: 0.7818 - mse: 0.7818\n",
      "Epoch 388/1000\n",
      "11/11 - 0s - loss: 0.7724 - mse: 0.7724\n",
      "Epoch 389/1000\n",
      "11/11 - 0s - loss: 0.7632 - mse: 0.7632\n",
      "Epoch 390/1000\n",
      "11/11 - 0s - loss: 0.7541 - mse: 0.7541\n",
      "Epoch 391/1000\n",
      "11/11 - 0s - loss: 0.7452 - mse: 0.7452\n",
      "Epoch 392/1000\n",
      "11/11 - 0s - loss: 0.7363 - mse: 0.7363\n",
      "Epoch 393/1000\n",
      "11/11 - 0s - loss: 0.7276 - mse: 0.7276\n",
      "Epoch 394/1000\n",
      "11/11 - 0s - loss: 0.7191 - mse: 0.7191\n",
      "Epoch 395/1000\n",
      "11/11 - 0s - loss: 0.7106 - mse: 0.7106\n",
      "Epoch 396/1000\n",
      "11/11 - 0s - loss: 0.7023 - mse: 0.7023\n",
      "Epoch 397/1000\n",
      "11/11 - 0s - loss: 0.6941 - mse: 0.6941\n",
      "Epoch 398/1000\n",
      "11/11 - 0s - loss: 0.6861 - mse: 0.6861\n",
      "Epoch 399/1000\n",
      "11/11 - 0s - loss: 0.6781 - mse: 0.6781\n",
      "Epoch 400/1000\n",
      "11/11 - 0s - loss: 0.6703 - mse: 0.6703\n",
      "Epoch 401/1000\n",
      "11/11 - 0s - loss: 0.6626 - mse: 0.6626\n",
      "Epoch 402/1000\n",
      "11/11 - 0s - loss: 0.6550 - mse: 0.6550\n",
      "Epoch 403/1000\n",
      "11/11 - 0s - loss: 0.6476 - mse: 0.6476\n",
      "Epoch 404/1000\n",
      "11/11 - 0s - loss: 0.6402 - mse: 0.6402\n",
      "Epoch 405/1000\n",
      "11/11 - 0s - loss: 0.6330 - mse: 0.6330\n",
      "Epoch 406/1000\n",
      "11/11 - 0s - loss: 0.6259 - mse: 0.6259\n",
      "Epoch 407/1000\n",
      "11/11 - 0s - loss: 0.6189 - mse: 0.6189\n",
      "Epoch 408/1000\n",
      "11/11 - 0s - loss: 0.6120 - mse: 0.6120\n",
      "Epoch 409/1000\n",
      "11/11 - 0s - loss: 0.6052 - mse: 0.6052\n",
      "Epoch 410/1000\n",
      "11/11 - 0s - loss: 0.5985 - mse: 0.5985\n",
      "Epoch 411/1000\n",
      "11/11 - 0s - loss: 0.5920 - mse: 0.5920\n",
      "Epoch 412/1000\n",
      "11/11 - 0s - loss: 0.5855 - mse: 0.5855\n",
      "Epoch 413/1000\n",
      "11/11 - 0s - loss: 0.5792 - mse: 0.5792\n",
      "Epoch 414/1000\n",
      "11/11 - 0s - loss: 0.5729 - mse: 0.5729\n",
      "Epoch 415/1000\n",
      "11/11 - 0s - loss: 0.5668 - mse: 0.5668\n",
      "Epoch 416/1000\n",
      "11/11 - 0s - loss: 0.5607 - mse: 0.5607\n",
      "Epoch 417/1000\n",
      "11/11 - 0s - loss: 0.5548 - mse: 0.5548\n",
      "Epoch 418/1000\n",
      "11/11 - 0s - loss: 0.5489 - mse: 0.5489\n",
      "Epoch 419/1000\n",
      "11/11 - 0s - loss: 0.5432 - mse: 0.5432\n",
      "Epoch 420/1000\n",
      "11/11 - 0s - loss: 0.5376 - mse: 0.5376\n",
      "Epoch 421/1000\n",
      "11/11 - 0s - loss: 0.5320 - mse: 0.5320\n",
      "Epoch 422/1000\n",
      "11/11 - 0s - loss: 0.5266 - mse: 0.5266\n",
      "Epoch 423/1000\n",
      "11/11 - 0s - loss: 0.5212 - mse: 0.5212\n",
      "Epoch 424/1000\n",
      "11/11 - 0s - loss: 0.5159 - mse: 0.5159\n",
      "Epoch 425/1000\n",
      "11/11 - 0s - loss: 0.5108 - mse: 0.5108\n",
      "Epoch 426/1000\n",
      "11/11 - 0s - loss: 0.5057 - mse: 0.5057\n",
      "Epoch 427/1000\n",
      "11/11 - 0s - loss: 0.5007 - mse: 0.5007\n",
      "Epoch 428/1000\n",
      "11/11 - 0s - loss: 0.4958 - mse: 0.4958\n",
      "Epoch 429/1000\n",
      "11/11 - 0s - loss: 0.4910 - mse: 0.4910\n",
      "Epoch 430/1000\n",
      "11/11 - 0s - loss: 0.4862 - mse: 0.4862\n",
      "Epoch 431/1000\n",
      "11/11 - 0s - loss: 0.4816 - mse: 0.4816\n",
      "Epoch 432/1000\n",
      "11/11 - 0s - loss: 0.4770 - mse: 0.4770\n",
      "Epoch 433/1000\n",
      "11/11 - 0s - loss: 0.4725 - mse: 0.4725\n",
      "Epoch 434/1000\n",
      "11/11 - 0s - loss: 0.4681 - mse: 0.4681\n",
      "Epoch 435/1000\n",
      "11/11 - 0s - loss: 0.4638 - mse: 0.4638\n",
      "Epoch 436/1000\n",
      "11/11 - 0s - loss: 0.4596 - mse: 0.4596\n",
      "Epoch 437/1000\n",
      "11/11 - 0s - loss: 0.4554 - mse: 0.4554\n",
      "Epoch 438/1000\n",
      "11/11 - 0s - loss: 0.4513 - mse: 0.4513\n",
      "Epoch 439/1000\n",
      "11/11 - 0s - loss: 0.4473 - mse: 0.4473\n",
      "Epoch 440/1000\n",
      "11/11 - 0s - loss: 0.4434 - mse: 0.4434\n",
      "Epoch 441/1000\n",
      "11/11 - 0s - loss: 0.4395 - mse: 0.4395\n",
      "Epoch 442/1000\n",
      "11/11 - 0s - loss: 0.4357 - mse: 0.4357\n",
      "Epoch 443/1000\n",
      "11/11 - 0s - loss: 0.4320 - mse: 0.4320\n",
      "Epoch 444/1000\n",
      "11/11 - 0s - loss: 0.4284 - mse: 0.4284\n",
      "Epoch 445/1000\n",
      "11/11 - 0s - loss: 0.4248 - mse: 0.4248\n",
      "Epoch 446/1000\n",
      "11/11 - 0s - loss: 0.4213 - mse: 0.4213\n",
      "Epoch 447/1000\n",
      "11/11 - 0s - loss: 0.4179 - mse: 0.4179\n",
      "Epoch 448/1000\n",
      "11/11 - 0s - loss: 0.4145 - mse: 0.4145\n",
      "Epoch 449/1000\n",
      "11/11 - 0s - loss: 0.4112 - mse: 0.4112\n",
      "Epoch 450/1000\n",
      "11/11 - 0s - loss: 0.4080 - mse: 0.4080\n",
      "Epoch 451/1000\n",
      "11/11 - 0s - loss: 0.4048 - mse: 0.4048\n",
      "Epoch 452/1000\n",
      "11/11 - 0s - loss: 0.4017 - mse: 0.4017\n",
      "Epoch 453/1000\n",
      "11/11 - 0s - loss: 0.3987 - mse: 0.3987\n",
      "Epoch 454/1000\n",
      "11/11 - 0s - loss: 0.3957 - mse: 0.3957\n",
      "Epoch 455/1000\n",
      "11/11 - 0s - loss: 0.3928 - mse: 0.3928\n",
      "Epoch 456/1000\n",
      "11/11 - 0s - loss: 0.3899 - mse: 0.3899\n",
      "Epoch 457/1000\n",
      "11/11 - 0s - loss: 0.3871 - mse: 0.3871\n",
      "Epoch 458/1000\n",
      "11/11 - 0s - loss: 0.3844 - mse: 0.3844\n",
      "Epoch 459/1000\n",
      "11/11 - 0s - loss: 0.3817 - mse: 0.3817\n",
      "Epoch 460/1000\n",
      "11/11 - 0s - loss: 0.3790 - mse: 0.3790\n",
      "Epoch 461/1000\n",
      "11/11 - 0s - loss: 0.3765 - mse: 0.3765\n",
      "Epoch 462/1000\n",
      "11/11 - 0s - loss: 0.3739 - mse: 0.3739\n",
      "Epoch 463/1000\n",
      "11/11 - 0s - loss: 0.3715 - mse: 0.3715\n",
      "Epoch 464/1000\n",
      "11/11 - 0s - loss: 0.3690 - mse: 0.3690\n",
      "Epoch 465/1000\n",
      "11/11 - 0s - loss: 0.3667 - mse: 0.3667\n",
      "Epoch 466/1000\n",
      "11/11 - 0s - loss: 0.3644 - mse: 0.3644\n",
      "Epoch 467/1000\n",
      "11/11 - 0s - loss: 0.3621 - mse: 0.3621\n",
      "Epoch 468/1000\n",
      "11/11 - 0s - loss: 0.3599 - mse: 0.3599\n",
      "Epoch 469/1000\n",
      "11/11 - 0s - loss: 0.3577 - mse: 0.3577\n",
      "Epoch 470/1000\n",
      "11/11 - 0s - loss: 0.3556 - mse: 0.3556\n",
      "Epoch 471/1000\n",
      "11/11 - 0s - loss: 0.3535 - mse: 0.3535\n",
      "Epoch 472/1000\n",
      "11/11 - 0s - loss: 0.3515 - mse: 0.3515\n",
      "Epoch 473/1000\n",
      "11/11 - 0s - loss: 0.3495 - mse: 0.3495\n",
      "Epoch 474/1000\n",
      "11/11 - 0s - loss: 0.3475 - mse: 0.3475\n",
      "Epoch 475/1000\n",
      "11/11 - 0s - loss: 0.3456 - mse: 0.3456\n",
      "Epoch 476/1000\n",
      "11/11 - 0s - loss: 0.3438 - mse: 0.3438\n",
      "Epoch 477/1000\n",
      "11/11 - 0s - loss: 0.3419 - mse: 0.3419\n",
      "Epoch 478/1000\n",
      "11/11 - 0s - loss: 0.3402 - mse: 0.3402\n",
      "Epoch 479/1000\n",
      "11/11 - 0s - loss: 0.3384 - mse: 0.3384\n",
      "Epoch 480/1000\n",
      "11/11 - 0s - loss: 0.3367 - mse: 0.3367\n",
      "Epoch 481/1000\n",
      "11/11 - 0s - loss: 0.3351 - mse: 0.3351\n",
      "Epoch 482/1000\n",
      "11/11 - 0s - loss: 0.3335 - mse: 0.3335\n",
      "Epoch 483/1000\n",
      "11/11 - 0s - loss: 0.3319 - mse: 0.3319\n",
      "Epoch 484/1000\n",
      "11/11 - 0s - loss: 0.3303 - mse: 0.3303\n",
      "Epoch 485/1000\n",
      "11/11 - 0s - loss: 0.3288 - mse: 0.3288\n",
      "Epoch 486/1000\n",
      "11/11 - 0s - loss: 0.3273 - mse: 0.3273\n",
      "Epoch 487/1000\n",
      "11/11 - 0s - loss: 0.3259 - mse: 0.3259\n",
      "Epoch 488/1000\n",
      "11/11 - 0s - loss: 0.3245 - mse: 0.3245\n",
      "Epoch 489/1000\n",
      "11/11 - 0s - loss: 0.3231 - mse: 0.3231\n",
      "Epoch 490/1000\n",
      "11/11 - 0s - loss: 0.3218 - mse: 0.3218\n",
      "Epoch 491/1000\n",
      "11/11 - 0s - loss: 0.3205 - mse: 0.3205\n",
      "Epoch 492/1000\n",
      "11/11 - 0s - loss: 0.3192 - mse: 0.3192\n",
      "Epoch 493/1000\n",
      "11/11 - 0s - loss: 0.3179 - mse: 0.3179\n",
      "Epoch 494/1000\n",
      "11/11 - 0s - loss: 0.3167 - mse: 0.3167\n",
      "Epoch 495/1000\n",
      "11/11 - 0s - loss: 0.3155 - mse: 0.3155\n",
      "Epoch 496/1000\n",
      "11/11 - 0s - loss: 0.3143 - mse: 0.3143\n",
      "Epoch 497/1000\n",
      "11/11 - 0s - loss: 0.3132 - mse: 0.3132\n",
      "Epoch 498/1000\n",
      "11/11 - 0s - loss: 0.3121 - mse: 0.3121\n",
      "Epoch 499/1000\n",
      "11/11 - 0s - loss: 0.3110 - mse: 0.3110\n",
      "Epoch 500/1000\n",
      "11/11 - 0s - loss: 0.3100 - mse: 0.3100\n",
      "Epoch 501/1000\n",
      "11/11 - 0s - loss: 0.3089 - mse: 0.3089\n",
      "Epoch 502/1000\n",
      "11/11 - 0s - loss: 0.3079 - mse: 0.3079\n",
      "Epoch 503/1000\n",
      "11/11 - 0s - loss: 0.3069 - mse: 0.3069\n",
      "Epoch 504/1000\n",
      "11/11 - 0s - loss: 0.3060 - mse: 0.3060\n",
      "Epoch 505/1000\n",
      "11/11 - 0s - loss: 0.3050 - mse: 0.3050\n",
      "Epoch 506/1000\n",
      "11/11 - 0s - loss: 0.3041 - mse: 0.3041\n",
      "Epoch 507/1000\n",
      "11/11 - 0s - loss: 0.3032 - mse: 0.3032\n",
      "Epoch 508/1000\n",
      "11/11 - 0s - loss: 0.3024 - mse: 0.3024\n",
      "Epoch 509/1000\n",
      "11/11 - 0s - loss: 0.3015 - mse: 0.3015\n",
      "Epoch 510/1000\n",
      "11/11 - 0s - loss: 0.3007 - mse: 0.3007\n",
      "Epoch 511/1000\n",
      "11/11 - 0s - loss: 0.2999 - mse: 0.2999\n",
      "Epoch 512/1000\n",
      "11/11 - 0s - loss: 0.2991 - mse: 0.2991\n",
      "Epoch 513/1000\n",
      "11/11 - 0s - loss: 0.2983 - mse: 0.2983\n",
      "Epoch 514/1000\n",
      "11/11 - 0s - loss: 0.2976 - mse: 0.2976\n",
      "Epoch 515/1000\n",
      "11/11 - 0s - loss: 0.2969 - mse: 0.2969\n",
      "Epoch 516/1000\n",
      "11/11 - 0s - loss: 0.2962 - mse: 0.2962\n",
      "Epoch 517/1000\n",
      "11/11 - 0s - loss: 0.2955 - mse: 0.2955\n",
      "Epoch 518/1000\n",
      "11/11 - 0s - loss: 0.2948 - mse: 0.2948\n",
      "Epoch 519/1000\n",
      "11/11 - 0s - loss: 0.2941 - mse: 0.2941\n",
      "Epoch 520/1000\n",
      "11/11 - 0s - loss: 0.2935 - mse: 0.2935\n",
      "Epoch 521/1000\n",
      "11/11 - 0s - loss: 0.2929 - mse: 0.2929\n",
      "Epoch 522/1000\n",
      "11/11 - 0s - loss: 0.2922 - mse: 0.2922\n",
      "Epoch 523/1000\n",
      "11/11 - 0s - loss: 0.2916 - mse: 0.2916\n",
      "Epoch 524/1000\n",
      "11/11 - 0s - loss: 0.2911 - mse: 0.2911\n",
      "Epoch 525/1000\n",
      "11/11 - 0s - loss: 0.2905 - mse: 0.2905\n",
      "Epoch 526/1000\n",
      "11/11 - 0s - loss: 0.2899 - mse: 0.2899\n",
      "Epoch 527/1000\n",
      "11/11 - 0s - loss: 0.2894 - mse: 0.2894\n",
      "Epoch 528/1000\n",
      "11/11 - 0s - loss: 0.2889 - mse: 0.2889\n",
      "Epoch 529/1000\n",
      "11/11 - 0s - loss: 0.2883 - mse: 0.2883\n",
      "Epoch 530/1000\n",
      "11/11 - 0s - loss: 0.2878 - mse: 0.2878\n",
      "Epoch 531/1000\n",
      "11/11 - 0s - loss: 0.2874 - mse: 0.2874\n",
      "Epoch 532/1000\n",
      "11/11 - 0s - loss: 0.2869 - mse: 0.2869\n",
      "Epoch 533/1000\n",
      "11/11 - 0s - loss: 0.2864 - mse: 0.2864\n",
      "Epoch 534/1000\n",
      "11/11 - 0s - loss: 0.2859 - mse: 0.2859\n",
      "Epoch 535/1000\n",
      "11/11 - 0s - loss: 0.2855 - mse: 0.2855\n",
      "Epoch 536/1000\n",
      "11/11 - 0s - loss: 0.2851 - mse: 0.2851\n",
      "Epoch 537/1000\n",
      "11/11 - 0s - loss: 0.2846 - mse: 0.2846\n",
      "Epoch 538/1000\n",
      "11/11 - 0s - loss: 0.2842 - mse: 0.2842\n",
      "Epoch 539/1000\n",
      "11/11 - 0s - loss: 0.2838 - mse: 0.2838\n",
      "Epoch 540/1000\n",
      "11/11 - 0s - loss: 0.2834 - mse: 0.2834\n",
      "Epoch 541/1000\n",
      "11/11 - 0s - loss: 0.2830 - mse: 0.2830\n",
      "Epoch 542/1000\n",
      "11/11 - 0s - loss: 0.2826 - mse: 0.2826\n",
      "Epoch 543/1000\n",
      "11/11 - 0s - loss: 0.2823 - mse: 0.2823\n",
      "Epoch 544/1000\n",
      "11/11 - 0s - loss: 0.2819 - mse: 0.2819\n",
      "Epoch 545/1000\n",
      "11/11 - 0s - loss: 0.2816 - mse: 0.2816\n",
      "Epoch 546/1000\n",
      "11/11 - 0s - loss: 0.2812 - mse: 0.2812\n",
      "Epoch 547/1000\n",
      "11/11 - 0s - loss: 0.2809 - mse: 0.2809\n",
      "Epoch 548/1000\n",
      "11/11 - 0s - loss: 0.2805 - mse: 0.2805\n",
      "Epoch 549/1000\n",
      "11/11 - 0s - loss: 0.2802 - mse: 0.2802\n",
      "Epoch 550/1000\n",
      "11/11 - 0s - loss: 0.2799 - mse: 0.2799\n",
      "Epoch 551/1000\n",
      "11/11 - 0s - loss: 0.2796 - mse: 0.2796\n",
      "Epoch 552/1000\n",
      "11/11 - 0s - loss: 0.2793 - mse: 0.2793\n",
      "Epoch 553/1000\n",
      "11/11 - 0s - loss: 0.2790 - mse: 0.2790\n",
      "Epoch 554/1000\n",
      "11/11 - 0s - loss: 0.2787 - mse: 0.2787\n",
      "Epoch 555/1000\n",
      "11/11 - 0s - loss: 0.2784 - mse: 0.2784\n",
      "Epoch 556/1000\n",
      "11/11 - 0s - loss: 0.2781 - mse: 0.2781\n",
      "Epoch 557/1000\n",
      "11/11 - 0s - loss: 0.2778 - mse: 0.2778\n",
      "Epoch 558/1000\n",
      "11/11 - 0s - loss: 0.2776 - mse: 0.2776\n",
      "Epoch 559/1000\n",
      "11/11 - 0s - loss: 0.2773 - mse: 0.2773\n",
      "Epoch 560/1000\n",
      "11/11 - 0s - loss: 0.2770 - mse: 0.2770\n",
      "Epoch 561/1000\n",
      "11/11 - 0s - loss: 0.2768 - mse: 0.2768\n",
      "Epoch 562/1000\n",
      "11/11 - 0s - loss: 0.2765 - mse: 0.2765\n",
      "Epoch 563/1000\n",
      "11/11 - 0s - loss: 0.2763 - mse: 0.2763\n",
      "Epoch 564/1000\n",
      "11/11 - 0s - loss: 0.2760 - mse: 0.2760\n",
      "Epoch 565/1000\n",
      "11/11 - 0s - loss: 0.2758 - mse: 0.2758\n",
      "Epoch 566/1000\n",
      "11/11 - 0s - loss: 0.2756 - mse: 0.2756\n",
      "Epoch 567/1000\n",
      "11/11 - 0s - loss: 0.2753 - mse: 0.2753\n",
      "Epoch 568/1000\n",
      "11/11 - 0s - loss: 0.2751 - mse: 0.2751\n",
      "Epoch 569/1000\n",
      "11/11 - 0s - loss: 0.2749 - mse: 0.2749\n",
      "Epoch 570/1000\n",
      "11/11 - 0s - loss: 0.2747 - mse: 0.2747\n",
      "Epoch 571/1000\n",
      "11/11 - 0s - loss: 0.2745 - mse: 0.2745\n",
      "Epoch 572/1000\n",
      "11/11 - 0s - loss: 0.2742 - mse: 0.2742\n",
      "Epoch 573/1000\n",
      "11/11 - 0s - loss: 0.2740 - mse: 0.2740\n",
      "Epoch 574/1000\n",
      "11/11 - 0s - loss: 0.2738 - mse: 0.2738\n",
      "Epoch 575/1000\n",
      "11/11 - 0s - loss: 0.2736 - mse: 0.2736\n",
      "Epoch 576/1000\n",
      "11/11 - 0s - loss: 0.2734 - mse: 0.2734\n",
      "Epoch 577/1000\n",
      "11/11 - 0s - loss: 0.2732 - mse: 0.2732\n",
      "Epoch 578/1000\n",
      "11/11 - 0s - loss: 0.2730 - mse: 0.2730\n",
      "Epoch 579/1000\n",
      "11/11 - 0s - loss: 0.2728 - mse: 0.2728\n",
      "Epoch 580/1000\n",
      "11/11 - 0s - loss: 0.2727 - mse: 0.2727\n",
      "Epoch 581/1000\n",
      "11/11 - 0s - loss: 0.2725 - mse: 0.2725\n",
      "Epoch 582/1000\n",
      "11/11 - 0s - loss: 0.2723 - mse: 0.2723\n",
      "Epoch 583/1000\n",
      "11/11 - 0s - loss: 0.2721 - mse: 0.2721\n",
      "Epoch 584/1000\n",
      "11/11 - 0s - loss: 0.2719 - mse: 0.2719\n",
      "Epoch 585/1000\n",
      "11/11 - 0s - loss: 0.2717 - mse: 0.2717\n",
      "Epoch 586/1000\n",
      "11/11 - 0s - loss: 0.2716 - mse: 0.2716\n",
      "Epoch 587/1000\n",
      "11/11 - 0s - loss: 0.2714 - mse: 0.2714\n",
      "Epoch 588/1000\n",
      "11/11 - 0s - loss: 0.2712 - mse: 0.2712\n",
      "Epoch 589/1000\n",
      "11/11 - 0s - loss: 0.2710 - mse: 0.2710\n",
      "Epoch 590/1000\n",
      "11/11 - 0s - loss: 0.2709 - mse: 0.2709\n",
      "Epoch 591/1000\n",
      "11/11 - 0s - loss: 0.2707 - mse: 0.2707\n",
      "Epoch 592/1000\n",
      "11/11 - 0s - loss: 0.2705 - mse: 0.2705\n",
      "Epoch 593/1000\n",
      "11/11 - 0s - loss: 0.2704 - mse: 0.2704\n",
      "Epoch 594/1000\n",
      "11/11 - 0s - loss: 0.2702 - mse: 0.2702\n",
      "Epoch 595/1000\n",
      "11/11 - 0s - loss: 0.2700 - mse: 0.2700\n",
      "Epoch 596/1000\n",
      "11/11 - 0s - loss: 0.2699 - mse: 0.2699\n",
      "Epoch 597/1000\n",
      "11/11 - 0s - loss: 0.2697 - mse: 0.2697\n",
      "Epoch 598/1000\n",
      "11/11 - 0s - loss: 0.2696 - mse: 0.2696\n",
      "Epoch 599/1000\n",
      "11/11 - 0s - loss: 0.2694 - mse: 0.2694\n",
      "Epoch 600/1000\n",
      "11/11 - 0s - loss: 0.2692 - mse: 0.2692\n",
      "Epoch 601/1000\n",
      "11/11 - 0s - loss: 0.2691 - mse: 0.2691\n",
      "Epoch 602/1000\n",
      "11/11 - 0s - loss: 0.2689 - mse: 0.2689\n",
      "Epoch 603/1000\n",
      "11/11 - 0s - loss: 0.2688 - mse: 0.2688\n",
      "Epoch 604/1000\n",
      "11/11 - 0s - loss: 0.2686 - mse: 0.2686\n",
      "Epoch 605/1000\n",
      "11/11 - 0s - loss: 0.2685 - mse: 0.2685\n",
      "Epoch 606/1000\n",
      "11/11 - 0s - loss: 0.2683 - mse: 0.2683\n",
      "Epoch 607/1000\n",
      "11/11 - 0s - loss: 0.2682 - mse: 0.2682\n",
      "Epoch 608/1000\n",
      "11/11 - 0s - loss: 0.2680 - mse: 0.2680\n",
      "Epoch 609/1000\n",
      "11/11 - 0s - loss: 0.2679 - mse: 0.2679\n",
      "Epoch 610/1000\n",
      "11/11 - 0s - loss: 0.2677 - mse: 0.2677\n",
      "Epoch 611/1000\n",
      "11/11 - 0s - loss: 0.2676 - mse: 0.2676\n",
      "Epoch 612/1000\n",
      "11/11 - 0s - loss: 0.2674 - mse: 0.2674\n",
      "Epoch 613/1000\n",
      "11/11 - 0s - loss: 0.2673 - mse: 0.2673\n",
      "Epoch 614/1000\n",
      "11/11 - 0s - loss: 0.2671 - mse: 0.2671\n",
      "Epoch 615/1000\n",
      "11/11 - 0s - loss: 0.2670 - mse: 0.2670\n",
      "Epoch 616/1000\n",
      "11/11 - 0s - loss: 0.2669 - mse: 0.2669\n",
      "Epoch 617/1000\n",
      "11/11 - 0s - loss: 0.2667 - mse: 0.2667\n",
      "Epoch 618/1000\n",
      "11/11 - 0s - loss: 0.2666 - mse: 0.2666\n",
      "Epoch 619/1000\n",
      "11/11 - 0s - loss: 0.2664 - mse: 0.2664\n",
      "Epoch 620/1000\n",
      "11/11 - 0s - loss: 0.2663 - mse: 0.2663\n",
      "Epoch 621/1000\n",
      "11/11 - 0s - loss: 0.2662 - mse: 0.2662\n",
      "Epoch 622/1000\n",
      "11/11 - 0s - loss: 0.2660 - mse: 0.2660\n",
      "Epoch 623/1000\n",
      "11/11 - 0s - loss: 0.2659 - mse: 0.2659\n",
      "Epoch 624/1000\n",
      "11/11 - 0s - loss: 0.2657 - mse: 0.2657\n",
      "Epoch 625/1000\n",
      "11/11 - 0s - loss: 0.2656 - mse: 0.2656\n",
      "Epoch 626/1000\n",
      "11/11 - 0s - loss: 0.2655 - mse: 0.2655\n",
      "Epoch 627/1000\n",
      "11/11 - 0s - loss: 0.2653 - mse: 0.2653\n",
      "Epoch 628/1000\n",
      "11/11 - 0s - loss: 0.2652 - mse: 0.2652\n",
      "Epoch 629/1000\n",
      "11/11 - 0s - loss: 0.2651 - mse: 0.2651\n",
      "Epoch 630/1000\n",
      "11/11 - 0s - loss: 0.2649 - mse: 0.2649\n",
      "Epoch 631/1000\n",
      "11/11 - 0s - loss: 0.2648 - mse: 0.2648\n",
      "Epoch 632/1000\n",
      "11/11 - 0s - loss: 0.2647 - mse: 0.2647\n",
      "Epoch 633/1000\n",
      "11/11 - 0s - loss: 0.2645 - mse: 0.2645\n",
      "Epoch 634/1000\n",
      "11/11 - 0s - loss: 0.2644 - mse: 0.2644\n",
      "Epoch 635/1000\n",
      "11/11 - 0s - loss: 0.2643 - mse: 0.2643\n",
      "Epoch 636/1000\n",
      "11/11 - 0s - loss: 0.2641 - mse: 0.2641\n",
      "Epoch 637/1000\n",
      "11/11 - 0s - loss: 0.2640 - mse: 0.2640\n",
      "Epoch 638/1000\n",
      "11/11 - 0s - loss: 0.2639 - mse: 0.2639\n",
      "Epoch 639/1000\n",
      "11/11 - 0s - loss: 0.2637 - mse: 0.2637\n",
      "Epoch 640/1000\n",
      "11/11 - 0s - loss: 0.2636 - mse: 0.2636\n",
      "Epoch 641/1000\n",
      "11/11 - 0s - loss: 0.2635 - mse: 0.2635\n",
      "Epoch 642/1000\n",
      "11/11 - 0s - loss: 0.2633 - mse: 0.2633\n",
      "Epoch 643/1000\n",
      "11/11 - 0s - loss: 0.2632 - mse: 0.2632\n",
      "Epoch 644/1000\n",
      "11/11 - 0s - loss: 0.2631 - mse: 0.2631\n",
      "Epoch 645/1000\n",
      "11/11 - 0s - loss: 0.2630 - mse: 0.2630\n",
      "Epoch 646/1000\n",
      "11/11 - 0s - loss: 0.2628 - mse: 0.2628\n",
      "Epoch 647/1000\n",
      "11/11 - 0s - loss: 0.2627 - mse: 0.2627\n",
      "Epoch 648/1000\n",
      "11/11 - 0s - loss: 0.2626 - mse: 0.2626\n",
      "Epoch 649/1000\n",
      "11/11 - 0s - loss: 0.2624 - mse: 0.2624\n",
      "Epoch 650/1000\n",
      "11/11 - 0s - loss: 0.2623 - mse: 0.2623\n",
      "Epoch 651/1000\n",
      "11/11 - 0s - loss: 0.2622 - mse: 0.2622\n",
      "Epoch 652/1000\n",
      "11/11 - 0s - loss: 0.2621 - mse: 0.2621\n",
      "Epoch 653/1000\n",
      "11/11 - 0s - loss: 0.2619 - mse: 0.2619\n",
      "Epoch 654/1000\n",
      "11/11 - 0s - loss: 0.2618 - mse: 0.2618\n",
      "Epoch 655/1000\n",
      "11/11 - 0s - loss: 0.2617 - mse: 0.2617\n",
      "Epoch 656/1000\n",
      "11/11 - 0s - loss: 0.2616 - mse: 0.2616\n",
      "Epoch 657/1000\n",
      "11/11 - 0s - loss: 0.2615 - mse: 0.2615\n",
      "Epoch 658/1000\n",
      "11/11 - 0s - loss: 0.2613 - mse: 0.2613\n",
      "Epoch 659/1000\n",
      "11/11 - 0s - loss: 0.2612 - mse: 0.2612\n",
      "Epoch 660/1000\n",
      "11/11 - 0s - loss: 0.2611 - mse: 0.2611\n",
      "Epoch 661/1000\n",
      "11/11 - 0s - loss: 0.2610 - mse: 0.2610\n",
      "Epoch 662/1000\n",
      "11/11 - 0s - loss: 0.2608 - mse: 0.2608\n",
      "Epoch 663/1000\n",
      "11/11 - 0s - loss: 0.2607 - mse: 0.2607\n",
      "Epoch 664/1000\n",
      "11/11 - 0s - loss: 0.2606 - mse: 0.2606\n",
      "Epoch 665/1000\n",
      "11/11 - 0s - loss: 0.2605 - mse: 0.2605\n",
      "Epoch 666/1000\n",
      "11/11 - 0s - loss: 0.2604 - mse: 0.2604\n",
      "Epoch 667/1000\n",
      "11/11 - 0s - loss: 0.2603 - mse: 0.2603\n",
      "Epoch 668/1000\n",
      "11/11 - 0s - loss: 0.2601 - mse: 0.2601\n",
      "Epoch 669/1000\n",
      "11/11 - 0s - loss: 0.2600 - mse: 0.2600\n",
      "Epoch 670/1000\n",
      "11/11 - 0s - loss: 0.2599 - mse: 0.2599\n",
      "Epoch 671/1000\n",
      "11/11 - 0s - loss: 0.2598 - mse: 0.2598\n",
      "Epoch 672/1000\n",
      "11/11 - 0s - loss: 0.2597 - mse: 0.2597\n",
      "Epoch 673/1000\n",
      "11/11 - 0s - loss: 0.2596 - mse: 0.2596\n",
      "Epoch 674/1000\n",
      "11/11 - 0s - loss: 0.2594 - mse: 0.2594\n",
      "Epoch 675/1000\n",
      "11/11 - 0s - loss: 0.2593 - mse: 0.2593\n",
      "Epoch 676/1000\n",
      "11/11 - 0s - loss: 0.2592 - mse: 0.2592\n",
      "Epoch 677/1000\n",
      "11/11 - 0s - loss: 0.2591 - mse: 0.2591\n",
      "Epoch 678/1000\n",
      "11/11 - 0s - loss: 0.2590 - mse: 0.2590\n",
      "Epoch 679/1000\n",
      "11/11 - 0s - loss: 0.2589 - mse: 0.2589\n",
      "Epoch 680/1000\n",
      "11/11 - 0s - loss: 0.2588 - mse: 0.2588\n",
      "Epoch 681/1000\n",
      "11/11 - 0s - loss: 0.2586 - mse: 0.2586\n",
      "Epoch 682/1000\n",
      "11/11 - 0s - loss: 0.2585 - mse: 0.2585\n",
      "Epoch 683/1000\n",
      "11/11 - 0s - loss: 0.2584 - mse: 0.2584\n",
      "Epoch 684/1000\n",
      "11/11 - 0s - loss: 0.2583 - mse: 0.2583\n",
      "Epoch 685/1000\n",
      "11/11 - 0s - loss: 0.2582 - mse: 0.2582\n",
      "Epoch 686/1000\n",
      "11/11 - 0s - loss: 0.2581 - mse: 0.2581\n",
      "Epoch 687/1000\n",
      "11/11 - 0s - loss: 0.2580 - mse: 0.2580\n",
      "Epoch 688/1000\n",
      "11/11 - 0s - loss: 0.2579 - mse: 0.2579\n",
      "Epoch 689/1000\n",
      "11/11 - 0s - loss: 0.2578 - mse: 0.2578\n",
      "Epoch 690/1000\n",
      "11/11 - 0s - loss: 0.2577 - mse: 0.2577\n",
      "Epoch 691/1000\n",
      "11/11 - 0s - loss: 0.2576 - mse: 0.2576\n",
      "Epoch 692/1000\n",
      "11/11 - 0s - loss: 0.2575 - mse: 0.2575\n",
      "Epoch 693/1000\n",
      "11/11 - 0s - loss: 0.2573 - mse: 0.2573\n",
      "Epoch 694/1000\n",
      "11/11 - 0s - loss: 0.2572 - mse: 0.2572\n",
      "Epoch 695/1000\n",
      "11/11 - 0s - loss: 0.2571 - mse: 0.2571\n",
      "Epoch 696/1000\n",
      "11/11 - 0s - loss: 0.2570 - mse: 0.2570\n",
      "Epoch 697/1000\n",
      "11/11 - 0s - loss: 0.2569 - mse: 0.2569\n",
      "Epoch 698/1000\n",
      "11/11 - 0s - loss: 0.2568 - mse: 0.2568\n",
      "Epoch 699/1000\n",
      "11/11 - 0s - loss: 0.2567 - mse: 0.2567\n",
      "Epoch 700/1000\n",
      "11/11 - 0s - loss: 0.2566 - mse: 0.2566\n",
      "Epoch 701/1000\n",
      "11/11 - 0s - loss: 0.2565 - mse: 0.2565\n",
      "Epoch 702/1000\n",
      "11/11 - 0s - loss: 0.2564 - mse: 0.2564\n",
      "Epoch 703/1000\n",
      "11/11 - 0s - loss: 0.2563 - mse: 0.2563\n",
      "Epoch 704/1000\n",
      "11/11 - 0s - loss: 0.2562 - mse: 0.2562\n",
      "Epoch 705/1000\n",
      "11/11 - 0s - loss: 0.2561 - mse: 0.2561\n",
      "Epoch 706/1000\n",
      "11/11 - 0s - loss: 0.2560 - mse: 0.2560\n",
      "Epoch 707/1000\n",
      "11/11 - 0s - loss: 0.2559 - mse: 0.2559\n",
      "Epoch 708/1000\n",
      "11/11 - 0s - loss: 0.2558 - mse: 0.2558\n",
      "Epoch 709/1000\n",
      "11/11 - 0s - loss: 0.2557 - mse: 0.2557\n",
      "Epoch 710/1000\n",
      "11/11 - 0s - loss: 0.2556 - mse: 0.2556\n",
      "Epoch 711/1000\n",
      "11/11 - 0s - loss: 0.2555 - mse: 0.2555\n",
      "Epoch 712/1000\n",
      "11/11 - 0s - loss: 0.2554 - mse: 0.2554\n",
      "Epoch 713/1000\n",
      "11/11 - 0s - loss: 0.2554 - mse: 0.2554\n",
      "Epoch 714/1000\n",
      "11/11 - 0s - loss: 0.2553 - mse: 0.2553\n",
      "Epoch 715/1000\n",
      "11/11 - 0s - loss: 0.2552 - mse: 0.2552\n",
      "Epoch 716/1000\n",
      "11/11 - 0s - loss: 0.2551 - mse: 0.2551\n",
      "Epoch 717/1000\n",
      "11/11 - 0s - loss: 0.2550 - mse: 0.2550\n",
      "Epoch 718/1000\n",
      "11/11 - 0s - loss: 0.2549 - mse: 0.2549\n",
      "Epoch 719/1000\n",
      "11/11 - 0s - loss: 0.2548 - mse: 0.2548\n",
      "Epoch 720/1000\n",
      "11/11 - 0s - loss: 0.2547 - mse: 0.2547\n",
      "Epoch 721/1000\n",
      "11/11 - 0s - loss: 0.2546 - mse: 0.2546\n",
      "Epoch 722/1000\n",
      "11/11 - 0s - loss: 0.2545 - mse: 0.2545\n",
      "Epoch 723/1000\n",
      "11/11 - 0s - loss: 0.2544 - mse: 0.2544\n",
      "Epoch 724/1000\n",
      "11/11 - 0s - loss: 0.2544 - mse: 0.2544\n",
      "Epoch 725/1000\n",
      "11/11 - 0s - loss: 0.2543 - mse: 0.2543\n",
      "Epoch 726/1000\n",
      "11/11 - 0s - loss: 0.2542 - mse: 0.2542\n",
      "Epoch 727/1000\n",
      "11/11 - 0s - loss: 0.2541 - mse: 0.2541\n",
      "Epoch 728/1000\n",
      "11/11 - 0s - loss: 0.2540 - mse: 0.2540\n",
      "Epoch 729/1000\n",
      "11/11 - 0s - loss: 0.2539 - mse: 0.2539\n",
      "Epoch 730/1000\n",
      "11/11 - 0s - loss: 0.2538 - mse: 0.2538\n",
      "Epoch 731/1000\n",
      "11/11 - 0s - loss: 0.2538 - mse: 0.2538\n",
      "Epoch 732/1000\n",
      "11/11 - 0s - loss: 0.2537 - mse: 0.2537\n",
      "Epoch 733/1000\n",
      "11/11 - 0s - loss: 0.2536 - mse: 0.2536\n",
      "Epoch 734/1000\n",
      "11/11 - 0s - loss: 0.2535 - mse: 0.2535\n",
      "Epoch 735/1000\n",
      "11/11 - 0s - loss: 0.2534 - mse: 0.2534\n",
      "Epoch 736/1000\n",
      "11/11 - 0s - loss: 0.2533 - mse: 0.2533\n",
      "Epoch 737/1000\n",
      "11/11 - 0s - loss: 0.2533 - mse: 0.2533\n",
      "Epoch 738/1000\n",
      "11/11 - 0s - loss: 0.2532 - mse: 0.2532\n",
      "Epoch 739/1000\n",
      "11/11 - 0s - loss: 0.2531 - mse: 0.2531\n",
      "Epoch 740/1000\n",
      "11/11 - 0s - loss: 0.2530 - mse: 0.2530\n",
      "Epoch 741/1000\n",
      "11/11 - 0s - loss: 0.2530 - mse: 0.2530\n",
      "Epoch 742/1000\n",
      "11/11 - 0s - loss: 0.2529 - mse: 0.2529\n",
      "Epoch 743/1000\n",
      "11/11 - 0s - loss: 0.2528 - mse: 0.2528\n",
      "Epoch 744/1000\n",
      "11/11 - 0s - loss: 0.2527 - mse: 0.2527\n",
      "Epoch 745/1000\n",
      "11/11 - 0s - loss: 0.2526 - mse: 0.2526\n",
      "Epoch 746/1000\n",
      "11/11 - 0s - loss: 0.2526 - mse: 0.2526\n",
      "Epoch 747/1000\n",
      "11/11 - 0s - loss: 0.2525 - mse: 0.2525\n",
      "Epoch 748/1000\n",
      "11/11 - 0s - loss: 0.2524 - mse: 0.2524\n",
      "Epoch 749/1000\n",
      "11/11 - 0s - loss: 0.2524 - mse: 0.2524\n",
      "Epoch 750/1000\n",
      "11/11 - 0s - loss: 0.2523 - mse: 0.2523\n",
      "Epoch 751/1000\n",
      "11/11 - 0s - loss: 0.2522 - mse: 0.2522\n",
      "Epoch 752/1000\n",
      "11/11 - 0s - loss: 0.2521 - mse: 0.2521\n",
      "Epoch 753/1000\n",
      "11/11 - 0s - loss: 0.2521 - mse: 0.2521\n",
      "Epoch 754/1000\n",
      "11/11 - 0s - loss: 0.2520 - mse: 0.2520\n",
      "Epoch 755/1000\n",
      "11/11 - 0s - loss: 0.2519 - mse: 0.2519\n",
      "Epoch 756/1000\n",
      "11/11 - 0s - loss: 0.2519 - mse: 0.2519\n",
      "Epoch 757/1000\n",
      "11/11 - 0s - loss: 0.2518 - mse: 0.2518\n",
      "Epoch 758/1000\n",
      "11/11 - 0s - loss: 0.2517 - mse: 0.2517\n",
      "Epoch 759/1000\n",
      "11/11 - 0s - loss: 0.2517 - mse: 0.2517\n",
      "Epoch 760/1000\n",
      "11/11 - 0s - loss: 0.2516 - mse: 0.2516\n",
      "Epoch 761/1000\n",
      "11/11 - 0s - loss: 0.2515 - mse: 0.2515\n",
      "Epoch 762/1000\n",
      "11/11 - 0s - loss: 0.2515 - mse: 0.2515\n",
      "Epoch 763/1000\n",
      "11/11 - 0s - loss: 0.2514 - mse: 0.2514\n",
      "Epoch 764/1000\n",
      "11/11 - 0s - loss: 0.2513 - mse: 0.2513\n",
      "Epoch 765/1000\n",
      "11/11 - 0s - loss: 0.2513 - mse: 0.2513\n",
      "Epoch 766/1000\n",
      "11/11 - 0s - loss: 0.2512 - mse: 0.2512\n",
      "Epoch 767/1000\n",
      "11/11 - 0s - loss: 0.2511 - mse: 0.2511\n",
      "Epoch 768/1000\n",
      "11/11 - 0s - loss: 0.2511 - mse: 0.2511\n",
      "Epoch 769/1000\n",
      "11/11 - 0s - loss: 0.2510 - mse: 0.2510\n",
      "Epoch 770/1000\n",
      "11/11 - 0s - loss: 0.2510 - mse: 0.2510\n",
      "Epoch 771/1000\n",
      "11/11 - 0s - loss: 0.2509 - mse: 0.2509\n",
      "Epoch 772/1000\n",
      "11/11 - 0s - loss: 0.2508 - mse: 0.2508\n",
      "Epoch 773/1000\n",
      "11/11 - 0s - loss: 0.2508 - mse: 0.2508\n",
      "Epoch 774/1000\n",
      "11/11 - 0s - loss: 0.2507 - mse: 0.2507\n",
      "Epoch 775/1000\n",
      "11/11 - 0s - loss: 0.2507 - mse: 0.2507\n",
      "Epoch 776/1000\n",
      "11/11 - 0s - loss: 0.2506 - mse: 0.2506\n",
      "Epoch 777/1000\n",
      "11/11 - 0s - loss: 0.2506 - mse: 0.2506\n",
      "Epoch 778/1000\n",
      "11/11 - 0s - loss: 0.2505 - mse: 0.2505\n",
      "Epoch 779/1000\n",
      "11/11 - 0s - loss: 0.2505 - mse: 0.2505\n",
      "Epoch 780/1000\n",
      "11/11 - 0s - loss: 0.2504 - mse: 0.2504\n",
      "Epoch 781/1000\n",
      "11/11 - 0s - loss: 0.2503 - mse: 0.2503\n",
      "Epoch 782/1000\n",
      "11/11 - 0s - loss: 0.2503 - mse: 0.2503\n",
      "Epoch 783/1000\n",
      "11/11 - 0s - loss: 0.2502 - mse: 0.2502\n",
      "Epoch 784/1000\n",
      "11/11 - 0s - loss: 0.2502 - mse: 0.2502\n",
      "Epoch 785/1000\n",
      "11/11 - 0s - loss: 0.2501 - mse: 0.2501\n",
      "Epoch 786/1000\n",
      "11/11 - 0s - loss: 0.2501 - mse: 0.2501\n",
      "Epoch 787/1000\n",
      "11/11 - 0s - loss: 0.2500 - mse: 0.2500\n",
      "Epoch 788/1000\n",
      "11/11 - 0s - loss: 0.2500 - mse: 0.2500\n",
      "Epoch 789/1000\n",
      "11/11 - 0s - loss: 0.2499 - mse: 0.2499\n",
      "Epoch 790/1000\n",
      "11/11 - 0s - loss: 0.2499 - mse: 0.2499\n",
      "Epoch 791/1000\n",
      "11/11 - 0s - loss: 0.2498 - mse: 0.2498\n",
      "Epoch 792/1000\n",
      "11/11 - 0s - loss: 0.2498 - mse: 0.2498\n",
      "Epoch 793/1000\n",
      "11/11 - 0s - loss: 0.2497 - mse: 0.2497\n",
      "Epoch 794/1000\n",
      "11/11 - 0s - loss: 0.2497 - mse: 0.2497\n",
      "Epoch 795/1000\n",
      "11/11 - 0s - loss: 0.2496 - mse: 0.2496\n",
      "Epoch 796/1000\n",
      "11/11 - 0s - loss: 0.2496 - mse: 0.2496\n",
      "Epoch 797/1000\n",
      "11/11 - 0s - loss: 0.2496 - mse: 0.2496\n",
      "Epoch 798/1000\n",
      "11/11 - 0s - loss: 0.2495 - mse: 0.2495\n",
      "Epoch 799/1000\n",
      "11/11 - 0s - loss: 0.2495 - mse: 0.2495\n",
      "Epoch 800/1000\n",
      "11/11 - 0s - loss: 0.2494 - mse: 0.2494\n",
      "Epoch 801/1000\n",
      "11/11 - 0s - loss: 0.2494 - mse: 0.2494\n",
      "Epoch 802/1000\n",
      "11/11 - 0s - loss: 0.2493 - mse: 0.2493\n",
      "Epoch 803/1000\n",
      "11/11 - 0s - loss: 0.2493 - mse: 0.2493\n",
      "Epoch 804/1000\n",
      "11/11 - 0s - loss: 0.2493 - mse: 0.2493\n",
      "Epoch 805/1000\n",
      "11/11 - 0s - loss: 0.2492 - mse: 0.2492\n",
      "Epoch 806/1000\n",
      "11/11 - 0s - loss: 0.2492 - mse: 0.2492\n",
      "Epoch 807/1000\n",
      "11/11 - 0s - loss: 0.2491 - mse: 0.2491\n",
      "Epoch 808/1000\n",
      "11/11 - 0s - loss: 0.2491 - mse: 0.2491\n",
      "Epoch 809/1000\n",
      "11/11 - 0s - loss: 0.2491 - mse: 0.2491\n",
      "Epoch 810/1000\n",
      "11/11 - 0s - loss: 0.2490 - mse: 0.2490\n",
      "Epoch 811/1000\n",
      "11/11 - 0s - loss: 0.2490 - mse: 0.2490\n",
      "Epoch 812/1000\n",
      "11/11 - 0s - loss: 0.2489 - mse: 0.2489\n",
      "Epoch 813/1000\n",
      "11/11 - 0s - loss: 0.2489 - mse: 0.2489\n",
      "Epoch 814/1000\n",
      "11/11 - 0s - loss: 0.2489 - mse: 0.2489\n",
      "Epoch 815/1000\n",
      "11/11 - 0s - loss: 0.2488 - mse: 0.2488\n",
      "Epoch 816/1000\n",
      "11/11 - 0s - loss: 0.2488 - mse: 0.2488\n",
      "Epoch 817/1000\n",
      "11/11 - 0s - loss: 0.2488 - mse: 0.2488\n",
      "Epoch 818/1000\n",
      "11/11 - 0s - loss: 0.2487 - mse: 0.2487\n",
      "Epoch 819/1000\n",
      "11/11 - 0s - loss: 0.2487 - mse: 0.2487\n",
      "Epoch 820/1000\n",
      "11/11 - 0s - loss: 0.2487 - mse: 0.2487\n",
      "Epoch 821/1000\n",
      "11/11 - 0s - loss: 0.2486 - mse: 0.2486\n",
      "Epoch 822/1000\n",
      "11/11 - 0s - loss: 0.2486 - mse: 0.2486\n",
      "Epoch 823/1000\n",
      "11/11 - 0s - loss: 0.2486 - mse: 0.2486\n",
      "Epoch 824/1000\n",
      "11/11 - 0s - loss: 0.2485 - mse: 0.2485\n",
      "Epoch 825/1000\n",
      "11/11 - 0s - loss: 0.2485 - mse: 0.2485\n",
      "Epoch 826/1000\n",
      "11/11 - 0s - loss: 0.2485 - mse: 0.2485\n",
      "Epoch 827/1000\n",
      "11/11 - 0s - loss: 0.2484 - mse: 0.2484\n",
      "Epoch 828/1000\n",
      "11/11 - 0s - loss: 0.2484 - mse: 0.2484\n",
      "Epoch 829/1000\n",
      "11/11 - 0s - loss: 0.2484 - mse: 0.2484\n",
      "Epoch 830/1000\n",
      "11/11 - 0s - loss: 0.2483 - mse: 0.2483\n",
      "Epoch 831/1000\n",
      "11/11 - 0s - loss: 0.2483 - mse: 0.2483\n",
      "Epoch 832/1000\n",
      "11/11 - 0s - loss: 0.2483 - mse: 0.2483\n",
      "Epoch 833/1000\n",
      "11/11 - 0s - loss: 0.2483 - mse: 0.2483\n",
      "Epoch 834/1000\n",
      "11/11 - 0s - loss: 0.2482 - mse: 0.2482\n",
      "Epoch 835/1000\n",
      "11/11 - 0s - loss: 0.2482 - mse: 0.2482\n",
      "Epoch 836/1000\n",
      "11/11 - 0s - loss: 0.2482 - mse: 0.2482\n",
      "Epoch 837/1000\n",
      "11/11 - 0s - loss: 0.2482 - mse: 0.2482\n",
      "Epoch 838/1000\n",
      "11/11 - 0s - loss: 0.2481 - mse: 0.2481\n",
      "Epoch 839/1000\n",
      "11/11 - 0s - loss: 0.2481 - mse: 0.2481\n",
      "Epoch 840/1000\n",
      "11/11 - 0s - loss: 0.2481 - mse: 0.2481\n",
      "Epoch 841/1000\n",
      "11/11 - 0s - loss: 0.2481 - mse: 0.2481\n",
      "Epoch 842/1000\n",
      "11/11 - 0s - loss: 0.2480 - mse: 0.2480\n",
      "Epoch 843/1000\n",
      "11/11 - 0s - loss: 0.2480 - mse: 0.2480\n",
      "Epoch 844/1000\n",
      "11/11 - 0s - loss: 0.2480 - mse: 0.2480\n",
      "Epoch 845/1000\n",
      "11/11 - 0s - loss: 0.2480 - mse: 0.2480\n",
      "Epoch 846/1000\n",
      "11/11 - 0s - loss: 0.2479 - mse: 0.2479\n",
      "Epoch 847/1000\n",
      "11/11 - 0s - loss: 0.2479 - mse: 0.2479\n",
      "Epoch 848/1000\n",
      "11/11 - 0s - loss: 0.2479 - mse: 0.2479\n",
      "Epoch 849/1000\n",
      "11/11 - 0s - loss: 0.2479 - mse: 0.2479\n",
      "Epoch 850/1000\n",
      "11/11 - 0s - loss: 0.2478 - mse: 0.2478\n",
      "Epoch 851/1000\n",
      "11/11 - 0s - loss: 0.2478 - mse: 0.2478\n",
      "Epoch 852/1000\n",
      "11/11 - 0s - loss: 0.2478 - mse: 0.2478\n",
      "Epoch 853/1000\n",
      "11/11 - 0s - loss: 0.2478 - mse: 0.2478\n",
      "Epoch 854/1000\n",
      "11/11 - 0s - loss: 0.2478 - mse: 0.2478\n",
      "Epoch 855/1000\n",
      "11/11 - 0s - loss: 0.2477 - mse: 0.2477\n",
      "Epoch 856/1000\n",
      "11/11 - 0s - loss: 0.2477 - mse: 0.2477\n",
      "Epoch 857/1000\n",
      "11/11 - 0s - loss: 0.2477 - mse: 0.2477\n",
      "Epoch 858/1000\n",
      "11/11 - 0s - loss: 0.2477 - mse: 0.2477\n",
      "Epoch 859/1000\n",
      "11/11 - 0s - loss: 0.2477 - mse: 0.2477\n",
      "Epoch 860/1000\n",
      "11/11 - 0s - loss: 0.2476 - mse: 0.2476\n",
      "Epoch 861/1000\n",
      "11/11 - 0s - loss: 0.2476 - mse: 0.2476\n",
      "Epoch 862/1000\n",
      "11/11 - 0s - loss: 0.2476 - mse: 0.2476\n",
      "Epoch 863/1000\n",
      "11/11 - 0s - loss: 0.2476 - mse: 0.2476\n",
      "Epoch 864/1000\n",
      "11/11 - 0s - loss: 0.2476 - mse: 0.2476\n",
      "Epoch 865/1000\n",
      "11/11 - 0s - loss: 0.2476 - mse: 0.2476\n",
      "Epoch 866/1000\n",
      "11/11 - 0s - loss: 0.2475 - mse: 0.2475\n",
      "Epoch 867/1000\n",
      "11/11 - 0s - loss: 0.2475 - mse: 0.2475\n",
      "Epoch 868/1000\n",
      "11/11 - 0s - loss: 0.2475 - mse: 0.2475\n",
      "Epoch 869/1000\n",
      "11/11 - 0s - loss: 0.2475 - mse: 0.2475\n",
      "Epoch 870/1000\n",
      "11/11 - 0s - loss: 0.2475 - mse: 0.2475\n",
      "Epoch 871/1000\n",
      "11/11 - 0s - loss: 0.2475 - mse: 0.2475\n",
      "Epoch 872/1000\n",
      "11/11 - 0s - loss: 0.2474 - mse: 0.2474\n",
      "Epoch 873/1000\n",
      "11/11 - 0s - loss: 0.2474 - mse: 0.2474\n",
      "Epoch 874/1000\n",
      "11/11 - 0s - loss: 0.2474 - mse: 0.2474\n",
      "Epoch 875/1000\n",
      "11/11 - 0s - loss: 0.2474 - mse: 0.2474\n",
      "Epoch 876/1000\n",
      "11/11 - 0s - loss: 0.2474 - mse: 0.2474\n",
      "Epoch 877/1000\n",
      "11/11 - 0s - loss: 0.2474 - mse: 0.2474\n",
      "Epoch 878/1000\n",
      "11/11 - 0s - loss: 0.2474 - mse: 0.2474\n",
      "Epoch 879/1000\n",
      "11/11 - 0s - loss: 0.2473 - mse: 0.2473\n",
      "Epoch 880/1000\n",
      "11/11 - 0s - loss: 0.2473 - mse: 0.2473\n",
      "Epoch 881/1000\n",
      "11/11 - 0s - loss: 0.2473 - mse: 0.2473\n",
      "Epoch 882/1000\n",
      "11/11 - 0s - loss: 0.2473 - mse: 0.2473\n",
      "Epoch 883/1000\n",
      "11/11 - 0s - loss: 0.2473 - mse: 0.2473\n",
      "Epoch 884/1000\n",
      "11/11 - 0s - loss: 0.2473 - mse: 0.2473\n",
      "Epoch 885/1000\n",
      "11/11 - 0s - loss: 0.2473 - mse: 0.2473\n",
      "Epoch 886/1000\n",
      "11/11 - 0s - loss: 0.2473 - mse: 0.2473\n",
      "Epoch 887/1000\n",
      "11/11 - 0s - loss: 0.2472 - mse: 0.2472\n",
      "Epoch 888/1000\n",
      "11/11 - 0s - loss: 0.2472 - mse: 0.2472\n",
      "Epoch 889/1000\n",
      "11/11 - 0s - loss: 0.2472 - mse: 0.2472\n",
      "Epoch 890/1000\n",
      "11/11 - 0s - loss: 0.2472 - mse: 0.2472\n",
      "Epoch 891/1000\n",
      "11/11 - 0s - loss: 0.2472 - mse: 0.2472\n",
      "Epoch 892/1000\n",
      "11/11 - 0s - loss: 0.2472 - mse: 0.2472\n",
      "Epoch 893/1000\n",
      "11/11 - 0s - loss: 0.2472 - mse: 0.2472\n",
      "Epoch 894/1000\n",
      "11/11 - 0s - loss: 0.2472 - mse: 0.2472\n",
      "Epoch 895/1000\n",
      "11/11 - 0s - loss: 0.2472 - mse: 0.2472\n",
      "Epoch 896/1000\n",
      "11/11 - 0s - loss: 0.2472 - mse: 0.2472\n",
      "Epoch 897/1000\n",
      "11/11 - 0s - loss: 0.2471 - mse: 0.2471\n",
      "Epoch 898/1000\n",
      "11/11 - 0s - loss: 0.2471 - mse: 0.2471\n",
      "Epoch 899/1000\n",
      "11/11 - 0s - loss: 0.2471 - mse: 0.2471\n",
      "Epoch 900/1000\n",
      "11/11 - 0s - loss: 0.2471 - mse: 0.2471\n",
      "Epoch 901/1000\n",
      "11/11 - 0s - loss: 0.2471 - mse: 0.2471\n",
      "Epoch 902/1000\n",
      "11/11 - 0s - loss: 0.2471 - mse: 0.2471\n",
      "Epoch 903/1000\n",
      "11/11 - 0s - loss: 0.2471 - mse: 0.2471\n",
      "Epoch 904/1000\n",
      "11/11 - 0s - loss: 0.2471 - mse: 0.2471\n",
      "Epoch 905/1000\n",
      "11/11 - 0s - loss: 0.2471 - mse: 0.2471\n",
      "Epoch 906/1000\n",
      "11/11 - 0s - loss: 0.2471 - mse: 0.2471\n",
      "Epoch 907/1000\n",
      "11/11 - 0s - loss: 0.2471 - mse: 0.2471\n",
      "Epoch 908/1000\n",
      "11/11 - 0s - loss: 0.2471 - mse: 0.2471\n",
      "Epoch 909/1000\n",
      "11/11 - 0s - loss: 0.2470 - mse: 0.2470\n",
      "Epoch 910/1000\n",
      "11/11 - 0s - loss: 0.2470 - mse: 0.2470\n",
      "Epoch 911/1000\n",
      "11/11 - 0s - loss: 0.2470 - mse: 0.2470\n",
      "Epoch 912/1000\n",
      "11/11 - 0s - loss: 0.2470 - mse: 0.2470\n",
      "Epoch 913/1000\n",
      "11/11 - 0s - loss: 0.2470 - mse: 0.2470\n",
      "Epoch 914/1000\n",
      "11/11 - 0s - loss: 0.2470 - mse: 0.2470\n",
      "Epoch 915/1000\n",
      "11/11 - 0s - loss: 0.2470 - mse: 0.2470\n",
      "Epoch 916/1000\n",
      "11/11 - 0s - loss: 0.2470 - mse: 0.2470\n",
      "Epoch 917/1000\n",
      "11/11 - 0s - loss: 0.2470 - mse: 0.2470\n",
      "Epoch 918/1000\n",
      "11/11 - 0s - loss: 0.2470 - mse: 0.2470\n",
      "Epoch 919/1000\n",
      "11/11 - 0s - loss: 0.2470 - mse: 0.2470\n",
      "Epoch 920/1000\n",
      "11/11 - 0s - loss: 0.2470 - mse: 0.2470\n",
      "Epoch 921/1000\n",
      "11/11 - 0s - loss: 0.2470 - mse: 0.2470\n",
      "Epoch 922/1000\n",
      "11/11 - 0s - loss: 0.2470 - mse: 0.2470\n",
      "Epoch 923/1000\n",
      "11/11 - 0s - loss: 0.2470 - mse: 0.2470\n",
      "Epoch 924/1000\n",
      "11/11 - 0s - loss: 0.2470 - mse: 0.2470\n",
      "Epoch 925/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 926/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 927/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 928/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 929/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 930/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 931/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 932/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 933/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 934/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 935/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 936/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 937/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 938/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 939/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 940/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 941/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 942/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 943/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 944/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 945/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 946/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 947/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 948/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 949/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 950/1000\n",
      "11/11 - 0s - loss: 0.2469 - mse: 0.2469\n",
      "Epoch 951/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 952/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 953/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 954/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 955/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 956/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 957/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 958/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 959/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 960/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 961/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 962/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 963/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 964/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 965/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 966/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 967/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 968/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 969/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 970/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 971/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 972/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 973/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 974/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 975/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 976/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 977/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 978/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 979/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 980/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 981/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 982/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 983/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 984/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 985/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 986/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 987/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 988/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 989/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 990/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 991/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 992/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 993/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 994/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 995/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 996/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 997/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 998/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 999/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n",
      "Epoch 1000/1000\n",
      "11/11 - 0s - loss: 0.2468 - mse: 0.2468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x215890c2d60>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the data \n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=x_train.shape[1], activation='linear'))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "model.fit(x_train,y_train, batch_size=int(x_train.shape[0]/10), epochs=1000, shuffle=False, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 460us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2764385228250033"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "# calculate MSE\n",
    "y_pred = model.predict(x_test,verbose=1)\n",
    "mean_squared_error(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.4176197 ],\n",
       "        [ 0.87356246],\n",
       "        [ 0.08387318],\n",
       "        [-2.996035  ],\n",
       "        [ 2.1908495 ],\n",
       "        [-1.2839789 ]], dtype=float32),\n",
       " array([-0.03909401], dtype=float32)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine coefs and make sure they look right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Add Regularization\n",
    "## Steps:\n",
    " - ### Make a function that given a `weight_factor` returns a well-formed keras regularization function that penalizes the variance of coefficients by this factor. \n",
    " - ### Fit a model with this regularizer with a `weight_factor` of `1.0`. \n",
    " - ### Calculate the `mse` on the test set and examine the coefficients. \n",
    "\n",
    "__Note__: the coefficients should be closer together. Are they?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reg_function(wt_factor):\n",
    "    def my_reg(weight_matrix):\n",
    "        return tf.reduce_mean(tf.square(tf.subtract(weight_matrix,tf.reduce_mean(weight_matrix))))*wt_factor\n",
    "    return my_reg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "11/11 - 0s - loss: 39.8541 - mse: 39.6465\n",
      "Epoch 2/1000\n",
      "11/11 - 0s - loss: 38.8056 - mse: 38.5925\n",
      "Epoch 3/1000\n",
      "11/11 - 0s - loss: 37.7920 - mse: 37.5732\n",
      "Epoch 4/1000\n",
      "11/11 - 0s - loss: 36.8124 - mse: 36.5878\n",
      "Epoch 5/1000\n",
      "11/11 - 0s - loss: 35.8671 - mse: 35.6366\n",
      "Epoch 6/1000\n",
      "11/11 - 0s - loss: 34.9562 - mse: 34.7197\n",
      "Epoch 7/1000\n",
      "11/11 - 0s - loss: 34.0789 - mse: 33.8364\n",
      "Epoch 8/1000\n",
      "11/11 - 0s - loss: 33.2344 - mse: 32.9858\n",
      "Epoch 9/1000\n",
      "11/11 - 0s - loss: 32.4217 - mse: 32.1669\n",
      "Epoch 10/1000\n",
      "11/11 - 0s - loss: 31.6397 - mse: 31.3789\n",
      "Epoch 11/1000\n",
      "11/11 - 0s - loss: 30.8875 - mse: 30.6205\n",
      "Epoch 12/1000\n",
      "11/11 - 0s - loss: 30.1638 - mse: 29.8907\n",
      "Epoch 13/1000\n",
      "11/11 - 0s - loss: 29.4676 - mse: 29.1884\n",
      "Epoch 14/1000\n",
      "11/11 - 0s - loss: 28.7978 - mse: 28.5125\n",
      "Epoch 15/1000\n",
      "11/11 - 0s - loss: 28.1532 - mse: 27.8619\n",
      "Epoch 16/1000\n",
      "11/11 - 0s - loss: 27.5326 - mse: 27.2354\n",
      "Epoch 17/1000\n",
      "11/11 - 0s - loss: 26.9350 - mse: 26.6319\n",
      "Epoch 18/1000\n",
      "11/11 - 0s - loss: 26.3592 - mse: 26.0504\n",
      "Epoch 19/1000\n",
      "11/11 - 0s - loss: 25.8041 - mse: 25.4896\n",
      "Epoch 20/1000\n",
      "11/11 - 0s - loss: 25.2687 - mse: 24.9486\n",
      "Epoch 21/1000\n",
      "11/11 - 0s - loss: 24.7518 - mse: 24.4264\n",
      "Epoch 22/1000\n",
      "11/11 - 0s - loss: 24.2525 - mse: 23.9218\n",
      "Epoch 23/1000\n",
      "11/11 - 0s - loss: 23.7698 - mse: 23.4339\n",
      "Epoch 24/1000\n",
      "11/11 - 0s - loss: 23.3029 - mse: 22.9619\n",
      "Epoch 25/1000\n",
      "11/11 - 0s - loss: 22.8509 - mse: 22.5049\n",
      "Epoch 26/1000\n",
      "11/11 - 0s - loss: 22.4130 - mse: 22.0621\n",
      "Epoch 27/1000\n",
      "11/11 - 0s - loss: 21.9884 - mse: 21.6327\n",
      "Epoch 28/1000\n",
      "11/11 - 0s - loss: 21.5767 - mse: 21.2162\n",
      "Epoch 29/1000\n",
      "11/11 - 0s - loss: 21.1770 - mse: 20.8119\n",
      "Epoch 30/1000\n",
      "11/11 - 0s - loss: 20.7890 - mse: 20.4192\n",
      "Epoch 31/1000\n",
      "11/11 - 0s - loss: 20.4120 - mse: 20.0377\n",
      "Epoch 32/1000\n",
      "11/11 - 0s - loss: 20.0457 - mse: 19.6669\n",
      "Epoch 33/1000\n",
      "11/11 - 0s - loss: 19.6897 - mse: 19.3063\n",
      "Epoch 34/1000\n",
      "11/11 - 0s - loss: 19.3435 - mse: 18.9556\n",
      "Epoch 35/1000\n",
      "11/11 - 0s - loss: 19.0068 - mse: 18.6144\n",
      "Epoch 36/1000\n",
      "11/11 - 0s - loss: 18.6793 - mse: 18.2824\n",
      "Epoch 37/1000\n",
      "11/11 - 0s - loss: 18.3607 - mse: 17.9593\n",
      "Epoch 38/1000\n",
      "11/11 - 0s - loss: 18.0506 - mse: 17.6447\n",
      "Epoch 39/1000\n",
      "11/11 - 0s - loss: 17.7490 - mse: 17.3385\n",
      "Epoch 40/1000\n",
      "11/11 - 0s - loss: 17.4554 - mse: 17.0403\n",
      "Epoch 41/1000\n",
      "11/11 - 0s - loss: 17.1696 - mse: 16.7499\n",
      "Epoch 42/1000\n",
      "11/11 - 0s - loss: 16.8914 - mse: 16.4671\n",
      "Epoch 43/1000\n",
      "11/11 - 0s - loss: 16.6206 - mse: 16.1916\n",
      "Epoch 44/1000\n",
      "11/11 - 0s - loss: 16.3569 - mse: 15.9232\n",
      "Epoch 45/1000\n",
      "11/11 - 0s - loss: 16.1002 - mse: 15.6617\n",
      "Epoch 46/1000\n",
      "11/11 - 0s - loss: 15.8502 - mse: 15.4070\n",
      "Epoch 47/1000\n",
      "11/11 - 0s - loss: 15.6068 - mse: 15.1587\n",
      "Epoch 48/1000\n",
      "11/11 - 0s - loss: 15.3697 - mse: 14.9167\n",
      "Epoch 49/1000\n",
      "11/11 - 0s - loss: 15.1387 - mse: 14.6808\n",
      "Epoch 50/1000\n",
      "11/11 - 0s - loss: 14.9138 - mse: 14.4508\n",
      "Epoch 51/1000\n",
      "11/11 - 0s - loss: 14.6946 - mse: 14.2266\n",
      "Epoch 52/1000\n",
      "11/11 - 0s - loss: 14.4810 - mse: 14.0080\n",
      "Epoch 53/1000\n",
      "11/11 - 0s - loss: 14.2728 - mse: 13.7947\n",
      "Epoch 54/1000\n",
      "11/11 - 0s - loss: 14.0700 - mse: 13.5867\n",
      "Epoch 55/1000\n",
      "11/11 - 0s - loss: 13.8722 - mse: 13.3838\n",
      "Epoch 56/1000\n",
      "11/11 - 0s - loss: 13.6794 - mse: 13.1857\n",
      "Epoch 57/1000\n",
      "11/11 - 0s - loss: 13.4914 - mse: 12.9924\n",
      "Epoch 58/1000\n",
      "11/11 - 0s - loss: 13.3080 - mse: 12.8038\n",
      "Epoch 59/1000\n",
      "11/11 - 0s - loss: 13.1291 - mse: 12.6195\n",
      "Epoch 60/1000\n",
      "11/11 - 0s - loss: 12.9546 - mse: 12.4396\n",
      "Epoch 61/1000\n",
      "11/11 - 0s - loss: 12.7843 - mse: 12.2639\n",
      "Epoch 62/1000\n",
      "11/11 - 0s - loss: 12.6180 - mse: 12.0922\n",
      "Epoch 63/1000\n",
      "11/11 - 0s - loss: 12.4557 - mse: 11.9244\n",
      "Epoch 64/1000\n",
      "11/11 - 0s - loss: 12.2972 - mse: 11.7604\n",
      "Epoch 65/1000\n",
      "11/11 - 0s - loss: 12.1425 - mse: 11.6001\n",
      "Epoch 66/1000\n",
      "11/11 - 0s - loss: 11.9912 - mse: 11.4433\n",
      "Epoch 67/1000\n",
      "11/11 - 0s - loss: 11.8435 - mse: 11.2899\n",
      "Epoch 68/1000\n",
      "11/11 - 0s - loss: 11.6991 - mse: 11.1399\n",
      "Epoch 69/1000\n",
      "11/11 - 0s - loss: 11.5579 - mse: 10.9930\n",
      "Epoch 70/1000\n",
      "11/11 - 0s - loss: 11.4198 - mse: 10.8493\n",
      "Epoch 71/1000\n",
      "11/11 - 0s - loss: 11.2848 - mse: 10.7085\n",
      "Epoch 72/1000\n",
      "11/11 - 0s - loss: 11.1527 - mse: 10.5706\n",
      "Epoch 73/1000\n",
      "11/11 - 0s - loss: 11.0234 - mse: 10.4356\n",
      "Epoch 74/1000\n",
      "11/11 - 0s - loss: 10.8969 - mse: 10.3032\n",
      "Epoch 75/1000\n",
      "11/11 - 0s - loss: 10.7730 - mse: 10.1735\n",
      "Epoch 76/1000\n",
      "11/11 - 0s - loss: 10.6517 - mse: 10.0463\n",
      "Epoch 77/1000\n",
      "11/11 - 0s - loss: 10.5328 - mse: 9.9216\n",
      "Epoch 78/1000\n",
      "11/11 - 0s - loss: 10.4163 - mse: 9.7993\n",
      "Epoch 79/1000\n",
      "11/11 - 0s - loss: 10.3022 - mse: 9.6792\n",
      "Epoch 80/1000\n",
      "11/11 - 0s - loss: 10.1903 - mse: 9.5614\n",
      "Epoch 81/1000\n",
      "11/11 - 0s - loss: 10.0806 - mse: 9.4457\n",
      "Epoch 82/1000\n",
      "11/11 - 0s - loss: 9.9730 - mse: 9.3321\n",
      "Epoch 83/1000\n",
      "11/11 - 0s - loss: 9.8675 - mse: 9.2206\n",
      "Epoch 84/1000\n",
      "11/11 - 0s - loss: 9.7639 - mse: 9.1110\n",
      "Epoch 85/1000\n",
      "11/11 - 0s - loss: 9.6622 - mse: 9.0033\n",
      "Epoch 86/1000\n",
      "11/11 - 0s - loss: 9.5624 - mse: 8.8974\n",
      "Epoch 87/1000\n",
      "11/11 - 0s - loss: 9.4644 - mse: 8.7933\n",
      "Epoch 88/1000\n",
      "11/11 - 0s - loss: 9.3682 - mse: 8.6910\n",
      "Epoch 89/1000\n",
      "11/11 - 0s - loss: 9.2736 - mse: 8.5903\n",
      "Epoch 90/1000\n",
      "11/11 - 0s - loss: 9.1807 - mse: 8.4913\n",
      "Epoch 91/1000\n",
      "11/11 - 0s - loss: 9.0894 - mse: 8.3938\n",
      "Epoch 92/1000\n",
      "11/11 - 0s - loss: 8.9996 - mse: 8.2979\n",
      "Epoch 93/1000\n",
      "11/11 - 0s - loss: 8.9114 - mse: 8.2035\n",
      "Epoch 94/1000\n",
      "11/11 - 0s - loss: 8.8246 - mse: 8.1105\n",
      "Epoch 95/1000\n",
      "11/11 - 0s - loss: 8.7392 - mse: 8.0190\n",
      "Epoch 96/1000\n",
      "11/11 - 0s - loss: 8.6552 - mse: 7.9288\n",
      "Epoch 97/1000\n",
      "11/11 - 0s - loss: 8.5726 - mse: 7.8399\n",
      "Epoch 98/1000\n",
      "11/11 - 0s - loss: 8.4913 - mse: 7.7524\n",
      "Epoch 99/1000\n",
      "11/11 - 0s - loss: 8.4113 - mse: 7.6661\n",
      "Epoch 100/1000\n",
      "11/11 - 0s - loss: 8.3325 - mse: 7.5810\n",
      "Epoch 101/1000\n",
      "11/11 - 0s - loss: 8.2549 - mse: 7.4972\n",
      "Epoch 102/1000\n",
      "11/11 - 0s - loss: 8.1785 - mse: 7.4145\n",
      "Epoch 103/1000\n",
      "11/11 - 0s - loss: 8.1033 - mse: 7.3329\n",
      "Epoch 104/1000\n",
      "11/11 - 0s - loss: 8.0292 - mse: 7.2525\n",
      "Epoch 105/1000\n",
      "11/11 - 0s - loss: 7.9562 - mse: 7.1732\n",
      "Epoch 106/1000\n",
      "11/11 - 0s - loss: 7.8843 - mse: 7.0949\n",
      "Epoch 107/1000\n",
      "11/11 - 0s - loss: 7.8134 - mse: 7.0177\n",
      "Epoch 108/1000\n",
      "11/11 - 0s - loss: 7.7436 - mse: 6.9415\n",
      "Epoch 109/1000\n",
      "11/11 - 0s - loss: 7.6748 - mse: 6.8663\n",
      "Epoch 110/1000\n",
      "11/11 - 0s - loss: 7.6069 - mse: 6.7920\n",
      "Epoch 111/1000\n",
      "11/11 - 0s - loss: 7.5400 - mse: 6.7187\n",
      "Epoch 112/1000\n",
      "11/11 - 0s - loss: 7.4741 - mse: 6.6464\n",
      "Epoch 113/1000\n",
      "11/11 - 0s - loss: 7.4091 - mse: 6.5749\n",
      "Epoch 114/1000\n",
      "11/11 - 0s - loss: 7.3449 - mse: 6.5043\n",
      "Epoch 115/1000\n",
      "11/11 - 0s - loss: 7.2817 - mse: 6.4347\n",
      "Epoch 116/1000\n",
      "11/11 - 0s - loss: 7.2193 - mse: 6.3658\n",
      "Epoch 117/1000\n",
      "11/11 - 0s - loss: 7.1578 - mse: 6.2979\n",
      "Epoch 118/1000\n",
      "11/11 - 0s - loss: 7.0972 - mse: 6.2307\n",
      "Epoch 119/1000\n",
      "11/11 - 0s - loss: 7.0373 - mse: 6.1644\n",
      "Epoch 120/1000\n",
      "11/11 - 0s - loss: 6.9783 - mse: 6.0988\n",
      "Epoch 121/1000\n",
      "11/11 - 0s - loss: 6.9200 - mse: 6.0340\n",
      "Epoch 122/1000\n",
      "11/11 - 0s - loss: 6.8625 - mse: 5.9700\n",
      "Epoch 123/1000\n",
      "11/11 - 0s - loss: 6.8058 - mse: 5.9068\n",
      "Epoch 124/1000\n",
      "11/11 - 0s - loss: 6.7499 - mse: 5.8443\n",
      "Epoch 125/1000\n",
      "11/11 - 0s - loss: 6.6946 - mse: 5.7825\n",
      "Epoch 126/1000\n",
      "11/11 - 0s - loss: 6.6401 - mse: 5.7215\n",
      "Epoch 127/1000\n",
      "11/11 - 0s - loss: 6.5864 - mse: 5.6611\n",
      "Epoch 128/1000\n",
      "11/11 - 0s - loss: 6.5333 - mse: 5.6015\n",
      "Epoch 129/1000\n",
      "11/11 - 0s - loss: 6.4809 - mse: 5.5425\n",
      "Epoch 130/1000\n",
      "11/11 - 0s - loss: 6.4292 - mse: 5.4842\n",
      "Epoch 131/1000\n",
      "11/11 - 0s - loss: 6.3781 - mse: 5.4266\n",
      "Epoch 132/1000\n",
      "11/11 - 0s - loss: 6.3278 - mse: 5.3696\n",
      "Epoch 133/1000\n",
      "11/11 - 0s - loss: 6.2780 - mse: 5.3133\n",
      "Epoch 134/1000\n",
      "11/11 - 0s - loss: 6.2289 - mse: 5.2576\n",
      "Epoch 135/1000\n",
      "11/11 - 0s - loss: 6.1805 - mse: 5.2025\n",
      "Epoch 136/1000\n",
      "11/11 - 0s - loss: 6.1326 - mse: 5.1480\n",
      "Epoch 137/1000\n",
      "11/11 - 0s - loss: 6.0854 - mse: 5.0942\n",
      "Epoch 138/1000\n",
      "11/11 - 0s - loss: 6.0388 - mse: 5.0409\n",
      "Epoch 139/1000\n",
      "11/11 - 0s - loss: 5.9928 - mse: 4.9883\n",
      "Epoch 140/1000\n",
      "11/11 - 0s - loss: 5.9473 - mse: 4.9362\n",
      "Epoch 141/1000\n",
      "11/11 - 0s - loss: 5.9025 - mse: 4.8847\n",
      "Epoch 142/1000\n",
      "11/11 - 0s - loss: 5.8582 - mse: 4.8337\n",
      "Epoch 143/1000\n",
      "11/11 - 0s - loss: 5.8145 - mse: 4.7833\n",
      "Epoch 144/1000\n",
      "11/11 - 0s - loss: 5.7713 - mse: 4.7335\n",
      "Epoch 145/1000\n",
      "11/11 - 0s - loss: 5.7287 - mse: 4.6842\n",
      "Epoch 146/1000\n",
      "11/11 - 0s - loss: 5.6866 - mse: 4.6355\n",
      "Epoch 147/1000\n",
      "11/11 - 0s - loss: 5.6451 - mse: 4.5873\n",
      "Epoch 148/1000\n",
      "11/11 - 0s - loss: 5.6041 - mse: 4.5396\n",
      "Epoch 149/1000\n",
      "11/11 - 0s - loss: 5.5636 - mse: 4.4924\n",
      "Epoch 150/1000\n",
      "11/11 - 0s - loss: 5.5236 - mse: 4.4458\n",
      "Epoch 151/1000\n",
      "11/11 - 0s - loss: 5.4841 - mse: 4.3997\n",
      "Epoch 152/1000\n",
      "11/11 - 0s - loss: 5.4452 - mse: 4.3540\n",
      "Epoch 153/1000\n",
      "11/11 - 0s - loss: 5.4067 - mse: 4.3089\n",
      "Epoch 154/1000\n",
      "11/11 - 0s - loss: 5.3687 - mse: 4.2642\n",
      "Epoch 155/1000\n",
      "11/11 - 0s - loss: 5.3312 - mse: 4.2200\n",
      "Epoch 156/1000\n",
      "11/11 - 0s - loss: 5.2942 - mse: 4.1763\n",
      "Epoch 157/1000\n",
      "11/11 - 0s - loss: 5.2577 - mse: 4.1331\n",
      "Epoch 158/1000\n",
      "11/11 - 0s - loss: 5.2216 - mse: 4.0904\n",
      "Epoch 159/1000\n",
      "11/11 - 0s - loss: 5.1860 - mse: 4.0481\n",
      "Epoch 160/1000\n",
      "11/11 - 0s - loss: 5.1508 - mse: 4.0062\n",
      "Epoch 161/1000\n",
      "11/11 - 0s - loss: 5.1161 - mse: 3.9648\n",
      "Epoch 162/1000\n",
      "11/11 - 0s - loss: 5.0818 - mse: 3.9239\n",
      "Epoch 163/1000\n",
      "11/11 - 0s - loss: 5.0480 - mse: 3.8834\n",
      "Epoch 164/1000\n",
      "11/11 - 0s - loss: 5.0146 - mse: 3.8433\n",
      "Epoch 165/1000\n",
      "11/11 - 0s - loss: 4.9816 - mse: 3.8037\n",
      "Epoch 166/1000\n",
      "11/11 - 0s - loss: 4.9491 - mse: 3.7645\n",
      "Epoch 167/1000\n",
      "11/11 - 0s - loss: 4.9169 - mse: 3.7257\n",
      "Epoch 168/1000\n",
      "11/11 - 0s - loss: 4.8852 - mse: 3.6874\n",
      "Epoch 169/1000\n",
      "11/11 - 0s - loss: 4.8539 - mse: 3.6494\n",
      "Epoch 170/1000\n",
      "11/11 - 0s - loss: 4.8230 - mse: 3.6119\n",
      "Epoch 171/1000\n",
      "11/11 - 0s - loss: 4.7925 - mse: 3.5748\n",
      "Epoch 172/1000\n",
      "11/11 - 0s - loss: 4.7624 - mse: 3.5381\n",
      "Epoch 173/1000\n",
      "11/11 - 0s - loss: 4.7327 - mse: 3.5017\n",
      "Epoch 174/1000\n",
      "11/11 - 0s - loss: 4.7034 - mse: 3.4658\n",
      "Epoch 175/1000\n",
      "11/11 - 0s - loss: 4.6745 - mse: 3.4302\n",
      "Epoch 176/1000\n",
      "11/11 - 0s - loss: 4.6459 - mse: 3.3951\n",
      "Epoch 177/1000\n",
      "11/11 - 0s - loss: 4.6177 - mse: 3.3603\n",
      "Epoch 178/1000\n",
      "11/11 - 0s - loss: 4.5899 - mse: 3.3259\n",
      "Epoch 179/1000\n",
      "11/11 - 0s - loss: 4.5625 - mse: 3.2919\n",
      "Epoch 180/1000\n",
      "11/11 - 0s - loss: 4.5354 - mse: 3.2582\n",
      "Epoch 181/1000\n",
      "11/11 - 0s - loss: 4.5087 - mse: 3.2249\n",
      "Epoch 182/1000\n",
      "11/11 - 0s - loss: 4.4823 - mse: 3.1920\n",
      "Epoch 183/1000\n",
      "11/11 - 0s - loss: 4.4563 - mse: 3.1595\n",
      "Epoch 184/1000\n",
      "11/11 - 0s - loss: 4.4306 - mse: 3.1272\n",
      "Epoch 185/1000\n",
      "11/11 - 0s - loss: 4.4052 - mse: 3.0954\n",
      "Epoch 186/1000\n",
      "11/11 - 0s - loss: 4.3802 - mse: 3.0639\n",
      "Epoch 187/1000\n",
      "11/11 - 0s - loss: 4.3556 - mse: 3.0327\n",
      "Epoch 188/1000\n",
      "11/11 - 0s - loss: 4.3312 - mse: 3.0019\n",
      "Epoch 189/1000\n",
      "11/11 - 0s - loss: 4.3072 - mse: 2.9714\n",
      "Epoch 190/1000\n",
      "11/11 - 0s - loss: 4.2836 - mse: 2.9413\n",
      "Epoch 191/1000\n",
      "11/11 - 0s - loss: 4.2602 - mse: 2.9115\n",
      "Epoch 192/1000\n",
      "11/11 - 0s - loss: 4.2371 - mse: 2.8820\n",
      "Epoch 193/1000\n",
      "11/11 - 0s - loss: 4.2144 - mse: 2.8528\n",
      "Epoch 194/1000\n",
      "11/11 - 0s - loss: 4.1920 - mse: 2.8240\n",
      "Epoch 195/1000\n",
      "11/11 - 0s - loss: 4.1699 - mse: 2.7955\n",
      "Epoch 196/1000\n",
      "11/11 - 0s - loss: 4.1481 - mse: 2.7673\n",
      "Epoch 197/1000\n",
      "11/11 - 0s - loss: 4.1265 - mse: 2.7394\n",
      "Epoch 198/1000\n",
      "11/11 - 0s - loss: 4.1053 - mse: 2.7118\n",
      "Epoch 199/1000\n",
      "11/11 - 0s - loss: 4.0844 - mse: 2.6845\n",
      "Epoch 200/1000\n",
      "11/11 - 0s - loss: 4.0638 - mse: 2.6576\n",
      "Epoch 201/1000\n",
      "11/11 - 0s - loss: 4.0434 - mse: 2.6309\n",
      "Epoch 202/1000\n",
      "11/11 - 0s - loss: 4.0233 - mse: 2.6045\n",
      "Epoch 203/1000\n",
      "11/11 - 0s - loss: 4.0036 - mse: 2.5785\n",
      "Epoch 204/1000\n",
      "11/11 - 0s - loss: 3.9840 - mse: 2.5527\n",
      "Epoch 205/1000\n",
      "11/11 - 0s - loss: 3.9648 - mse: 2.5272\n",
      "Epoch 206/1000\n",
      "11/11 - 0s - loss: 3.9458 - mse: 2.5020\n",
      "Epoch 207/1000\n",
      "11/11 - 0s - loss: 3.9271 - mse: 2.4771\n",
      "Epoch 208/1000\n",
      "11/11 - 0s - loss: 3.9087 - mse: 2.4525\n",
      "Epoch 209/1000\n",
      "11/11 - 0s - loss: 3.8905 - mse: 2.4281\n",
      "Epoch 210/1000\n",
      "11/11 - 0s - loss: 3.8726 - mse: 2.4040\n",
      "Epoch 211/1000\n",
      "11/11 - 0s - loss: 3.8550 - mse: 2.3802\n",
      "Epoch 212/1000\n",
      "11/11 - 0s - loss: 3.8375 - mse: 2.3567\n",
      "Epoch 213/1000\n",
      "11/11 - 0s - loss: 3.8204 - mse: 2.3334\n",
      "Epoch 214/1000\n",
      "11/11 - 0s - loss: 3.8035 - mse: 2.3105\n",
      "Epoch 215/1000\n",
      "11/11 - 0s - loss: 3.7868 - mse: 2.2877\n",
      "Epoch 216/1000\n",
      "11/11 - 0s - loss: 3.7704 - mse: 2.2653\n",
      "Epoch 217/1000\n",
      "11/11 - 0s - loss: 3.7542 - mse: 2.2430\n",
      "Epoch 218/1000\n",
      "11/11 - 0s - loss: 3.7382 - mse: 2.2211\n",
      "Epoch 219/1000\n",
      "11/11 - 0s - loss: 3.7225 - mse: 2.1994\n",
      "Epoch 220/1000\n",
      "11/11 - 0s - loss: 3.7070 - mse: 2.1780\n",
      "Epoch 221/1000\n",
      "11/11 - 0s - loss: 3.6918 - mse: 2.1568\n",
      "Epoch 222/1000\n",
      "11/11 - 0s - loss: 3.6767 - mse: 2.1358\n",
      "Epoch 223/1000\n",
      "11/11 - 0s - loss: 3.6619 - mse: 2.1151\n",
      "Epoch 224/1000\n",
      "11/11 - 0s - loss: 3.6473 - mse: 2.0946\n",
      "Epoch 225/1000\n",
      "11/11 - 0s - loss: 3.6330 - mse: 2.0744\n",
      "Epoch 226/1000\n",
      "11/11 - 0s - loss: 3.6188 - mse: 2.0544\n",
      "Epoch 227/1000\n",
      "11/11 - 0s - loss: 3.6049 - mse: 2.0347\n",
      "Epoch 228/1000\n",
      "11/11 - 0s - loss: 3.5911 - mse: 2.0152\n",
      "Epoch 229/1000\n",
      "11/11 - 0s - loss: 3.5776 - mse: 1.9959\n",
      "Epoch 230/1000\n",
      "11/11 - 0s - loss: 3.5643 - mse: 1.9768\n",
      "Epoch 231/1000\n",
      "11/11 - 0s - loss: 3.5512 - mse: 1.9580\n",
      "Epoch 232/1000\n",
      "11/11 - 0s - loss: 3.5382 - mse: 1.9394\n",
      "Epoch 233/1000\n",
      "11/11 - 0s - loss: 3.5255 - mse: 1.9210\n",
      "Epoch 234/1000\n",
      "11/11 - 0s - loss: 3.5130 - mse: 1.9028\n",
      "Epoch 235/1000\n",
      "11/11 - 0s - loss: 3.5006 - mse: 1.8849\n",
      "Epoch 236/1000\n",
      "11/11 - 0s - loss: 3.4885 - mse: 1.8671\n",
      "Epoch 237/1000\n",
      "11/11 - 0s - loss: 3.4765 - mse: 1.8496\n",
      "Epoch 238/1000\n",
      "11/11 - 0s - loss: 3.4648 - mse: 1.8323\n",
      "Epoch 239/1000\n",
      "11/11 - 0s - loss: 3.4532 - mse: 1.8152\n",
      "Epoch 240/1000\n",
      "11/11 - 0s - loss: 3.4418 - mse: 1.7983\n",
      "Epoch 241/1000\n",
      "11/11 - 0s - loss: 3.4305 - mse: 1.7817\n",
      "Epoch 242/1000\n",
      "11/11 - 0s - loss: 3.4195 - mse: 1.7652\n",
      "Epoch 243/1000\n",
      "11/11 - 0s - loss: 3.4086 - mse: 1.7489\n",
      "Epoch 244/1000\n",
      "11/11 - 0s - loss: 3.3979 - mse: 1.7328\n",
      "Epoch 245/1000\n",
      "11/11 - 0s - loss: 3.3874 - mse: 1.7169\n",
      "Epoch 246/1000\n",
      "11/11 - 0s - loss: 3.3770 - mse: 1.7013\n",
      "Epoch 247/1000\n",
      "11/11 - 0s - loss: 3.3668 - mse: 1.6858\n",
      "Epoch 248/1000\n",
      "11/11 - 0s - loss: 3.3568 - mse: 1.6705\n",
      "Epoch 249/1000\n",
      "11/11 - 0s - loss: 3.3469 - mse: 1.6553\n",
      "Epoch 250/1000\n",
      "11/11 - 0s - loss: 3.3372 - mse: 1.6404\n",
      "Epoch 251/1000\n",
      "11/11 - 0s - loss: 3.3276 - mse: 1.6257\n",
      "Epoch 252/1000\n",
      "11/11 - 0s - loss: 3.3182 - mse: 1.6111\n",
      "Epoch 253/1000\n",
      "11/11 - 0s - loss: 3.3090 - mse: 1.5968\n",
      "Epoch 254/1000\n",
      "11/11 - 0s - loss: 3.2999 - mse: 1.5826\n",
      "Epoch 255/1000\n",
      "11/11 - 0s - loss: 3.2909 - mse: 1.5686\n",
      "Epoch 256/1000\n",
      "11/11 - 0s - loss: 3.2821 - mse: 1.5547\n",
      "Epoch 257/1000\n",
      "11/11 - 0s - loss: 3.2735 - mse: 1.5411\n",
      "Epoch 258/1000\n",
      "11/11 - 0s - loss: 3.2649 - mse: 1.5276\n",
      "Epoch 259/1000\n",
      "11/11 - 0s - loss: 3.2566 - mse: 1.5143\n",
      "Epoch 260/1000\n",
      "11/11 - 0s - loss: 3.2484 - mse: 1.5012\n",
      "Epoch 261/1000\n",
      "11/11 - 0s - loss: 3.2403 - mse: 1.4882\n",
      "Epoch 262/1000\n",
      "11/11 - 0s - loss: 3.2323 - mse: 1.4754\n",
      "Epoch 263/1000\n",
      "11/11 - 0s - loss: 3.2245 - mse: 1.4627\n",
      "Epoch 264/1000\n",
      "11/11 - 0s - loss: 3.2168 - mse: 1.4503\n",
      "Epoch 265/1000\n",
      "11/11 - 0s - loss: 3.2093 - mse: 1.4379\n",
      "Epoch 266/1000\n",
      "11/11 - 0s - loss: 3.2018 - mse: 1.4258\n",
      "Epoch 267/1000\n",
      "11/11 - 0s - loss: 3.1945 - mse: 1.4138\n",
      "Epoch 268/1000\n",
      "11/11 - 0s - loss: 3.1874 - mse: 1.4019\n",
      "Epoch 269/1000\n",
      "11/11 - 0s - loss: 3.1803 - mse: 1.3903\n",
      "Epoch 270/1000\n",
      "11/11 - 0s - loss: 3.1734 - mse: 1.3787\n",
      "Epoch 271/1000\n",
      "11/11 - 0s - loss: 3.1666 - mse: 1.3673\n",
      "Epoch 272/1000\n",
      "11/11 - 0s - loss: 3.1599 - mse: 1.3561\n",
      "Epoch 273/1000\n",
      "11/11 - 0s - loss: 3.1533 - mse: 1.3450\n",
      "Epoch 274/1000\n",
      "11/11 - 0s - loss: 3.1469 - mse: 1.3341\n",
      "Epoch 275/1000\n",
      "11/11 - 0s - loss: 3.1405 - mse: 1.3233\n",
      "Epoch 276/1000\n",
      "11/11 - 0s - loss: 3.1343 - mse: 1.3127\n",
      "Epoch 277/1000\n",
      "11/11 - 0s - loss: 3.1282 - mse: 1.3022\n",
      "Epoch 278/1000\n",
      "11/11 - 0s - loss: 3.1222 - mse: 1.2918\n",
      "Epoch 279/1000\n",
      "11/11 - 0s - loss: 3.1163 - mse: 1.2816\n",
      "Epoch 280/1000\n",
      "11/11 - 0s - loss: 3.1105 - mse: 1.2715\n",
      "Epoch 281/1000\n",
      "11/11 - 0s - loss: 3.1048 - mse: 1.2616\n",
      "Epoch 282/1000\n",
      "11/11 - 0s - loss: 3.0992 - mse: 1.2517\n",
      "Epoch 283/1000\n",
      "11/11 - 0s - loss: 3.0937 - mse: 1.2421\n",
      "Epoch 284/1000\n",
      "11/11 - 0s - loss: 3.0883 - mse: 1.2325\n",
      "Epoch 285/1000\n",
      "11/11 - 0s - loss: 3.0830 - mse: 1.2231\n",
      "Epoch 286/1000\n",
      "11/11 - 0s - loss: 3.0778 - mse: 1.2138\n",
      "Epoch 287/1000\n",
      "11/11 - 0s - loss: 3.0727 - mse: 1.2047\n",
      "Epoch 288/1000\n",
      "11/11 - 0s - loss: 3.0677 - mse: 1.1956\n",
      "Epoch 289/1000\n",
      "11/11 - 0s - loss: 3.0628 - mse: 1.1867\n",
      "Epoch 290/1000\n",
      "11/11 - 0s - loss: 3.0580 - mse: 1.1780\n",
      "Epoch 291/1000\n",
      "11/11 - 0s - loss: 3.0533 - mse: 1.1693\n",
      "Epoch 292/1000\n",
      "11/11 - 0s - loss: 3.0486 - mse: 1.1608\n",
      "Epoch 293/1000\n",
      "11/11 - 0s - loss: 3.0441 - mse: 1.1524\n",
      "Epoch 294/1000\n",
      "11/11 - 0s - loss: 3.0396 - mse: 1.1441\n",
      "Epoch 295/1000\n",
      "11/11 - 0s - loss: 3.0352 - mse: 1.1359\n",
      "Epoch 296/1000\n",
      "11/11 - 0s - loss: 3.0309 - mse: 1.1278\n",
      "Epoch 297/1000\n",
      "11/11 - 0s - loss: 3.0267 - mse: 1.1199\n",
      "Epoch 298/1000\n",
      "11/11 - 0s - loss: 3.0226 - mse: 1.1120\n",
      "Epoch 299/1000\n",
      "11/11 - 0s - loss: 3.0185 - mse: 1.1043\n",
      "Epoch 300/1000\n",
      "11/11 - 0s - loss: 3.0145 - mse: 1.0967\n",
      "Epoch 301/1000\n",
      "11/11 - 0s - loss: 3.0106 - mse: 1.0892\n",
      "Epoch 302/1000\n",
      "11/11 - 0s - loss: 3.0068 - mse: 1.0818\n",
      "Epoch 303/1000\n",
      "11/11 - 0s - loss: 3.0030 - mse: 1.0745\n",
      "Epoch 304/1000\n",
      "11/11 - 0s - loss: 2.9993 - mse: 1.0673\n",
      "Epoch 305/1000\n",
      "11/11 - 0s - loss: 2.9957 - mse: 1.0603\n",
      "Epoch 306/1000\n",
      "11/11 - 0s - loss: 2.9922 - mse: 1.0533\n",
      "Epoch 307/1000\n",
      "11/11 - 0s - loss: 2.9887 - mse: 1.0464\n",
      "Epoch 308/1000\n",
      "11/11 - 0s - loss: 2.9853 - mse: 1.0397\n",
      "Epoch 309/1000\n",
      "11/11 - 0s - loss: 2.9820 - mse: 1.0330\n",
      "Epoch 310/1000\n",
      "11/11 - 0s - loss: 2.9787 - mse: 1.0264\n",
      "Epoch 311/1000\n",
      "11/11 - 0s - loss: 2.9755 - mse: 1.0199\n",
      "Epoch 312/1000\n",
      "11/11 - 0s - loss: 2.9724 - mse: 1.0136\n",
      "Epoch 313/1000\n",
      "11/11 - 0s - loss: 2.9693 - mse: 1.0073\n",
      "Epoch 314/1000\n",
      "11/11 - 0s - loss: 2.9663 - mse: 1.0011\n",
      "Epoch 315/1000\n",
      "11/11 - 0s - loss: 2.9633 - mse: 0.9950\n",
      "Epoch 316/1000\n",
      "11/11 - 0s - loss: 2.9604 - mse: 0.9890\n",
      "Epoch 317/1000\n",
      "11/11 - 0s - loss: 2.9576 - mse: 0.9831\n",
      "Epoch 318/1000\n",
      "11/11 - 0s - loss: 2.9548 - mse: 0.9773\n",
      "Epoch 319/1000\n",
      "11/11 - 0s - loss: 2.9521 - mse: 0.9716\n",
      "Epoch 320/1000\n",
      "11/11 - 0s - loss: 2.9494 - mse: 0.9659\n",
      "Epoch 321/1000\n",
      "11/11 - 0s - loss: 2.9468 - mse: 0.9604\n",
      "Epoch 322/1000\n",
      "11/11 - 0s - loss: 2.9443 - mse: 0.9549\n",
      "Epoch 323/1000\n",
      "11/11 - 0s - loss: 2.9418 - mse: 0.9495\n",
      "Epoch 324/1000\n",
      "11/11 - 0s - loss: 2.9393 - mse: 0.9442\n",
      "Epoch 325/1000\n",
      "11/11 - 0s - loss: 2.9369 - mse: 0.9390\n",
      "Epoch 326/1000\n",
      "11/11 - 0s - loss: 2.9346 - mse: 0.9339\n",
      "Epoch 327/1000\n",
      "11/11 - 0s - loss: 2.9323 - mse: 0.9288\n",
      "Epoch 328/1000\n",
      "11/11 - 0s - loss: 2.9300 - mse: 0.9239\n",
      "Epoch 329/1000\n",
      "11/11 - 0s - loss: 2.9278 - mse: 0.9190\n",
      "Epoch 330/1000\n",
      "11/11 - 0s - loss: 2.9257 - mse: 0.9141\n",
      "Epoch 331/1000\n",
      "11/11 - 0s - loss: 2.9236 - mse: 0.9094\n",
      "Epoch 332/1000\n",
      "11/11 - 0s - loss: 2.9215 - mse: 0.9047\n",
      "Epoch 333/1000\n",
      "11/11 - 0s - loss: 2.9195 - mse: 0.9002\n",
      "Epoch 334/1000\n",
      "11/11 - 0s - loss: 2.9175 - mse: 0.8956\n",
      "Epoch 335/1000\n",
      "11/11 - 0s - loss: 2.9156 - mse: 0.8912\n",
      "Epoch 336/1000\n",
      "11/11 - 0s - loss: 2.9137 - mse: 0.8868\n",
      "Epoch 337/1000\n",
      "11/11 - 0s - loss: 2.9118 - mse: 0.8825\n",
      "Epoch 338/1000\n",
      "11/11 - 0s - loss: 2.9100 - mse: 0.8783\n",
      "Epoch 339/1000\n",
      "11/11 - 0s - loss: 2.9082 - mse: 0.8741\n",
      "Epoch 340/1000\n",
      "11/11 - 0s - loss: 2.9065 - mse: 0.8701\n",
      "Epoch 341/1000\n",
      "11/11 - 0s - loss: 2.9048 - mse: 0.8660\n",
      "Epoch 342/1000\n",
      "11/11 - 0s - loss: 2.9031 - mse: 0.8621\n",
      "Epoch 343/1000\n",
      "11/11 - 0s - loss: 2.9015 - mse: 0.8582\n",
      "Epoch 344/1000\n",
      "11/11 - 0s - loss: 2.8999 - mse: 0.8544\n",
      "Epoch 345/1000\n",
      "11/11 - 0s - loss: 2.8984 - mse: 0.8506\n",
      "Epoch 346/1000\n",
      "11/11 - 0s - loss: 2.8969 - mse: 0.8469\n",
      "Epoch 347/1000\n",
      "11/11 - 0s - loss: 2.8954 - mse: 0.8433\n",
      "Epoch 348/1000\n",
      "11/11 - 0s - loss: 2.8939 - mse: 0.8397\n",
      "Epoch 349/1000\n",
      "11/11 - 0s - loss: 2.8925 - mse: 0.8362\n",
      "Epoch 350/1000\n",
      "11/11 - 0s - loss: 2.8911 - mse: 0.8328\n",
      "Epoch 351/1000\n",
      "11/11 - 0s - loss: 2.8898 - mse: 0.8294\n",
      "Epoch 352/1000\n",
      "11/11 - 0s - loss: 2.8885 - mse: 0.8260\n",
      "Epoch 353/1000\n",
      "11/11 - 0s - loss: 2.8872 - mse: 0.8228\n",
      "Epoch 354/1000\n",
      "11/11 - 0s - loss: 2.8859 - mse: 0.8195\n",
      "Epoch 355/1000\n",
      "11/11 - 0s - loss: 2.8847 - mse: 0.8164\n",
      "Epoch 356/1000\n",
      "11/11 - 0s - loss: 2.8835 - mse: 0.8133\n",
      "Epoch 357/1000\n",
      "11/11 - 0s - loss: 2.8823 - mse: 0.8102\n",
      "Epoch 358/1000\n",
      "11/11 - 0s - loss: 2.8811 - mse: 0.8072\n",
      "Epoch 359/1000\n",
      "11/11 - 0s - loss: 2.8800 - mse: 0.8043\n",
      "Epoch 360/1000\n",
      "11/11 - 0s - loss: 2.8789 - mse: 0.8014\n",
      "Epoch 361/1000\n",
      "11/11 - 0s - loss: 2.8779 - mse: 0.7986\n",
      "Epoch 362/1000\n",
      "11/11 - 0s - loss: 2.8768 - mse: 0.7958\n",
      "Epoch 363/1000\n",
      "11/11 - 0s - loss: 2.8758 - mse: 0.7930\n",
      "Epoch 364/1000\n",
      "11/11 - 0s - loss: 2.8748 - mse: 0.7903\n",
      "Epoch 365/1000\n",
      "11/11 - 0s - loss: 2.8738 - mse: 0.7877\n",
      "Epoch 366/1000\n",
      "11/11 - 0s - loss: 2.8729 - mse: 0.7851\n",
      "Epoch 367/1000\n",
      "11/11 - 0s - loss: 2.8719 - mse: 0.7826\n",
      "Epoch 368/1000\n",
      "11/11 - 0s - loss: 2.8710 - mse: 0.7801\n",
      "Epoch 369/1000\n",
      "11/11 - 0s - loss: 2.8701 - mse: 0.7776\n",
      "Epoch 370/1000\n",
      "11/11 - 0s - loss: 2.8693 - mse: 0.7752\n",
      "Epoch 371/1000\n",
      "11/11 - 0s - loss: 2.8684 - mse: 0.7729\n",
      "Epoch 372/1000\n",
      "11/11 - 0s - loss: 2.8676 - mse: 0.7706\n",
      "Epoch 373/1000\n",
      "11/11 - 0s - loss: 2.8668 - mse: 0.7683\n",
      "Epoch 374/1000\n",
      "11/11 - 0s - loss: 2.8660 - mse: 0.7661\n",
      "Epoch 375/1000\n",
      "11/11 - 0s - loss: 2.8653 - mse: 0.7639\n",
      "Epoch 376/1000\n",
      "11/11 - 0s - loss: 2.8645 - mse: 0.7617\n",
      "Epoch 377/1000\n",
      "11/11 - 0s - loss: 2.8638 - mse: 0.7596\n",
      "Epoch 378/1000\n",
      "11/11 - 0s - loss: 2.8631 - mse: 0.7576\n",
      "Epoch 379/1000\n",
      "11/11 - 0s - loss: 2.8624 - mse: 0.7555\n",
      "Epoch 380/1000\n",
      "11/11 - 0s - loss: 2.8617 - mse: 0.7535\n",
      "Epoch 381/1000\n",
      "11/11 - 0s - loss: 2.8611 - mse: 0.7516\n",
      "Epoch 382/1000\n",
      "11/11 - 0s - loss: 2.8604 - mse: 0.7497\n",
      "Epoch 383/1000\n",
      "11/11 - 0s - loss: 2.8598 - mse: 0.7478\n",
      "Epoch 384/1000\n",
      "11/11 - 0s - loss: 2.8592 - mse: 0.7460\n",
      "Epoch 385/1000\n",
      "11/11 - 0s - loss: 2.8586 - mse: 0.7442\n",
      "Epoch 386/1000\n",
      "11/11 - 0s - loss: 2.8580 - mse: 0.7424\n",
      "Epoch 387/1000\n",
      "11/11 - 0s - loss: 2.8574 - mse: 0.7407\n",
      "Epoch 388/1000\n",
      "11/11 - 0s - loss: 2.8569 - mse: 0.7390\n",
      "Epoch 389/1000\n",
      "11/11 - 0s - loss: 2.8563 - mse: 0.7374\n",
      "Epoch 390/1000\n",
      "11/11 - 0s - loss: 2.8558 - mse: 0.7357\n",
      "Epoch 391/1000\n",
      "11/11 - 0s - loss: 2.8553 - mse: 0.7342\n",
      "Epoch 392/1000\n",
      "11/11 - 0s - loss: 2.8548 - mse: 0.7326\n",
      "Epoch 393/1000\n",
      "11/11 - 0s - loss: 2.8543 - mse: 0.7311\n",
      "Epoch 394/1000\n",
      "11/11 - 0s - loss: 2.8539 - mse: 0.7296\n",
      "Epoch 395/1000\n",
      "11/11 - 0s - loss: 2.8534 - mse: 0.7281\n",
      "Epoch 396/1000\n",
      "11/11 - 0s - loss: 2.8530 - mse: 0.7267\n",
      "Epoch 397/1000\n",
      "11/11 - 0s - loss: 2.8525 - mse: 0.7253\n",
      "Epoch 398/1000\n",
      "11/11 - 0s - loss: 2.8521 - mse: 0.7239\n",
      "Epoch 399/1000\n",
      "11/11 - 0s - loss: 2.8517 - mse: 0.7226\n",
      "Epoch 400/1000\n",
      "11/11 - 0s - loss: 2.8513 - mse: 0.7213\n",
      "Epoch 401/1000\n",
      "11/11 - 0s - loss: 2.8509 - mse: 0.7200\n",
      "Epoch 402/1000\n",
      "11/11 - 0s - loss: 2.8505 - mse: 0.7187\n",
      "Epoch 403/1000\n",
      "11/11 - 0s - loss: 2.8501 - mse: 0.7175\n",
      "Epoch 404/1000\n",
      "11/11 - 0s - loss: 2.8498 - mse: 0.7163\n",
      "Epoch 405/1000\n",
      "11/11 - 0s - loss: 2.8494 - mse: 0.7151\n",
      "Epoch 406/1000\n",
      "11/11 - 0s - loss: 2.8491 - mse: 0.7139\n",
      "Epoch 407/1000\n",
      "11/11 - 0s - loss: 2.8487 - mse: 0.7128\n",
      "Epoch 408/1000\n",
      "11/11 - 0s - loss: 2.8484 - mse: 0.7117\n",
      "Epoch 409/1000\n",
      "11/11 - 0s - loss: 2.8481 - mse: 0.7106\n",
      "Epoch 410/1000\n",
      "11/11 - 0s - loss: 2.8478 - mse: 0.7096\n",
      "Epoch 411/1000\n",
      "11/11 - 0s - loss: 2.8475 - mse: 0.7085\n",
      "Epoch 412/1000\n",
      "11/11 - 0s - loss: 2.8472 - mse: 0.7075\n",
      "Epoch 413/1000\n",
      "11/11 - 0s - loss: 2.8469 - mse: 0.7066\n",
      "Epoch 414/1000\n",
      "11/11 - 0s - loss: 2.8466 - mse: 0.7056\n",
      "Epoch 415/1000\n",
      "11/11 - 0s - loss: 2.8463 - mse: 0.7047\n",
      "Epoch 416/1000\n",
      "11/11 - 0s - loss: 2.8461 - mse: 0.7037\n",
      "Epoch 417/1000\n",
      "11/11 - 0s - loss: 2.8458 - mse: 0.7028\n",
      "Epoch 418/1000\n",
      "11/11 - 0s - loss: 2.8456 - mse: 0.7020\n",
      "Epoch 419/1000\n",
      "11/11 - 0s - loss: 2.8453 - mse: 0.7011\n",
      "Epoch 420/1000\n",
      "11/11 - 0s - loss: 2.8451 - mse: 0.7003\n",
      "Epoch 421/1000\n",
      "11/11 - 0s - loss: 2.8449 - mse: 0.6995\n",
      "Epoch 422/1000\n",
      "11/11 - 0s - loss: 2.8447 - mse: 0.6987\n",
      "Epoch 423/1000\n",
      "11/11 - 0s - loss: 2.8444 - mse: 0.6979\n",
      "Epoch 424/1000\n",
      "11/11 - 0s - loss: 2.8442 - mse: 0.6971\n",
      "Epoch 425/1000\n",
      "11/11 - 0s - loss: 2.8440 - mse: 0.6964\n",
      "Epoch 426/1000\n",
      "11/11 - 0s - loss: 2.8438 - mse: 0.6957\n",
      "Epoch 427/1000\n",
      "11/11 - 0s - loss: 2.8436 - mse: 0.6950\n",
      "Epoch 428/1000\n",
      "11/11 - 0s - loss: 2.8434 - mse: 0.6943\n",
      "Epoch 429/1000\n",
      "11/11 - 0s - loss: 2.8433 - mse: 0.6936\n",
      "Epoch 430/1000\n",
      "11/11 - 0s - loss: 2.8431 - mse: 0.6929\n",
      "Epoch 431/1000\n",
      "11/11 - 0s - loss: 2.8429 - mse: 0.6923\n",
      "Epoch 432/1000\n",
      "11/11 - 0s - loss: 2.8427 - mse: 0.6917\n",
      "Epoch 433/1000\n",
      "11/11 - 0s - loss: 2.8426 - mse: 0.6911\n",
      "Epoch 434/1000\n",
      "11/11 - 0s - loss: 2.8424 - mse: 0.6905\n",
      "Epoch 435/1000\n",
      "11/11 - 0s - loss: 2.8423 - mse: 0.6899\n",
      "Epoch 436/1000\n",
      "11/11 - 0s - loss: 2.8421 - mse: 0.6893\n",
      "Epoch 437/1000\n",
      "11/11 - 0s - loss: 2.8420 - mse: 0.6888\n",
      "Epoch 438/1000\n",
      "11/11 - 0s - loss: 2.8418 - mse: 0.6883\n",
      "Epoch 439/1000\n",
      "11/11 - 0s - loss: 2.8417 - mse: 0.6877\n",
      "Epoch 440/1000\n",
      "11/11 - 0s - loss: 2.8416 - mse: 0.6872\n",
      "Epoch 441/1000\n",
      "11/11 - 0s - loss: 2.8414 - mse: 0.6868\n",
      "Epoch 442/1000\n",
      "11/11 - 0s - loss: 2.8413 - mse: 0.6863\n",
      "Epoch 443/1000\n",
      "11/11 - 0s - loss: 2.8412 - mse: 0.6858\n",
      "Epoch 444/1000\n",
      "11/11 - 0s - loss: 2.8411 - mse: 0.6854\n",
      "Epoch 445/1000\n",
      "11/11 - 0s - loss: 2.8410 - mse: 0.6849\n",
      "Epoch 446/1000\n",
      "11/11 - 0s - loss: 2.8408 - mse: 0.6845\n",
      "Epoch 447/1000\n",
      "11/11 - 0s - loss: 2.8407 - mse: 0.6841\n",
      "Epoch 448/1000\n",
      "11/11 - 0s - loss: 2.8406 - mse: 0.6837\n",
      "Epoch 449/1000\n",
      "11/11 - 0s - loss: 2.8405 - mse: 0.6833\n",
      "Epoch 450/1000\n",
      "11/11 - 0s - loss: 2.8404 - mse: 0.6829\n",
      "Epoch 451/1000\n",
      "11/11 - 0s - loss: 2.8403 - mse: 0.6825\n",
      "Epoch 452/1000\n",
      "11/11 - 0s - loss: 2.8402 - mse: 0.6822\n",
      "Epoch 453/1000\n",
      "11/11 - 0s - loss: 2.8401 - mse: 0.6818\n",
      "Epoch 454/1000\n",
      "11/11 - 0s - loss: 2.8400 - mse: 0.6815\n",
      "Epoch 455/1000\n",
      "11/11 - 0s - loss: 2.8400 - mse: 0.6811\n",
      "Epoch 456/1000\n",
      "11/11 - 0s - loss: 2.8399 - mse: 0.6808\n",
      "Epoch 457/1000\n",
      "11/11 - 0s - loss: 2.8398 - mse: 0.6805\n",
      "Epoch 458/1000\n",
      "11/11 - 0s - loss: 2.8397 - mse: 0.6802\n",
      "Epoch 459/1000\n",
      "11/11 - 0s - loss: 2.8396 - mse: 0.6799\n",
      "Epoch 460/1000\n",
      "11/11 - 0s - loss: 2.8396 - mse: 0.6796\n",
      "Epoch 461/1000\n",
      "11/11 - 0s - loss: 2.8395 - mse: 0.6793\n",
      "Epoch 462/1000\n",
      "11/11 - 0s - loss: 2.8394 - mse: 0.6791\n",
      "Epoch 463/1000\n",
      "11/11 - 0s - loss: 2.8394 - mse: 0.6788\n",
      "Epoch 464/1000\n",
      "11/11 - 0s - loss: 2.8393 - mse: 0.6786\n",
      "Epoch 465/1000\n",
      "11/11 - 0s - loss: 2.8392 - mse: 0.6783\n",
      "Epoch 466/1000\n",
      "11/11 - 0s - loss: 2.8392 - mse: 0.6781\n",
      "Epoch 467/1000\n",
      "11/11 - 0s - loss: 2.8391 - mse: 0.6779\n",
      "Epoch 468/1000\n",
      "11/11 - 0s - loss: 2.8390 - mse: 0.6776\n",
      "Epoch 469/1000\n",
      "11/11 - 0s - loss: 2.8390 - mse: 0.6774\n",
      "Epoch 470/1000\n",
      "11/11 - 0s - loss: 2.8389 - mse: 0.6772\n",
      "Epoch 471/1000\n",
      "11/11 - 0s - loss: 2.8389 - mse: 0.6770\n",
      "Epoch 472/1000\n",
      "11/11 - 0s - loss: 2.8388 - mse: 0.6768\n",
      "Epoch 473/1000\n",
      "11/11 - 0s - loss: 2.8388 - mse: 0.6766\n",
      "Epoch 474/1000\n",
      "11/11 - 0s - loss: 2.8387 - mse: 0.6764\n",
      "Epoch 475/1000\n",
      "11/11 - 0s - loss: 2.8387 - mse: 0.6763\n",
      "Epoch 476/1000\n",
      "11/11 - 0s - loss: 2.8386 - mse: 0.6761\n",
      "Epoch 477/1000\n",
      "11/11 - 0s - loss: 2.8386 - mse: 0.6759\n",
      "Epoch 478/1000\n",
      "11/11 - 0s - loss: 2.8385 - mse: 0.6758\n",
      "Epoch 479/1000\n",
      "11/11 - 0s - loss: 2.8385 - mse: 0.6756\n",
      "Epoch 480/1000\n",
      "11/11 - 0s - loss: 2.8385 - mse: 0.6755\n",
      "Epoch 481/1000\n",
      "11/11 - 0s - loss: 2.8384 - mse: 0.6754\n",
      "Epoch 482/1000\n",
      "11/11 - 0s - loss: 2.8384 - mse: 0.6752\n",
      "Epoch 483/1000\n",
      "11/11 - 0s - loss: 2.8383 - mse: 0.6751\n",
      "Epoch 484/1000\n",
      "11/11 - 0s - loss: 2.8383 - mse: 0.6750\n",
      "Epoch 485/1000\n",
      "11/11 - 0s - loss: 2.8383 - mse: 0.6748\n",
      "Epoch 486/1000\n",
      "11/11 - 0s - loss: 2.8382 - mse: 0.6747\n",
      "Epoch 487/1000\n",
      "11/11 - 0s - loss: 2.8382 - mse: 0.6746\n",
      "Epoch 488/1000\n",
      "11/11 - 0s - loss: 2.8382 - mse: 0.6745\n",
      "Epoch 489/1000\n",
      "11/11 - 0s - loss: 2.8381 - mse: 0.6744\n",
      "Epoch 490/1000\n",
      "11/11 - 0s - loss: 2.8381 - mse: 0.6743\n",
      "Epoch 491/1000\n",
      "11/11 - 0s - loss: 2.8381 - mse: 0.6742\n",
      "Epoch 492/1000\n",
      "11/11 - 0s - loss: 2.8380 - mse: 0.6741\n",
      "Epoch 493/1000\n",
      "11/11 - 0s - loss: 2.8380 - mse: 0.6740\n",
      "Epoch 494/1000\n",
      "11/11 - 0s - loss: 2.8380 - mse: 0.6740\n",
      "Epoch 495/1000\n",
      "11/11 - 0s - loss: 2.8380 - mse: 0.6739\n",
      "Epoch 496/1000\n",
      "11/11 - 0s - loss: 2.8379 - mse: 0.6738\n",
      "Epoch 497/1000\n",
      "11/11 - 0s - loss: 2.8379 - mse: 0.6737\n",
      "Epoch 498/1000\n",
      "11/11 - 0s - loss: 2.8379 - mse: 0.6737\n",
      "Epoch 499/1000\n",
      "11/11 - 0s - loss: 2.8379 - mse: 0.6736\n",
      "Epoch 500/1000\n",
      "11/11 - 0s - loss: 2.8378 - mse: 0.6735\n",
      "Epoch 501/1000\n",
      "11/11 - 0s - loss: 2.8378 - mse: 0.6735\n",
      "Epoch 502/1000\n",
      "11/11 - 0s - loss: 2.8378 - mse: 0.6734\n",
      "Epoch 503/1000\n",
      "11/11 - 0s - loss: 2.8378 - mse: 0.6734\n",
      "Epoch 504/1000\n",
      "11/11 - 0s - loss: 2.8377 - mse: 0.6733\n",
      "Epoch 505/1000\n",
      "11/11 - 0s - loss: 2.8377 - mse: 0.6733\n",
      "Epoch 506/1000\n",
      "11/11 - 0s - loss: 2.8377 - mse: 0.6732\n",
      "Epoch 507/1000\n",
      "11/11 - 0s - loss: 2.8377 - mse: 0.6732\n",
      "Epoch 508/1000\n",
      "11/11 - 0s - loss: 2.8377 - mse: 0.6732\n",
      "Epoch 509/1000\n",
      "11/11 - 0s - loss: 2.8376 - mse: 0.6731\n",
      "Epoch 510/1000\n",
      "11/11 - 0s - loss: 2.8376 - mse: 0.6731\n",
      "Epoch 511/1000\n",
      "11/11 - 0s - loss: 2.8376 - mse: 0.6731\n",
      "Epoch 512/1000\n",
      "11/11 - 0s - loss: 2.8376 - mse: 0.6730\n",
      "Epoch 513/1000\n",
      "11/11 - 0s - loss: 2.8376 - mse: 0.6730\n",
      "Epoch 514/1000\n",
      "11/11 - 0s - loss: 2.8376 - mse: 0.6730\n",
      "Epoch 515/1000\n",
      "11/11 - 0s - loss: 2.8375 - mse: 0.6730\n",
      "Epoch 516/1000\n",
      "11/11 - 0s - loss: 2.8375 - mse: 0.6729\n",
      "Epoch 517/1000\n",
      "11/11 - 0s - loss: 2.8375 - mse: 0.6729\n",
      "Epoch 518/1000\n",
      "11/11 - 0s - loss: 2.8375 - mse: 0.6729\n",
      "Epoch 519/1000\n",
      "11/11 - 0s - loss: 2.8375 - mse: 0.6729\n",
      "Epoch 520/1000\n",
      "11/11 - 0s - loss: 2.8375 - mse: 0.6729\n",
      "Epoch 521/1000\n",
      "11/11 - 0s - loss: 2.8375 - mse: 0.6729\n",
      "Epoch 522/1000\n",
      "11/11 - 0s - loss: 2.8375 - mse: 0.6729\n",
      "Epoch 523/1000\n",
      "11/11 - 0s - loss: 2.8374 - mse: 0.6729\n",
      "Epoch 524/1000\n",
      "11/11 - 0s - loss: 2.8374 - mse: 0.6729\n",
      "Epoch 525/1000\n",
      "11/11 - 0s - loss: 2.8374 - mse: 0.6729\n",
      "Epoch 526/1000\n",
      "11/11 - 0s - loss: 2.8374 - mse: 0.6729\n",
      "Epoch 527/1000\n",
      "11/11 - 0s - loss: 2.8374 - mse: 0.6729\n",
      "Epoch 528/1000\n",
      "11/11 - 0s - loss: 2.8374 - mse: 0.6729\n",
      "Epoch 529/1000\n",
      "11/11 - 0s - loss: 2.8374 - mse: 0.6729\n",
      "Epoch 530/1000\n",
      "11/11 - 0s - loss: 2.8374 - mse: 0.6729\n",
      "Epoch 531/1000\n",
      "11/11 - 0s - loss: 2.8373 - mse: 0.6729\n",
      "Epoch 532/1000\n",
      "11/11 - 0s - loss: 2.8373 - mse: 0.6729\n",
      "Epoch 533/1000\n",
      "11/11 - 0s - loss: 2.8373 - mse: 0.6729\n",
      "Epoch 534/1000\n",
      "11/11 - 0s - loss: 2.8373 - mse: 0.6729\n",
      "Epoch 535/1000\n",
      "11/11 - 0s - loss: 2.8373 - mse: 0.6729\n",
      "Epoch 536/1000\n",
      "11/11 - 0s - loss: 2.8373 - mse: 0.6729\n",
      "Epoch 537/1000\n",
      "11/11 - 0s - loss: 2.8373 - mse: 0.6729\n",
      "Epoch 538/1000\n",
      "11/11 - 0s - loss: 2.8373 - mse: 0.6729\n",
      "Epoch 539/1000\n",
      "11/11 - 0s - loss: 2.8373 - mse: 0.6730\n",
      "Epoch 540/1000\n",
      "11/11 - 0s - loss: 2.8373 - mse: 0.6730\n",
      "Epoch 541/1000\n",
      "11/11 - 0s - loss: 2.8373 - mse: 0.6730\n",
      "Epoch 542/1000\n",
      "11/11 - 0s - loss: 2.8372 - mse: 0.6730\n",
      "Epoch 543/1000\n",
      "11/11 - 0s - loss: 2.8372 - mse: 0.6730\n",
      "Epoch 544/1000\n",
      "11/11 - 0s - loss: 2.8372 - mse: 0.6730\n",
      "Epoch 545/1000\n",
      "11/11 - 0s - loss: 2.8372 - mse: 0.6731\n",
      "Epoch 546/1000\n",
      "11/11 - 0s - loss: 2.8372 - mse: 0.6731\n",
      "Epoch 547/1000\n",
      "11/11 - 0s - loss: 2.8372 - mse: 0.6731\n",
      "Epoch 548/1000\n",
      "11/11 - 0s - loss: 2.8372 - mse: 0.6731\n",
      "Epoch 549/1000\n",
      "11/11 - 0s - loss: 2.8372 - mse: 0.6731\n",
      "Epoch 550/1000\n",
      "11/11 - 0s - loss: 2.8372 - mse: 0.6732\n",
      "Epoch 551/1000\n",
      "11/11 - 0s - loss: 2.8372 - mse: 0.6732\n",
      "Epoch 552/1000\n",
      "11/11 - 0s - loss: 2.8372 - mse: 0.6732\n",
      "Epoch 553/1000\n",
      "11/11 - 0s - loss: 2.8372 - mse: 0.6732\n",
      "Epoch 554/1000\n",
      "11/11 - 0s - loss: 2.8372 - mse: 0.6733\n",
      "Epoch 555/1000\n",
      "11/11 - 0s - loss: 2.8372 - mse: 0.6733\n",
      "Epoch 556/1000\n",
      "11/11 - 0s - loss: 2.8371 - mse: 0.6733\n",
      "Epoch 557/1000\n",
      "11/11 - 0s - loss: 2.8371 - mse: 0.6734\n",
      "Epoch 558/1000\n",
      "11/11 - 0s - loss: 2.8371 - mse: 0.6734\n",
      "Epoch 559/1000\n",
      "11/11 - 0s - loss: 2.8371 - mse: 0.6734\n",
      "Epoch 560/1000\n",
      "11/11 - 0s - loss: 2.8371 - mse: 0.6734\n",
      "Epoch 561/1000\n",
      "11/11 - 0s - loss: 2.8371 - mse: 0.6735\n",
      "Epoch 562/1000\n",
      "11/11 - 0s - loss: 2.8371 - mse: 0.6735\n",
      "Epoch 563/1000\n",
      "11/11 - 0s - loss: 2.8371 - mse: 0.6735\n",
      "Epoch 564/1000\n",
      "11/11 - 0s - loss: 2.8371 - mse: 0.6736\n",
      "Epoch 565/1000\n",
      "11/11 - 0s - loss: 2.8371 - mse: 0.6736\n",
      "Epoch 566/1000\n",
      "11/11 - 0s - loss: 2.8371 - mse: 0.6736\n",
      "Epoch 567/1000\n",
      "11/11 - 0s - loss: 2.8371 - mse: 0.6737\n",
      "Epoch 568/1000\n",
      "11/11 - 0s - loss: 2.8371 - mse: 0.6737\n",
      "Epoch 569/1000\n",
      "11/11 - 0s - loss: 2.8371 - mse: 0.6737\n",
      "Epoch 570/1000\n",
      "11/11 - 0s - loss: 2.8371 - mse: 0.6738\n",
      "Epoch 571/1000\n",
      "11/11 - 0s - loss: 2.8371 - mse: 0.6738\n",
      "Epoch 572/1000\n",
      "11/11 - 0s - loss: 2.8371 - mse: 0.6738\n",
      "Epoch 573/1000\n",
      "11/11 - 0s - loss: 2.8371 - mse: 0.6739\n",
      "Epoch 574/1000\n",
      "11/11 - 0s - loss: 2.8371 - mse: 0.6739\n",
      "Epoch 575/1000\n",
      "11/11 - 0s - loss: 2.8371 - mse: 0.6739\n",
      "Epoch 576/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6740\n",
      "Epoch 577/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6740\n",
      "Epoch 578/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6740\n",
      "Epoch 579/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6741\n",
      "Epoch 580/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6741\n",
      "Epoch 581/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6742\n",
      "Epoch 582/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6742\n",
      "Epoch 583/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6742\n",
      "Epoch 584/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6743\n",
      "Epoch 585/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6743\n",
      "Epoch 586/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6743\n",
      "Epoch 587/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6744\n",
      "Epoch 588/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6744\n",
      "Epoch 589/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6745\n",
      "Epoch 590/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6745\n",
      "Epoch 591/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6745\n",
      "Epoch 592/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6746\n",
      "Epoch 593/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6746\n",
      "Epoch 594/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6747\n",
      "Epoch 595/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6747\n",
      "Epoch 596/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6747\n",
      "Epoch 597/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6748\n",
      "Epoch 598/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6748\n",
      "Epoch 599/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6748\n",
      "Epoch 600/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6749\n",
      "Epoch 601/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6749\n",
      "Epoch 602/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6750\n",
      "Epoch 603/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6750\n",
      "Epoch 604/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6750\n",
      "Epoch 605/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6751\n",
      "Epoch 606/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6751\n",
      "Epoch 607/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6752\n",
      "Epoch 608/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6752\n",
      "Epoch 609/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6753\n",
      "Epoch 610/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6753\n",
      "Epoch 611/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6753\n",
      "Epoch 612/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6754\n",
      "Epoch 613/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6754\n",
      "Epoch 614/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6755\n",
      "Epoch 615/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6755\n",
      "Epoch 616/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6755\n",
      "Epoch 617/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6756\n",
      "Epoch 618/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6756\n",
      "Epoch 619/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6757\n",
      "Epoch 620/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6757\n",
      "Epoch 621/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6757\n",
      "Epoch 622/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6758\n",
      "Epoch 623/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6758\n",
      "Epoch 624/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6759\n",
      "Epoch 625/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6759\n",
      "Epoch 626/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6759\n",
      "Epoch 627/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6760\n",
      "Epoch 628/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6760\n",
      "Epoch 629/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6761\n",
      "Epoch 630/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6761\n",
      "Epoch 631/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6761\n",
      "Epoch 632/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6762\n",
      "Epoch 633/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6762\n",
      "Epoch 634/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6763\n",
      "Epoch 635/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6763\n",
      "Epoch 636/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6763\n",
      "Epoch 637/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6764\n",
      "Epoch 638/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6764\n",
      "Epoch 639/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6765\n",
      "Epoch 640/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6765\n",
      "Epoch 641/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6766\n",
      "Epoch 642/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6766\n",
      "Epoch 643/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6766\n",
      "Epoch 644/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6767\n",
      "Epoch 645/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6767\n",
      "Epoch 646/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6768\n",
      "Epoch 647/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6768\n",
      "Epoch 648/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6768\n",
      "Epoch 649/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6769\n",
      "Epoch 650/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6769\n",
      "Epoch 651/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6770\n",
      "Epoch 652/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6770\n",
      "Epoch 653/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6770\n",
      "Epoch 654/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6771\n",
      "Epoch 655/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6771\n",
      "Epoch 656/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6771\n",
      "Epoch 657/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6772\n",
      "Epoch 658/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6772\n",
      "Epoch 659/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6773\n",
      "Epoch 660/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6773\n",
      "Epoch 661/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6773\n",
      "Epoch 662/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6774\n",
      "Epoch 663/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6774\n",
      "Epoch 664/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6775\n",
      "Epoch 665/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6775\n",
      "Epoch 666/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6775\n",
      "Epoch 667/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6776\n",
      "Epoch 668/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6776\n",
      "Epoch 669/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6777\n",
      "Epoch 670/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6777\n",
      "Epoch 671/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6777\n",
      "Epoch 672/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6778\n",
      "Epoch 673/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6778\n",
      "Epoch 674/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6778\n",
      "Epoch 675/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6779\n",
      "Epoch 676/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6779\n",
      "Epoch 677/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6780\n",
      "Epoch 678/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6780\n",
      "Epoch 679/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6780\n",
      "Epoch 680/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6781\n",
      "Epoch 681/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6781\n",
      "Epoch 682/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6781\n",
      "Epoch 683/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6782\n",
      "Epoch 684/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6782\n",
      "Epoch 685/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6783\n",
      "Epoch 686/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6783\n",
      "Epoch 687/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6783\n",
      "Epoch 688/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6784\n",
      "Epoch 689/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6784\n",
      "Epoch 690/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6784\n",
      "Epoch 691/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6785\n",
      "Epoch 692/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6785\n",
      "Epoch 693/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6785\n",
      "Epoch 694/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6786\n",
      "Epoch 695/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6786\n",
      "Epoch 696/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6786\n",
      "Epoch 697/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6787\n",
      "Epoch 698/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6787\n",
      "Epoch 699/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6788\n",
      "Epoch 700/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6788\n",
      "Epoch 701/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6788\n",
      "Epoch 702/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6789\n",
      "Epoch 703/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6789\n",
      "Epoch 704/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6789\n",
      "Epoch 705/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6790\n",
      "Epoch 706/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6790\n",
      "Epoch 707/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6790\n",
      "Epoch 708/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6791\n",
      "Epoch 709/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6791\n",
      "Epoch 710/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6791\n",
      "Epoch 711/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6792\n",
      "Epoch 712/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6792\n",
      "Epoch 713/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6792\n",
      "Epoch 714/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6793\n",
      "Epoch 715/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6793\n",
      "Epoch 716/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6793\n",
      "Epoch 717/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6794\n",
      "Epoch 718/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6794\n",
      "Epoch 719/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6794\n",
      "Epoch 720/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6795\n",
      "Epoch 721/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6795\n",
      "Epoch 722/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6795\n",
      "Epoch 723/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6796\n",
      "Epoch 724/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6796\n",
      "Epoch 725/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6796\n",
      "Epoch 726/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6797\n",
      "Epoch 727/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6797\n",
      "Epoch 728/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6797\n",
      "Epoch 729/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6797\n",
      "Epoch 730/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6798\n",
      "Epoch 731/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6798\n",
      "Epoch 732/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6798\n",
      "Epoch 733/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6799\n",
      "Epoch 734/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6799\n",
      "Epoch 735/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6799\n",
      "Epoch 736/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6800\n",
      "Epoch 737/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6800\n",
      "Epoch 738/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6800\n",
      "Epoch 739/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6800\n",
      "Epoch 740/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6801\n",
      "Epoch 741/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6801\n",
      "Epoch 742/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6801\n",
      "Epoch 743/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6802\n",
      "Epoch 744/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6802\n",
      "Epoch 745/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6802\n",
      "Epoch 746/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6802\n",
      "Epoch 747/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6803\n",
      "Epoch 748/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6803\n",
      "Epoch 749/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6803\n",
      "Epoch 750/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6804\n",
      "Epoch 751/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6804\n",
      "Epoch 752/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6804\n",
      "Epoch 753/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6804\n",
      "Epoch 754/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6805\n",
      "Epoch 755/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6805\n",
      "Epoch 756/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6805\n",
      "Epoch 757/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6806\n",
      "Epoch 758/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6806\n",
      "Epoch 759/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6806\n",
      "Epoch 760/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6806\n",
      "Epoch 761/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6807\n",
      "Epoch 762/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6807\n",
      "Epoch 763/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6807\n",
      "Epoch 764/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6807\n",
      "Epoch 765/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6808\n",
      "Epoch 766/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6808\n",
      "Epoch 767/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6808\n",
      "Epoch 768/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6808\n",
      "Epoch 769/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6809\n",
      "Epoch 770/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6809\n",
      "Epoch 771/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6809\n",
      "Epoch 772/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6809\n",
      "Epoch 773/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6810\n",
      "Epoch 774/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6810\n",
      "Epoch 775/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6810\n",
      "Epoch 776/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6810\n",
      "Epoch 777/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6811\n",
      "Epoch 778/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6811\n",
      "Epoch 779/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6811\n",
      "Epoch 780/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6811\n",
      "Epoch 781/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6812\n",
      "Epoch 782/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6812\n",
      "Epoch 783/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6812\n",
      "Epoch 784/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6812\n",
      "Epoch 785/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6813\n",
      "Epoch 786/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6813\n",
      "Epoch 787/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6813\n",
      "Epoch 788/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6813\n",
      "Epoch 789/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6813\n",
      "Epoch 790/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6814\n",
      "Epoch 791/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6814\n",
      "Epoch 792/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6814\n",
      "Epoch 793/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6814\n",
      "Epoch 794/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6815\n",
      "Epoch 795/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6815\n",
      "Epoch 796/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6815\n",
      "Epoch 797/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6815\n",
      "Epoch 798/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6815\n",
      "Epoch 799/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6816\n",
      "Epoch 800/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6816\n",
      "Epoch 801/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6816\n",
      "Epoch 802/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6816\n",
      "Epoch 803/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6816\n",
      "Epoch 804/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6817\n",
      "Epoch 805/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6817\n",
      "Epoch 806/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6817\n",
      "Epoch 807/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6817\n",
      "Epoch 808/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6818\n",
      "Epoch 809/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6818\n",
      "Epoch 810/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6818\n",
      "Epoch 811/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6818\n",
      "Epoch 812/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6818\n",
      "Epoch 813/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6818\n",
      "Epoch 814/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6819\n",
      "Epoch 815/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6819\n",
      "Epoch 816/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6819\n",
      "Epoch 817/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6819\n",
      "Epoch 818/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6819\n",
      "Epoch 819/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6820\n",
      "Epoch 820/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6820\n",
      "Epoch 821/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6820\n",
      "Epoch 822/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6820\n",
      "Epoch 823/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6820\n",
      "Epoch 824/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6821\n",
      "Epoch 825/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6821\n",
      "Epoch 826/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6821\n",
      "Epoch 827/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6821\n",
      "Epoch 828/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6821\n",
      "Epoch 829/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6821\n",
      "Epoch 830/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6822\n",
      "Epoch 831/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6822\n",
      "Epoch 832/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6822\n",
      "Epoch 833/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6822\n",
      "Epoch 834/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6822\n",
      "Epoch 835/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6822\n",
      "Epoch 836/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6823\n",
      "Epoch 837/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6823\n",
      "Epoch 838/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6823\n",
      "Epoch 839/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6823\n",
      "Epoch 840/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6823\n",
      "Epoch 841/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6823\n",
      "Epoch 842/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6824\n",
      "Epoch 843/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6824\n",
      "Epoch 844/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6824\n",
      "Epoch 845/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6824\n",
      "Epoch 846/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6824\n",
      "Epoch 847/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6824\n",
      "Epoch 848/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6825\n",
      "Epoch 849/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6825\n",
      "Epoch 850/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6825\n",
      "Epoch 851/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6825\n",
      "Epoch 852/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6825\n",
      "Epoch 853/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6825\n",
      "Epoch 854/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6825\n",
      "Epoch 855/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6826\n",
      "Epoch 856/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6826\n",
      "Epoch 857/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6826\n",
      "Epoch 858/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6826\n",
      "Epoch 859/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6826\n",
      "Epoch 860/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6826\n",
      "Epoch 861/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6826\n",
      "Epoch 862/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6827\n",
      "Epoch 863/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6827\n",
      "Epoch 864/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6827\n",
      "Epoch 865/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6827\n",
      "Epoch 866/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6827\n",
      "Epoch 867/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6827\n",
      "Epoch 868/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6827\n",
      "Epoch 869/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6828\n",
      "Epoch 870/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6828\n",
      "Epoch 871/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6828\n",
      "Epoch 872/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6828\n",
      "Epoch 873/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6828\n",
      "Epoch 874/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6828\n",
      "Epoch 875/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6828\n",
      "Epoch 876/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6828\n",
      "Epoch 877/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6829\n",
      "Epoch 878/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6829\n",
      "Epoch 879/1000\n",
      "11/11 - 0s - loss: 2.8369 - mse: 0.6829\n",
      "Epoch 880/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6829\n",
      "Epoch 881/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6829\n",
      "Epoch 882/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6829\n",
      "Epoch 883/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6829\n",
      "Epoch 884/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6829\n",
      "Epoch 885/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6830\n",
      "Epoch 886/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6830\n",
      "Epoch 887/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6830\n",
      "Epoch 888/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6830\n",
      "Epoch 889/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6830\n",
      "Epoch 890/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6830\n",
      "Epoch 891/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6830\n",
      "Epoch 892/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6830\n",
      "Epoch 893/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6830\n",
      "Epoch 894/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6831\n",
      "Epoch 895/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6831\n",
      "Epoch 896/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6831\n",
      "Epoch 897/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6831\n",
      "Epoch 898/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6831\n",
      "Epoch 899/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6831\n",
      "Epoch 900/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6831\n",
      "Epoch 901/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6831\n",
      "Epoch 902/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6831\n",
      "Epoch 903/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6831\n",
      "Epoch 904/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6832\n",
      "Epoch 905/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6832\n",
      "Epoch 906/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6832\n",
      "Epoch 907/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6832\n",
      "Epoch 908/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6832\n",
      "Epoch 909/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6832\n",
      "Epoch 910/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6832\n",
      "Epoch 911/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6832\n",
      "Epoch 912/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6832\n",
      "Epoch 913/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6832\n",
      "Epoch 914/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6833\n",
      "Epoch 915/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6833\n",
      "Epoch 916/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6833\n",
      "Epoch 917/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6833\n",
      "Epoch 918/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6833\n",
      "Epoch 919/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6833\n",
      "Epoch 920/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6833\n",
      "Epoch 921/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6833\n",
      "Epoch 922/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6833\n",
      "Epoch 923/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6833\n",
      "Epoch 924/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6833\n",
      "Epoch 925/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6834\n",
      "Epoch 926/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6834\n",
      "Epoch 927/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6834\n",
      "Epoch 928/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6834\n",
      "Epoch 929/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6834\n",
      "Epoch 930/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6834\n",
      "Epoch 931/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6834\n",
      "Epoch 932/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6834\n",
      "Epoch 933/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6834\n",
      "Epoch 934/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6834\n",
      "Epoch 935/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6834\n",
      "Epoch 936/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6834\n",
      "Epoch 937/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6835\n",
      "Epoch 938/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6835\n",
      "Epoch 939/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6835\n",
      "Epoch 940/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6835\n",
      "Epoch 941/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6835\n",
      "Epoch 942/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6835\n",
      "Epoch 943/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6835\n",
      "Epoch 944/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6835\n",
      "Epoch 945/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6835\n",
      "Epoch 946/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6835\n",
      "Epoch 947/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6835\n",
      "Epoch 948/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6835\n",
      "Epoch 949/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6835\n",
      "Epoch 950/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6835\n",
      "Epoch 951/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6836\n",
      "Epoch 952/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6836\n",
      "Epoch 953/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6836\n",
      "Epoch 954/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6836\n",
      "Epoch 955/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6836\n",
      "Epoch 956/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6836\n",
      "Epoch 957/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6836\n",
      "Epoch 958/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6836\n",
      "Epoch 959/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6836\n",
      "Epoch 960/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6836\n",
      "Epoch 961/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6836\n",
      "Epoch 962/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6836\n",
      "Epoch 963/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6836\n",
      "Epoch 964/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6836\n",
      "Epoch 965/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6836\n",
      "Epoch 966/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6837\n",
      "Epoch 967/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6837\n",
      "Epoch 968/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6837\n",
      "Epoch 969/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6837\n",
      "Epoch 970/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6837\n",
      "Epoch 971/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6837\n",
      "Epoch 972/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6837\n",
      "Epoch 973/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6837\n",
      "Epoch 974/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6837\n",
      "Epoch 975/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6837\n",
      "Epoch 976/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6837\n",
      "Epoch 977/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6837\n",
      "Epoch 978/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6837\n",
      "Epoch 979/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6837\n",
      "Epoch 980/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6837\n",
      "Epoch 981/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6837\n",
      "Epoch 982/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6837\n",
      "Epoch 983/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6838\n",
      "Epoch 984/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6838\n",
      "Epoch 985/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6838\n",
      "Epoch 986/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6838\n",
      "Epoch 987/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6838\n",
      "Epoch 988/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6838\n",
      "Epoch 989/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6838\n",
      "Epoch 990/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6838\n",
      "Epoch 991/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6838\n",
      "Epoch 992/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6838\n",
      "Epoch 993/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6838\n",
      "Epoch 994/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6838\n",
      "Epoch 995/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6838\n",
      "Epoch 996/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6838\n",
      "Epoch 997/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6838\n",
      "Epoch 998/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6838\n",
      "Epoch 999/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6838\n",
      "Epoch 1000/1000\n",
      "11/11 - 0s - loss: 2.8370 - mse: 0.6838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2158926e040>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.astype('float64')\n",
    "y_train = y_train.astype('float64')\n",
    "x_test = x_test.astype('float64')\n",
    "y_test = y_test.astype('float64')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=x_train.shape[1], activation='linear', kernel_regularizer = make_reg_function(1.0)))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "model.fit(x_train,y_train, batch_size=int(x_train.shape[0]/10), epochs=1000, shuffle=False, verbose = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.2001652 ],\n",
       "        [ 0.74376196],\n",
       "        [ 0.09611327],\n",
       "        [-2.5134697 ],\n",
       "        [ 1.8633989 ],\n",
       "        [-1.0702358 ]], dtype=float32),\n",
       " array([-1.0333534], dtype=float32)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine coefs\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 537us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7148016528828431"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate MSE\n",
    "y_pred = model.predict(x_test,verbose=1)\n",
    "mean_squared_error(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Turn up the Regularization\n",
    "## Steps\n",
    " - ### Repeat part 1 with the weight factor turned up to many times its initial value\n",
    " - ### Calculate the `mse` on the test set\n",
    " - ### examine the coefficients and comment\n",
    " - ### compare these results to the best constant guess for `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "11/11 - 0s - loss: 24.9927 - mse: 19.7510\n",
      "Epoch 2/1000\n",
      "11/11 - 0s - loss: 24.6428 - mse: 19.3143\n",
      "Epoch 3/1000\n",
      "11/11 - 0s - loss: 24.3186 - mse: 18.9073\n",
      "Epoch 4/1000\n",
      "11/11 - 0s - loss: 24.0171 - mse: 18.5283\n",
      "Epoch 5/1000\n",
      "11/11 - 0s - loss: 23.7369 - mse: 18.1774\n",
      "Epoch 6/1000\n",
      "11/11 - 0s - loss: 23.4762 - mse: 17.8540\n",
      "Epoch 7/1000\n",
      "11/11 - 0s - loss: 23.2326 - mse: 17.5565\n",
      "Epoch 8/1000\n",
      "11/11 - 0s - loss: 23.0042 - mse: 17.2827\n",
      "Epoch 9/1000\n",
      "11/11 - 0s - loss: 22.7887 - mse: 17.0303\n",
      "Epoch 10/1000\n",
      "11/11 - 0s - loss: 22.5847 - mse: 16.7970\n",
      "Epoch 11/1000\n",
      "11/11 - 0s - loss: 22.3905 - mse: 16.5807\n",
      "Epoch 12/1000\n",
      "11/11 - 0s - loss: 22.2050 - mse: 16.3793\n",
      "Epoch 13/1000\n",
      "11/11 - 0s - loss: 22.0274 - mse: 16.1909\n",
      "Epoch 14/1000\n",
      "11/11 - 0s - loss: 21.8568 - mse: 16.0140\n",
      "Epoch 15/1000\n",
      "11/11 - 0s - loss: 21.6927 - mse: 15.8472\n",
      "Epoch 16/1000\n",
      "11/11 - 0s - loss: 21.5347 - mse: 15.6891\n",
      "Epoch 17/1000\n",
      "11/11 - 0s - loss: 21.3822 - mse: 15.5390\n",
      "Epoch 18/1000\n",
      "11/11 - 0s - loss: 21.2351 - mse: 15.3957\n",
      "Epoch 19/1000\n",
      "11/11 - 0s - loss: 21.0930 - mse: 15.2588\n",
      "Epoch 20/1000\n",
      "11/11 - 0s - loss: 20.9557 - mse: 15.1274\n",
      "Epoch 21/1000\n",
      "11/11 - 0s - loss: 20.8229 - mse: 15.0011\n",
      "Epoch 22/1000\n",
      "11/11 - 0s - loss: 20.6945 - mse: 14.8795\n",
      "Epoch 23/1000\n",
      "11/11 - 0s - loss: 20.5703 - mse: 14.7621\n",
      "Epoch 24/1000\n",
      "11/11 - 0s - loss: 20.4501 - mse: 14.6487\n",
      "Epoch 25/1000\n",
      "11/11 - 0s - loss: 20.3337 - mse: 14.5389\n",
      "Epoch 26/1000\n",
      "11/11 - 0s - loss: 20.2211 - mse: 14.4326\n",
      "Epoch 27/1000\n",
      "11/11 - 0s - loss: 20.1120 - mse: 14.3294\n",
      "Epoch 28/1000\n",
      "11/11 - 0s - loss: 20.0063 - mse: 14.2293\n",
      "Epoch 29/1000\n",
      "11/11 - 0s - loss: 19.9040 - mse: 14.1320\n",
      "Epoch 30/1000\n",
      "11/11 - 0s - loss: 19.8048 - mse: 14.0375\n",
      "Epoch 31/1000\n",
      "11/11 - 0s - loss: 19.7086 - mse: 13.9455\n",
      "Epoch 32/1000\n",
      "11/11 - 0s - loss: 19.6154 - mse: 13.8560\n",
      "Epoch 33/1000\n",
      "11/11 - 0s - loss: 19.5250 - mse: 13.7689\n",
      "Epoch 34/1000\n",
      "11/11 - 0s - loss: 19.4373 - mse: 13.6841\n",
      "Epoch 35/1000\n",
      "11/11 - 0s - loss: 19.3522 - mse: 13.6015\n",
      "Epoch 36/1000\n",
      "11/11 - 0s - loss: 19.2697 - mse: 13.5210\n",
      "Epoch 37/1000\n",
      "11/11 - 0s - loss: 19.1896 - mse: 13.4426\n",
      "Epoch 38/1000\n",
      "11/11 - 0s - loss: 19.1118 - mse: 13.3661\n",
      "Epoch 39/1000\n",
      "11/11 - 0s - loss: 19.0362 - mse: 13.2916\n",
      "Epoch 40/1000\n",
      "11/11 - 0s - loss: 18.9628 - mse: 13.2191\n",
      "Epoch 41/1000\n",
      "11/11 - 0s - loss: 18.8915 - mse: 13.1483\n",
      "Epoch 42/1000\n",
      "11/11 - 0s - loss: 18.8222 - mse: 13.0793\n",
      "Epoch 43/1000\n",
      "11/11 - 0s - loss: 18.7548 - mse: 13.0121\n",
      "Epoch 44/1000\n",
      "11/11 - 0s - loss: 18.6893 - mse: 12.9466\n",
      "Epoch 45/1000\n",
      "11/11 - 0s - loss: 18.6256 - mse: 12.8828\n",
      "Epoch 46/1000\n",
      "11/11 - 0s - loss: 18.5635 - mse: 12.8206\n",
      "Epoch 47/1000\n",
      "11/11 - 0s - loss: 18.5032 - mse: 12.7599\n",
      "Epoch 48/1000\n",
      "11/11 - 0s - loss: 18.4444 - mse: 12.7009\n",
      "Epoch 49/1000\n",
      "11/11 - 0s - loss: 18.3872 - mse: 12.6434\n",
      "Epoch 50/1000\n",
      "11/11 - 0s - loss: 18.3315 - mse: 12.5873\n",
      "Epoch 51/1000\n",
      "11/11 - 0s - loss: 18.2771 - mse: 12.5327\n",
      "Epoch 52/1000\n",
      "11/11 - 0s - loss: 18.2242 - mse: 12.4796\n",
      "Epoch 53/1000\n",
      "11/11 - 0s - loss: 18.1726 - mse: 12.4279\n",
      "Epoch 54/1000\n",
      "11/11 - 0s - loss: 18.1222 - mse: 12.3775\n",
      "Epoch 55/1000\n",
      "11/11 - 0s - loss: 18.0731 - mse: 12.3285\n",
      "Epoch 56/1000\n",
      "11/11 - 0s - loss: 18.0252 - mse: 12.2808\n",
      "Epoch 57/1000\n",
      "11/11 - 0s - loss: 17.9784 - mse: 12.2343\n",
      "Epoch 58/1000\n",
      "11/11 - 0s - loss: 17.9327 - mse: 12.1892\n",
      "Epoch 59/1000\n",
      "11/11 - 0s - loss: 17.8881 - mse: 12.1453\n",
      "Epoch 60/1000\n",
      "11/11 - 0s - loss: 17.8445 - mse: 12.1026\n",
      "Epoch 61/1000\n",
      "11/11 - 0s - loss: 17.8019 - mse: 12.0610\n",
      "Epoch 62/1000\n",
      "11/11 - 0s - loss: 17.7602 - mse: 12.0207\n",
      "Epoch 63/1000\n",
      "11/11 - 0s - loss: 17.7195 - mse: 11.9814\n",
      "Epoch 64/1000\n",
      "11/11 - 0s - loss: 17.6796 - mse: 11.9433\n",
      "Epoch 65/1000\n",
      "11/11 - 0s - loss: 17.6407 - mse: 11.9063\n",
      "Epoch 66/1000\n",
      "11/11 - 0s - loss: 17.6025 - mse: 11.8703\n",
      "Epoch 67/1000\n",
      "11/11 - 0s - loss: 17.5652 - mse: 11.8354\n",
      "Epoch 68/1000\n",
      "11/11 - 0s - loss: 17.5286 - mse: 11.8015\n",
      "Epoch 69/1000\n",
      "11/11 - 0s - loss: 17.4928 - mse: 11.7685\n",
      "Epoch 70/1000\n",
      "11/11 - 0s - loss: 17.4577 - mse: 11.7366\n",
      "Epoch 71/1000\n",
      "11/11 - 0s - loss: 17.4233 - mse: 11.7056\n",
      "Epoch 72/1000\n",
      "11/11 - 0s - loss: 17.3895 - mse: 11.6755\n",
      "Epoch 73/1000\n",
      "11/11 - 0s - loss: 17.3565 - mse: 11.6463\n",
      "Epoch 74/1000\n",
      "11/11 - 0s - loss: 17.3241 - mse: 11.6180\n",
      "Epoch 75/1000\n",
      "11/11 - 0s - loss: 17.2923 - mse: 11.5906\n",
      "Epoch 76/1000\n",
      "11/11 - 0s - loss: 17.2611 - mse: 11.5640\n",
      "Epoch 77/1000\n",
      "11/11 - 0s - loss: 17.2304 - mse: 11.5382\n",
      "Epoch 78/1000\n",
      "11/11 - 0s - loss: 17.2004 - mse: 11.5133\n",
      "Epoch 79/1000\n",
      "11/11 - 0s - loss: 17.1709 - mse: 11.4891\n",
      "Epoch 80/1000\n",
      "11/11 - 0s - loss: 17.1419 - mse: 11.4657\n",
      "Epoch 81/1000\n",
      "11/11 - 0s - loss: 17.1134 - mse: 11.4430\n",
      "Epoch 82/1000\n",
      "11/11 - 0s - loss: 17.0855 - mse: 11.4210\n",
      "Epoch 83/1000\n",
      "11/11 - 0s - loss: 17.0580 - mse: 11.3998\n",
      "Epoch 84/1000\n",
      "11/11 - 0s - loss: 17.0310 - mse: 11.3792\n",
      "Epoch 85/1000\n",
      "11/11 - 0s - loss: 17.0045 - mse: 11.3593\n",
      "Epoch 86/1000\n",
      "11/11 - 0s - loss: 16.9784 - mse: 11.3400\n",
      "Epoch 87/1000\n",
      "11/11 - 0s - loss: 16.9527 - mse: 11.3214\n",
      "Epoch 88/1000\n",
      "11/11 - 0s - loss: 16.9275 - mse: 11.3034\n",
      "Epoch 89/1000\n",
      "11/11 - 0s - loss: 16.9027 - mse: 11.2860\n",
      "Epoch 90/1000\n",
      "11/11 - 0s - loss: 16.8783 - mse: 11.2692\n",
      "Epoch 91/1000\n",
      "11/11 - 0s - loss: 16.8542 - mse: 11.2530\n",
      "Epoch 92/1000\n",
      "11/11 - 0s - loss: 16.8306 - mse: 11.2373\n",
      "Epoch 93/1000\n",
      "11/11 - 0s - loss: 16.8073 - mse: 11.2221\n",
      "Epoch 94/1000\n",
      "11/11 - 0s - loss: 16.7844 - mse: 11.2075\n",
      "Epoch 95/1000\n",
      "11/11 - 0s - loss: 16.7619 - mse: 11.1934\n",
      "Epoch 96/1000\n",
      "11/11 - 0s - loss: 16.7397 - mse: 11.1797\n",
      "Epoch 97/1000\n",
      "11/11 - 0s - loss: 16.7178 - mse: 11.1666\n",
      "Epoch 98/1000\n",
      "11/11 - 0s - loss: 16.6963 - mse: 11.1539\n",
      "Epoch 99/1000\n",
      "11/11 - 0s - loss: 16.6751 - mse: 11.1417\n",
      "Epoch 100/1000\n",
      "11/11 - 0s - loss: 16.6542 - mse: 11.1299\n",
      "Epoch 101/1000\n",
      "11/11 - 0s - loss: 16.6336 - mse: 11.1185\n",
      "Epoch 102/1000\n",
      "11/11 - 0s - loss: 16.6134 - mse: 11.1076\n",
      "Epoch 103/1000\n",
      "11/11 - 0s - loss: 16.5934 - mse: 11.0970\n",
      "Epoch 104/1000\n",
      "11/11 - 0s - loss: 16.5737 - mse: 11.0868\n",
      "Epoch 105/1000\n",
      "11/11 - 0s - loss: 16.5543 - mse: 11.0771\n",
      "Epoch 106/1000\n",
      "11/11 - 0s - loss: 16.5351 - mse: 11.0676\n",
      "Epoch 107/1000\n",
      "11/11 - 0s - loss: 16.5163 - mse: 11.0586\n",
      "Epoch 108/1000\n",
      "11/11 - 0s - loss: 16.4977 - mse: 11.0498\n",
      "Epoch 109/1000\n",
      "11/11 - 0s - loss: 16.4793 - mse: 11.0415\n",
      "Epoch 110/1000\n",
      "11/11 - 0s - loss: 16.4612 - mse: 11.0334\n",
      "Epoch 111/1000\n",
      "11/11 - 0s - loss: 16.4434 - mse: 11.0256\n",
      "Epoch 112/1000\n",
      "11/11 - 0s - loss: 16.4258 - mse: 11.0182\n",
      "Epoch 113/1000\n",
      "11/11 - 0s - loss: 16.4084 - mse: 11.0110\n",
      "Epoch 114/1000\n",
      "11/11 - 0s - loss: 16.3913 - mse: 11.0042\n",
      "Epoch 115/1000\n",
      "11/11 - 0s - loss: 16.3744 - mse: 10.9976\n",
      "Epoch 116/1000\n",
      "11/11 - 0s - loss: 16.3577 - mse: 10.9912\n",
      "Epoch 117/1000\n",
      "11/11 - 0s - loss: 16.3412 - mse: 10.9852\n",
      "Epoch 118/1000\n",
      "11/11 - 0s - loss: 16.3250 - mse: 10.9793\n",
      "Epoch 119/1000\n",
      "11/11 - 0s - loss: 16.3089 - mse: 10.9738\n",
      "Epoch 120/1000\n",
      "11/11 - 0s - loss: 16.2931 - mse: 10.9684\n",
      "Epoch 121/1000\n",
      "11/11 - 0s - loss: 16.2775 - mse: 10.9633\n",
      "Epoch 122/1000\n",
      "11/11 - 0s - loss: 16.2620 - mse: 10.9584\n",
      "Epoch 123/1000\n",
      "11/11 - 0s - loss: 16.2468 - mse: 10.9537\n",
      "Epoch 124/1000\n",
      "11/11 - 0s - loss: 16.2317 - mse: 10.9492\n",
      "Epoch 125/1000\n",
      "11/11 - 0s - loss: 16.2168 - mse: 10.9449\n",
      "Epoch 126/1000\n",
      "11/11 - 0s - loss: 16.2021 - mse: 10.9408\n",
      "Epoch 127/1000\n",
      "11/11 - 0s - loss: 16.1876 - mse: 10.9368\n",
      "Epoch 128/1000\n",
      "11/11 - 0s - loss: 16.1732 - mse: 10.9331\n",
      "Epoch 129/1000\n",
      "11/11 - 0s - loss: 16.1591 - mse: 10.9295\n",
      "Epoch 130/1000\n",
      "11/11 - 0s - loss: 16.1451 - mse: 10.9261\n",
      "Epoch 131/1000\n",
      "11/11 - 0s - loss: 16.1312 - mse: 10.9228\n",
      "Epoch 132/1000\n",
      "11/11 - 0s - loss: 16.1175 - mse: 10.9197\n",
      "Epoch 133/1000\n",
      "11/11 - 0s - loss: 16.1040 - mse: 10.9167\n",
      "Epoch 134/1000\n",
      "11/11 - 0s - loss: 16.0906 - mse: 10.9139\n",
      "Epoch 135/1000\n",
      "11/11 - 0s - loss: 16.0773 - mse: 10.9112\n",
      "Epoch 136/1000\n",
      "11/11 - 0s - loss: 16.0642 - mse: 10.9087\n",
      "Epoch 137/1000\n",
      "11/11 - 0s - loss: 16.0513 - mse: 10.9063\n",
      "Epoch 138/1000\n",
      "11/11 - 0s - loss: 16.0385 - mse: 10.9040\n",
      "Epoch 139/1000\n",
      "11/11 - 0s - loss: 16.0258 - mse: 10.9018\n",
      "Epoch 140/1000\n",
      "11/11 - 0s - loss: 16.0133 - mse: 10.8997\n",
      "Epoch 141/1000\n",
      "11/11 - 0s - loss: 16.0009 - mse: 10.8977\n",
      "Epoch 142/1000\n",
      "11/11 - 0s - loss: 15.9886 - mse: 10.8958\n",
      "Epoch 143/1000\n",
      "11/11 - 0s - loss: 15.9764 - mse: 10.8941\n",
      "Epoch 144/1000\n",
      "11/11 - 0s - loss: 15.9644 - mse: 10.8924\n",
      "Epoch 145/1000\n",
      "11/11 - 0s - loss: 15.9525 - mse: 10.8908\n",
      "Epoch 146/1000\n",
      "11/11 - 0s - loss: 15.9407 - mse: 10.8893\n",
      "Epoch 147/1000\n",
      "11/11 - 0s - loss: 15.9290 - mse: 10.8879\n",
      "Epoch 148/1000\n",
      "11/11 - 0s - loss: 15.9175 - mse: 10.8866\n",
      "Epoch 149/1000\n",
      "11/11 - 0s - loss: 15.9060 - mse: 10.8853\n",
      "Epoch 150/1000\n",
      "11/11 - 0s - loss: 15.8947 - mse: 10.8842\n",
      "Epoch 151/1000\n",
      "11/11 - 0s - loss: 15.8834 - mse: 10.8830\n",
      "Epoch 152/1000\n",
      "11/11 - 0s - loss: 15.8723 - mse: 10.8820\n",
      "Epoch 153/1000\n",
      "11/11 - 0s - loss: 15.8613 - mse: 10.8810\n",
      "Epoch 154/1000\n",
      "11/11 - 0s - loss: 15.8503 - mse: 10.8801\n",
      "Epoch 155/1000\n",
      "11/11 - 0s - loss: 15.8395 - mse: 10.8793\n",
      "Epoch 156/1000\n",
      "11/11 - 0s - loss: 15.8287 - mse: 10.8785\n",
      "Epoch 157/1000\n",
      "11/11 - 0s - loss: 15.8181 - mse: 10.8777\n",
      "Epoch 158/1000\n",
      "11/11 - 0s - loss: 15.8075 - mse: 10.8770\n",
      "Epoch 159/1000\n",
      "11/11 - 0s - loss: 15.7970 - mse: 10.8764\n",
      "Epoch 160/1000\n",
      "11/11 - 0s - loss: 15.7867 - mse: 10.8758\n",
      "Epoch 161/1000\n",
      "11/11 - 0s - loss: 15.7764 - mse: 10.8752\n",
      "Epoch 162/1000\n",
      "11/11 - 0s - loss: 15.7661 - mse: 10.8747\n",
      "Epoch 163/1000\n",
      "11/11 - 0s - loss: 15.7560 - mse: 10.8742\n",
      "Epoch 164/1000\n",
      "11/11 - 0s - loss: 15.7460 - mse: 10.8738\n",
      "Epoch 165/1000\n",
      "11/11 - 0s - loss: 15.7360 - mse: 10.8734\n",
      "Epoch 166/1000\n",
      "11/11 - 0s - loss: 15.7261 - mse: 10.8730\n",
      "Epoch 167/1000\n",
      "11/11 - 0s - loss: 15.7162 - mse: 10.8726\n",
      "Epoch 168/1000\n",
      "11/11 - 0s - loss: 15.7065 - mse: 10.8723\n",
      "Epoch 169/1000\n",
      "11/11 - 0s - loss: 15.6968 - mse: 10.8720\n",
      "Epoch 170/1000\n",
      "11/11 - 0s - loss: 15.6872 - mse: 10.8718\n",
      "Epoch 171/1000\n",
      "11/11 - 0s - loss: 15.6776 - mse: 10.8715\n",
      "Epoch 172/1000\n",
      "11/11 - 0s - loss: 15.6682 - mse: 10.8713\n",
      "Epoch 173/1000\n",
      "11/11 - 0s - loss: 15.6588 - mse: 10.8711\n",
      "Epoch 174/1000\n",
      "11/11 - 0s - loss: 15.6494 - mse: 10.8709\n",
      "Epoch 175/1000\n",
      "11/11 - 0s - loss: 15.6401 - mse: 10.8708\n",
      "Epoch 176/1000\n",
      "11/11 - 0s - loss: 15.6309 - mse: 10.8707\n",
      "Epoch 177/1000\n",
      "11/11 - 0s - loss: 15.6217 - mse: 10.8705\n",
      "Epoch 178/1000\n",
      "11/11 - 0s - loss: 15.6126 - mse: 10.8704\n",
      "Epoch 179/1000\n",
      "11/11 - 0s - loss: 15.6036 - mse: 10.8703\n",
      "Epoch 180/1000\n",
      "11/11 - 0s - loss: 15.5946 - mse: 10.8702\n",
      "Epoch 181/1000\n",
      "11/11 - 0s - loss: 15.5857 - mse: 10.8702\n",
      "Epoch 182/1000\n",
      "11/11 - 0s - loss: 15.5768 - mse: 10.8701\n",
      "Epoch 183/1000\n",
      "11/11 - 0s - loss: 15.5680 - mse: 10.8701\n",
      "Epoch 184/1000\n",
      "11/11 - 0s - loss: 15.5592 - mse: 10.8700\n",
      "Epoch 185/1000\n",
      "11/11 - 0s - loss: 15.5505 - mse: 10.8700\n",
      "Epoch 186/1000\n",
      "11/11 - 0s - loss: 15.5418 - mse: 10.8699\n",
      "Epoch 187/1000\n",
      "11/11 - 0s - loss: 15.5332 - mse: 10.8699\n",
      "Epoch 188/1000\n",
      "11/11 - 0s - loss: 15.5246 - mse: 10.8699\n",
      "Epoch 189/1000\n",
      "11/11 - 0s - loss: 15.5161 - mse: 10.8699\n",
      "Epoch 190/1000\n",
      "11/11 - 0s - loss: 15.5076 - mse: 10.8699\n",
      "Epoch 191/1000\n",
      "11/11 - 0s - loss: 15.4992 - mse: 10.8698\n",
      "Epoch 192/1000\n",
      "11/11 - 0s - loss: 15.4908 - mse: 10.8698\n",
      "Epoch 193/1000\n",
      "11/11 - 0s - loss: 15.4825 - mse: 10.8698\n",
      "Epoch 194/1000\n",
      "11/11 - 0s - loss: 15.4742 - mse: 10.8698\n",
      "Epoch 195/1000\n",
      "11/11 - 0s - loss: 15.4659 - mse: 10.8698\n",
      "Epoch 196/1000\n",
      "11/11 - 0s - loss: 15.4577 - mse: 10.8698\n",
      "Epoch 197/1000\n",
      "11/11 - 0s - loss: 15.4495 - mse: 10.8698\n",
      "Epoch 198/1000\n",
      "11/11 - 0s - loss: 15.4414 - mse: 10.8698\n",
      "Epoch 199/1000\n",
      "11/11 - 0s - loss: 15.4333 - mse: 10.8698\n",
      "Epoch 200/1000\n",
      "11/11 - 0s - loss: 15.4252 - mse: 10.8697\n",
      "Epoch 201/1000\n",
      "11/11 - 0s - loss: 15.4172 - mse: 10.8697\n",
      "Epoch 202/1000\n",
      "11/11 - 0s - loss: 15.4092 - mse: 10.8697\n",
      "Epoch 203/1000\n",
      "11/11 - 0s - loss: 15.4013 - mse: 10.8696\n",
      "Epoch 204/1000\n",
      "11/11 - 0s - loss: 15.3934 - mse: 10.8696\n",
      "Epoch 205/1000\n",
      "11/11 - 0s - loss: 15.3855 - mse: 10.8696\n",
      "Epoch 206/1000\n",
      "11/11 - 0s - loss: 15.3777 - mse: 10.8695\n",
      "Epoch 207/1000\n",
      "11/11 - 0s - loss: 15.3699 - mse: 10.8695\n",
      "Epoch 208/1000\n",
      "11/11 - 0s - loss: 15.3621 - mse: 10.8694\n",
      "Epoch 209/1000\n",
      "11/11 - 0s - loss: 15.3544 - mse: 10.8694\n",
      "Epoch 210/1000\n",
      "11/11 - 0s - loss: 15.3467 - mse: 10.8693\n",
      "Epoch 211/1000\n",
      "11/11 - 0s - loss: 15.3390 - mse: 10.8692\n",
      "Epoch 212/1000\n",
      "11/11 - 0s - loss: 15.3314 - mse: 10.8691\n",
      "Epoch 213/1000\n",
      "11/11 - 0s - loss: 15.3238 - mse: 10.8691\n",
      "Epoch 214/1000\n",
      "11/11 - 0s - loss: 15.3162 - mse: 10.8690\n",
      "Epoch 215/1000\n",
      "11/11 - 0s - loss: 15.3087 - mse: 10.8689\n",
      "Epoch 216/1000\n",
      "11/11 - 0s - loss: 15.3012 - mse: 10.8687\n",
      "Epoch 217/1000\n",
      "11/11 - 0s - loss: 15.2937 - mse: 10.8686\n",
      "Epoch 218/1000\n",
      "11/11 - 0s - loss: 15.2862 - mse: 10.8685\n",
      "Epoch 219/1000\n",
      "11/11 - 0s - loss: 15.2788 - mse: 10.8684\n",
      "Epoch 220/1000\n",
      "11/11 - 0s - loss: 15.2714 - mse: 10.8682\n",
      "Epoch 221/1000\n",
      "11/11 - 0s - loss: 15.2641 - mse: 10.8681\n",
      "Epoch 222/1000\n",
      "11/11 - 0s - loss: 15.2567 - mse: 10.8680\n",
      "Epoch 223/1000\n",
      "11/11 - 0s - loss: 15.2494 - mse: 10.8678\n",
      "Epoch 224/1000\n",
      "11/11 - 0s - loss: 15.2422 - mse: 10.8676\n",
      "Epoch 225/1000\n",
      "11/11 - 0s - loss: 15.2349 - mse: 10.8675\n",
      "Epoch 226/1000\n",
      "11/11 - 0s - loss: 15.2277 - mse: 10.8673\n",
      "Epoch 227/1000\n",
      "11/11 - 0s - loss: 15.2205 - mse: 10.8671\n",
      "Epoch 228/1000\n",
      "11/11 - 0s - loss: 15.2133 - mse: 10.8669\n",
      "Epoch 229/1000\n",
      "11/11 - 0s - loss: 15.2062 - mse: 10.8667\n",
      "Epoch 230/1000\n",
      "11/11 - 0s - loss: 15.1991 - mse: 10.8665\n",
      "Epoch 231/1000\n",
      "11/11 - 0s - loss: 15.1920 - mse: 10.8662\n",
      "Epoch 232/1000\n",
      "11/11 - 0s - loss: 15.1849 - mse: 10.8660\n",
      "Epoch 233/1000\n",
      "11/11 - 0s - loss: 15.1779 - mse: 10.8658\n",
      "Epoch 234/1000\n",
      "11/11 - 0s - loss: 15.1709 - mse: 10.8655\n",
      "Epoch 235/1000\n",
      "11/11 - 0s - loss: 15.1639 - mse: 10.8653\n",
      "Epoch 236/1000\n",
      "11/11 - 0s - loss: 15.1569 - mse: 10.8650\n",
      "Epoch 237/1000\n",
      "11/11 - 0s - loss: 15.1500 - mse: 10.8648\n",
      "Epoch 238/1000\n",
      "11/11 - 0s - loss: 15.1431 - mse: 10.8645\n",
      "Epoch 239/1000\n",
      "11/11 - 0s - loss: 15.1362 - mse: 10.8642\n",
      "Epoch 240/1000\n",
      "11/11 - 0s - loss: 15.1293 - mse: 10.8639\n",
      "Epoch 241/1000\n",
      "11/11 - 0s - loss: 15.1225 - mse: 10.8637\n",
      "Epoch 242/1000\n",
      "11/11 - 0s - loss: 15.1157 - mse: 10.8634\n",
      "Epoch 243/1000\n",
      "11/11 - 0s - loss: 15.1089 - mse: 10.8631\n",
      "Epoch 244/1000\n",
      "11/11 - 0s - loss: 15.1021 - mse: 10.8627\n",
      "Epoch 245/1000\n",
      "11/11 - 0s - loss: 15.0954 - mse: 10.8624\n",
      "Epoch 246/1000\n",
      "11/11 - 0s - loss: 15.0887 - mse: 10.8621\n",
      "Epoch 247/1000\n",
      "11/11 - 0s - loss: 15.0820 - mse: 10.8618\n",
      "Epoch 248/1000\n",
      "11/11 - 0s - loss: 15.0753 - mse: 10.8614\n",
      "Epoch 249/1000\n",
      "11/11 - 0s - loss: 15.0687 - mse: 10.8611\n",
      "Epoch 250/1000\n",
      "11/11 - 0s - loss: 15.0620 - mse: 10.8608\n",
      "Epoch 251/1000\n",
      "11/11 - 0s - loss: 15.0554 - mse: 10.8604\n",
      "Epoch 252/1000\n",
      "11/11 - 0s - loss: 15.0489 - mse: 10.8600\n",
      "Epoch 253/1000\n",
      "11/11 - 0s - loss: 15.0423 - mse: 10.8597\n",
      "Epoch 254/1000\n",
      "11/11 - 0s - loss: 15.0358 - mse: 10.8593\n",
      "Epoch 255/1000\n",
      "11/11 - 0s - loss: 15.0293 - mse: 10.8589\n",
      "Epoch 256/1000\n",
      "11/11 - 0s - loss: 15.0228 - mse: 10.8586\n",
      "Epoch 257/1000\n",
      "11/11 - 0s - loss: 15.0163 - mse: 10.8582\n",
      "Epoch 258/1000\n",
      "11/11 - 0s - loss: 15.0099 - mse: 10.8578\n",
      "Epoch 259/1000\n",
      "11/11 - 0s - loss: 15.0034 - mse: 10.8574\n",
      "Epoch 260/1000\n",
      "11/11 - 0s - loss: 14.9970 - mse: 10.8570\n",
      "Epoch 261/1000\n",
      "11/11 - 0s - loss: 14.9907 - mse: 10.8566\n",
      "Epoch 262/1000\n",
      "11/11 - 0s - loss: 14.9843 - mse: 10.8562\n",
      "Epoch 263/1000\n",
      "11/11 - 0s - loss: 14.9780 - mse: 10.8558\n",
      "Epoch 264/1000\n",
      "11/11 - 0s - loss: 14.9717 - mse: 10.8554\n",
      "Epoch 265/1000\n",
      "11/11 - 0s - loss: 14.9654 - mse: 10.8550\n",
      "Epoch 266/1000\n",
      "11/11 - 0s - loss: 14.9591 - mse: 10.8546\n",
      "Epoch 267/1000\n",
      "11/11 - 0s - loss: 14.9529 - mse: 10.8542\n",
      "Epoch 268/1000\n",
      "11/11 - 0s - loss: 14.9466 - mse: 10.8537\n",
      "Epoch 269/1000\n",
      "11/11 - 0s - loss: 14.9404 - mse: 10.8533\n",
      "Epoch 270/1000\n",
      "11/11 - 0s - loss: 14.9342 - mse: 10.8529\n",
      "Epoch 271/1000\n",
      "11/11 - 0s - loss: 14.9281 - mse: 10.8525\n",
      "Epoch 272/1000\n",
      "11/11 - 0s - loss: 14.9219 - mse: 10.8520\n",
      "Epoch 273/1000\n",
      "11/11 - 0s - loss: 14.9158 - mse: 10.8516\n",
      "Epoch 274/1000\n",
      "11/11 - 0s - loss: 14.9097 - mse: 10.8511\n",
      "Epoch 275/1000\n",
      "11/11 - 0s - loss: 14.9037 - mse: 10.8507\n",
      "Epoch 276/1000\n",
      "11/11 - 0s - loss: 14.8976 - mse: 10.8503\n",
      "Epoch 277/1000\n",
      "11/11 - 0s - loss: 14.8916 - mse: 10.8498\n",
      "Epoch 278/1000\n",
      "11/11 - 0s - loss: 14.8856 - mse: 10.8494\n",
      "Epoch 279/1000\n",
      "11/11 - 0s - loss: 14.8796 - mse: 10.8489\n",
      "Epoch 280/1000\n",
      "11/11 - 0s - loss: 14.8736 - mse: 10.8485\n",
      "Epoch 281/1000\n",
      "11/11 - 0s - loss: 14.8676 - mse: 10.8481\n",
      "Epoch 282/1000\n",
      "11/11 - 0s - loss: 14.8617 - mse: 10.8476\n",
      "Epoch 283/1000\n",
      "11/11 - 0s - loss: 14.8558 - mse: 10.8472\n",
      "Epoch 284/1000\n",
      "11/11 - 0s - loss: 14.8499 - mse: 10.8467\n",
      "Epoch 285/1000\n",
      "11/11 - 0s - loss: 14.8441 - mse: 10.8463\n",
      "Epoch 286/1000\n",
      "11/11 - 0s - loss: 14.8382 - mse: 10.8458\n",
      "Epoch 287/1000\n",
      "11/11 - 0s - loss: 14.8324 - mse: 10.8454\n",
      "Epoch 288/1000\n",
      "11/11 - 0s - loss: 14.8266 - mse: 10.8449\n",
      "Epoch 289/1000\n",
      "11/11 - 0s - loss: 14.8208 - mse: 10.8445\n",
      "Epoch 290/1000\n",
      "11/11 - 0s - loss: 14.8150 - mse: 10.8440\n",
      "Epoch 291/1000\n",
      "11/11 - 0s - loss: 14.8093 - mse: 10.8436\n",
      "Epoch 292/1000\n",
      "11/11 - 0s - loss: 14.8036 - mse: 10.8431\n",
      "Epoch 293/1000\n",
      "11/11 - 0s - loss: 14.7979 - mse: 10.8427\n",
      "Epoch 294/1000\n",
      "11/11 - 0s - loss: 14.7922 - mse: 10.8422\n",
      "Epoch 295/1000\n",
      "11/11 - 0s - loss: 14.7866 - mse: 10.8418\n",
      "Epoch 296/1000\n",
      "11/11 - 0s - loss: 14.7809 - mse: 10.8413\n",
      "Epoch 297/1000\n",
      "11/11 - 0s - loss: 14.7753 - mse: 10.8409\n",
      "Epoch 298/1000\n",
      "11/11 - 0s - loss: 14.7697 - mse: 10.8405\n",
      "Epoch 299/1000\n",
      "11/11 - 0s - loss: 14.7641 - mse: 10.8400\n",
      "Epoch 300/1000\n",
      "11/11 - 0s - loss: 14.7586 - mse: 10.8396\n",
      "Epoch 301/1000\n",
      "11/11 - 0s - loss: 14.7530 - mse: 10.8392\n",
      "Epoch 302/1000\n",
      "11/11 - 0s - loss: 14.7475 - mse: 10.8387\n",
      "Epoch 303/1000\n",
      "11/11 - 0s - loss: 14.7420 - mse: 10.8383\n",
      "Epoch 304/1000\n",
      "11/11 - 0s - loss: 14.7366 - mse: 10.8379\n",
      "Epoch 305/1000\n",
      "11/11 - 0s - loss: 14.7311 - mse: 10.8374\n",
      "Epoch 306/1000\n",
      "11/11 - 0s - loss: 14.7257 - mse: 10.8370\n",
      "Epoch 307/1000\n",
      "11/11 - 0s - loss: 14.7203 - mse: 10.8366\n",
      "Epoch 308/1000\n",
      "11/11 - 0s - loss: 14.7149 - mse: 10.8362\n",
      "Epoch 309/1000\n",
      "11/11 - 0s - loss: 14.7095 - mse: 10.8357\n",
      "Epoch 310/1000\n",
      "11/11 - 0s - loss: 14.7042 - mse: 10.8353\n",
      "Epoch 311/1000\n",
      "11/11 - 0s - loss: 14.6988 - mse: 10.8349\n",
      "Epoch 312/1000\n",
      "11/11 - 0s - loss: 14.6935 - mse: 10.8345\n",
      "Epoch 313/1000\n",
      "11/11 - 0s - loss: 14.6882 - mse: 10.8341\n",
      "Epoch 314/1000\n",
      "11/11 - 0s - loss: 14.6830 - mse: 10.8337\n",
      "Epoch 315/1000\n",
      "11/11 - 0s - loss: 14.6777 - mse: 10.8333\n",
      "Epoch 316/1000\n",
      "11/11 - 0s - loss: 14.6725 - mse: 10.8329\n",
      "Epoch 317/1000\n",
      "11/11 - 0s - loss: 14.6673 - mse: 10.8325\n",
      "Epoch 318/1000\n",
      "11/11 - 0s - loss: 14.6621 - mse: 10.8321\n",
      "Epoch 319/1000\n",
      "11/11 - 0s - loss: 14.6569 - mse: 10.8317\n",
      "Epoch 320/1000\n",
      "11/11 - 0s - loss: 14.6518 - mse: 10.8313\n",
      "Epoch 321/1000\n",
      "11/11 - 0s - loss: 14.6466 - mse: 10.8310\n",
      "Epoch 322/1000\n",
      "11/11 - 0s - loss: 14.6415 - mse: 10.8306\n",
      "Epoch 323/1000\n",
      "11/11 - 0s - loss: 14.6365 - mse: 10.8302\n",
      "Epoch 324/1000\n",
      "11/11 - 0s - loss: 14.6314 - mse: 10.8298\n",
      "Epoch 325/1000\n",
      "11/11 - 0s - loss: 14.6263 - mse: 10.8295\n",
      "Epoch 326/1000\n",
      "11/11 - 0s - loss: 14.6213 - mse: 10.8291\n",
      "Epoch 327/1000\n",
      "11/11 - 0s - loss: 14.6163 - mse: 10.8288\n",
      "Epoch 328/1000\n",
      "11/11 - 0s - loss: 14.6113 - mse: 10.8284\n",
      "Epoch 329/1000\n",
      "11/11 - 0s - loss: 14.6064 - mse: 10.8281\n",
      "Epoch 330/1000\n",
      "11/11 - 0s - loss: 14.6014 - mse: 10.8277\n",
      "Epoch 331/1000\n",
      "11/11 - 0s - loss: 14.5965 - mse: 10.8274\n",
      "Epoch 332/1000\n",
      "11/11 - 0s - loss: 14.5916 - mse: 10.8270\n",
      "Epoch 333/1000\n",
      "11/11 - 0s - loss: 14.5867 - mse: 10.8267\n",
      "Epoch 334/1000\n",
      "11/11 - 0s - loss: 14.5818 - mse: 10.8264\n",
      "Epoch 335/1000\n",
      "11/11 - 0s - loss: 14.5770 - mse: 10.8261\n",
      "Epoch 336/1000\n",
      "11/11 - 0s - loss: 14.5722 - mse: 10.8258\n",
      "Epoch 337/1000\n",
      "11/11 - 0s - loss: 14.5673 - mse: 10.8254\n",
      "Epoch 338/1000\n",
      "11/11 - 0s - loss: 14.5626 - mse: 10.8251\n",
      "Epoch 339/1000\n",
      "11/11 - 0s - loss: 14.5578 - mse: 10.8248\n",
      "Epoch 340/1000\n",
      "11/11 - 0s - loss: 14.5530 - mse: 10.8245\n",
      "Epoch 341/1000\n",
      "11/11 - 0s - loss: 14.5483 - mse: 10.8243\n",
      "Epoch 342/1000\n",
      "11/11 - 0s - loss: 14.5436 - mse: 10.8240\n",
      "Epoch 343/1000\n",
      "11/11 - 0s - loss: 14.5389 - mse: 10.8237\n",
      "Epoch 344/1000\n",
      "11/11 - 0s - loss: 14.5343 - mse: 10.8234\n",
      "Epoch 345/1000\n",
      "11/11 - 0s - loss: 14.5296 - mse: 10.8231\n",
      "Epoch 346/1000\n",
      "11/11 - 0s - loss: 14.5250 - mse: 10.8229\n",
      "Epoch 347/1000\n",
      "11/11 - 0s - loss: 14.5204 - mse: 10.8226\n",
      "Epoch 348/1000\n",
      "11/11 - 0s - loss: 14.5158 - mse: 10.8224\n",
      "Epoch 349/1000\n",
      "11/11 - 0s - loss: 14.5112 - mse: 10.8221\n",
      "Epoch 350/1000\n",
      "11/11 - 0s - loss: 14.5067 - mse: 10.8219\n",
      "Epoch 351/1000\n",
      "11/11 - 0s - loss: 14.5021 - mse: 10.8216\n",
      "Epoch 352/1000\n",
      "11/11 - 0s - loss: 14.4976 - mse: 10.8214\n",
      "Epoch 353/1000\n",
      "11/11 - 0s - loss: 14.4931 - mse: 10.8212\n",
      "Epoch 354/1000\n",
      "11/11 - 0s - loss: 14.4886 - mse: 10.8209\n",
      "Epoch 355/1000\n",
      "11/11 - 0s - loss: 14.4842 - mse: 10.8207\n",
      "Epoch 356/1000\n",
      "11/11 - 0s - loss: 14.4798 - mse: 10.8205\n",
      "Epoch 357/1000\n",
      "11/11 - 0s - loss: 14.4753 - mse: 10.8203\n",
      "Epoch 358/1000\n",
      "11/11 - 0s - loss: 14.4710 - mse: 10.8201\n",
      "Epoch 359/1000\n",
      "11/11 - 0s - loss: 14.4666 - mse: 10.8199\n",
      "Epoch 360/1000\n",
      "11/11 - 0s - loss: 14.4622 - mse: 10.8197\n",
      "Epoch 361/1000\n",
      "11/11 - 0s - loss: 14.4579 - mse: 10.8195\n",
      "Epoch 362/1000\n",
      "11/11 - 0s - loss: 14.4536 - mse: 10.8193\n",
      "Epoch 363/1000\n",
      "11/11 - 0s - loss: 14.4493 - mse: 10.8192\n",
      "Epoch 364/1000\n",
      "11/11 - 0s - loss: 14.4450 - mse: 10.8190\n",
      "Epoch 365/1000\n",
      "11/11 - 0s - loss: 14.4407 - mse: 10.8188\n",
      "Epoch 366/1000\n",
      "11/11 - 0s - loss: 14.4365 - mse: 10.8187\n",
      "Epoch 367/1000\n",
      "11/11 - 0s - loss: 14.4323 - mse: 10.8185\n",
      "Epoch 368/1000\n",
      "11/11 - 0s - loss: 14.4281 - mse: 10.8184\n",
      "Epoch 369/1000\n",
      "11/11 - 0s - loss: 14.4239 - mse: 10.8182\n",
      "Epoch 370/1000\n",
      "11/11 - 0s - loss: 14.4197 - mse: 10.8181\n",
      "Epoch 371/1000\n",
      "11/11 - 0s - loss: 14.4156 - mse: 10.8180\n",
      "Epoch 372/1000\n",
      "11/11 - 0s - loss: 14.4114 - mse: 10.8179\n",
      "Epoch 373/1000\n",
      "11/11 - 0s - loss: 14.4073 - mse: 10.8177\n",
      "Epoch 374/1000\n",
      "11/11 - 0s - loss: 14.4032 - mse: 10.8176\n",
      "Epoch 375/1000\n",
      "11/11 - 0s - loss: 14.3992 - mse: 10.8175\n",
      "Epoch 376/1000\n",
      "11/11 - 0s - loss: 14.3951 - mse: 10.8174\n",
      "Epoch 377/1000\n",
      "11/11 - 0s - loss: 14.3911 - mse: 10.8173\n",
      "Epoch 378/1000\n",
      "11/11 - 0s - loss: 14.3871 - mse: 10.8173\n",
      "Epoch 379/1000\n",
      "11/11 - 0s - loss: 14.3831 - mse: 10.8172\n",
      "Epoch 380/1000\n",
      "11/11 - 0s - loss: 14.3791 - mse: 10.8171\n",
      "Epoch 381/1000\n",
      "11/11 - 0s - loss: 14.3752 - mse: 10.8170\n",
      "Epoch 382/1000\n",
      "11/11 - 0s - loss: 14.3712 - mse: 10.8170\n",
      "Epoch 383/1000\n",
      "11/11 - 0s - loss: 14.3673 - mse: 10.8169\n",
      "Epoch 384/1000\n",
      "11/11 - 0s - loss: 14.3634 - mse: 10.8169\n",
      "Epoch 385/1000\n",
      "11/11 - 0s - loss: 14.3595 - mse: 10.8168\n",
      "Epoch 386/1000\n",
      "11/11 - 0s - loss: 14.3557 - mse: 10.8168\n",
      "Epoch 387/1000\n",
      "11/11 - 0s - loss: 14.3518 - mse: 10.8168\n",
      "Epoch 388/1000\n",
      "11/11 - 0s - loss: 14.3480 - mse: 10.8167\n",
      "Epoch 389/1000\n",
      "11/11 - 0s - loss: 14.3442 - mse: 10.8167\n",
      "Epoch 390/1000\n",
      "11/11 - 0s - loss: 14.3404 - mse: 10.8167\n",
      "Epoch 391/1000\n",
      "11/11 - 0s - loss: 14.3366 - mse: 10.8167\n",
      "Epoch 392/1000\n",
      "11/11 - 0s - loss: 14.3329 - mse: 10.8167\n",
      "Epoch 393/1000\n",
      "11/11 - 0s - loss: 14.3291 - mse: 10.8167\n",
      "Epoch 394/1000\n",
      "11/11 - 0s - loss: 14.3254 - mse: 10.8167\n",
      "Epoch 395/1000\n",
      "11/11 - 0s - loss: 14.3217 - mse: 10.8167\n",
      "Epoch 396/1000\n",
      "11/11 - 0s - loss: 14.3180 - mse: 10.8167\n",
      "Epoch 397/1000\n",
      "11/11 - 0s - loss: 14.3144 - mse: 10.8167\n",
      "Epoch 398/1000\n",
      "11/11 - 0s - loss: 14.3107 - mse: 10.8168\n",
      "Epoch 399/1000\n",
      "11/11 - 0s - loss: 14.3071 - mse: 10.8168\n",
      "Epoch 400/1000\n",
      "11/11 - 0s - loss: 14.3035 - mse: 10.8169\n",
      "Epoch 401/1000\n",
      "11/11 - 0s - loss: 14.2999 - mse: 10.8169\n",
      "Epoch 402/1000\n",
      "11/11 - 0s - loss: 14.2963 - mse: 10.8170\n",
      "Epoch 403/1000\n",
      "11/11 - 0s - loss: 14.2928 - mse: 10.8170\n",
      "Epoch 404/1000\n",
      "11/11 - 0s - loss: 14.2893 - mse: 10.8171\n",
      "Epoch 405/1000\n",
      "11/11 - 0s - loss: 14.2857 - mse: 10.8172\n",
      "Epoch 406/1000\n",
      "11/11 - 0s - loss: 14.2822 - mse: 10.8172\n",
      "Epoch 407/1000\n",
      "11/11 - 0s - loss: 14.2788 - mse: 10.8173\n",
      "Epoch 408/1000\n",
      "11/11 - 0s - loss: 14.2753 - mse: 10.8174\n",
      "Epoch 409/1000\n",
      "11/11 - 0s - loss: 14.2718 - mse: 10.8175\n",
      "Epoch 410/1000\n",
      "11/11 - 0s - loss: 14.2684 - mse: 10.8176\n",
      "Epoch 411/1000\n",
      "11/11 - 0s - loss: 14.2650 - mse: 10.8177\n",
      "Epoch 412/1000\n",
      "11/11 - 0s - loss: 14.2616 - mse: 10.8178\n",
      "Epoch 413/1000\n",
      "11/11 - 0s - loss: 14.2582 - mse: 10.8179\n",
      "Epoch 414/1000\n",
      "11/11 - 0s - loss: 14.2549 - mse: 10.8181\n",
      "Epoch 415/1000\n",
      "11/11 - 0s - loss: 14.2515 - mse: 10.8182\n",
      "Epoch 416/1000\n",
      "11/11 - 0s - loss: 14.2482 - mse: 10.8183\n",
      "Epoch 417/1000\n",
      "11/11 - 0s - loss: 14.2449 - mse: 10.8185\n",
      "Epoch 418/1000\n",
      "11/11 - 0s - loss: 14.2416 - mse: 10.8186\n",
      "Epoch 419/1000\n",
      "11/11 - 0s - loss: 14.2384 - mse: 10.8188\n",
      "Epoch 420/1000\n",
      "11/11 - 0s - loss: 14.2351 - mse: 10.8189\n",
      "Epoch 421/1000\n",
      "11/11 - 0s - loss: 14.2319 - mse: 10.8191\n",
      "Epoch 422/1000\n",
      "11/11 - 0s - loss: 14.2287 - mse: 10.8193\n",
      "Epoch 423/1000\n",
      "11/11 - 0s - loss: 14.2254 - mse: 10.8194\n",
      "Epoch 424/1000\n",
      "11/11 - 0s - loss: 14.2223 - mse: 10.8196\n",
      "Epoch 425/1000\n",
      "11/11 - 0s - loss: 14.2191 - mse: 10.8198\n",
      "Epoch 426/1000\n",
      "11/11 - 0s - loss: 14.2159 - mse: 10.8200\n",
      "Epoch 427/1000\n",
      "11/11 - 0s - loss: 14.2128 - mse: 10.8202\n",
      "Epoch 428/1000\n",
      "11/11 - 0s - loss: 14.2097 - mse: 10.8204\n",
      "Epoch 429/1000\n",
      "11/11 - 0s - loss: 14.2066 - mse: 10.8206\n",
      "Epoch 430/1000\n",
      "11/11 - 0s - loss: 14.2035 - mse: 10.8208\n",
      "Epoch 431/1000\n",
      "11/11 - 0s - loss: 14.2005 - mse: 10.8210\n",
      "Epoch 432/1000\n",
      "11/11 - 0s - loss: 14.1974 - mse: 10.8212\n",
      "Epoch 433/1000\n",
      "11/11 - 0s - loss: 14.1944 - mse: 10.8215\n",
      "Epoch 434/1000\n",
      "11/11 - 0s - loss: 14.1914 - mse: 10.8217\n",
      "Epoch 435/1000\n",
      "11/11 - 0s - loss: 14.1884 - mse: 10.8219\n",
      "Epoch 436/1000\n",
      "11/11 - 0s - loss: 14.1854 - mse: 10.8222\n",
      "Epoch 437/1000\n",
      "11/11 - 0s - loss: 14.1824 - mse: 10.8224\n",
      "Epoch 438/1000\n",
      "11/11 - 0s - loss: 14.1795 - mse: 10.8227\n",
      "Epoch 439/1000\n",
      "11/11 - 0s - loss: 14.1765 - mse: 10.8229\n",
      "Epoch 440/1000\n",
      "11/11 - 0s - loss: 14.1736 - mse: 10.8232\n",
      "Epoch 441/1000\n",
      "11/11 - 0s - loss: 14.1707 - mse: 10.8235\n",
      "Epoch 442/1000\n",
      "11/11 - 0s - loss: 14.1678 - mse: 10.8237\n",
      "Epoch 443/1000\n",
      "11/11 - 0s - loss: 14.1650 - mse: 10.8240\n",
      "Epoch 444/1000\n",
      "11/11 - 0s - loss: 14.1621 - mse: 10.8243\n",
      "Epoch 445/1000\n",
      "11/11 - 0s - loss: 14.1593 - mse: 10.8246\n",
      "Epoch 446/1000\n",
      "11/11 - 0s - loss: 14.1565 - mse: 10.8249\n",
      "Epoch 447/1000\n",
      "11/11 - 0s - loss: 14.1537 - mse: 10.8252\n",
      "Epoch 448/1000\n",
      "11/11 - 0s - loss: 14.1509 - mse: 10.8255\n",
      "Epoch 449/1000\n",
      "11/11 - 0s - loss: 14.1481 - mse: 10.8258\n",
      "Epoch 450/1000\n",
      "11/11 - 0s - loss: 14.1454 - mse: 10.8261\n",
      "Epoch 451/1000\n",
      "11/11 - 0s - loss: 14.1426 - mse: 10.8264\n",
      "Epoch 452/1000\n",
      "11/11 - 0s - loss: 14.1399 - mse: 10.8267\n",
      "Epoch 453/1000\n",
      "11/11 - 0s - loss: 14.1372 - mse: 10.8271\n",
      "Epoch 454/1000\n",
      "11/11 - 0s - loss: 14.1345 - mse: 10.8274\n",
      "Epoch 455/1000\n",
      "11/11 - 0s - loss: 14.1318 - mse: 10.8277\n",
      "Epoch 456/1000\n",
      "11/11 - 0s - loss: 14.1292 - mse: 10.8281\n",
      "Epoch 457/1000\n",
      "11/11 - 0s - loss: 14.1265 - mse: 10.8284\n",
      "Epoch 458/1000\n",
      "11/11 - 0s - loss: 14.1239 - mse: 10.8288\n",
      "Epoch 459/1000\n",
      "11/11 - 0s - loss: 14.1213 - mse: 10.8291\n",
      "Epoch 460/1000\n",
      "11/11 - 0s - loss: 14.1187 - mse: 10.8295\n",
      "Epoch 461/1000\n",
      "11/11 - 0s - loss: 14.1161 - mse: 10.8299\n",
      "Epoch 462/1000\n",
      "11/11 - 0s - loss: 14.1135 - mse: 10.8302\n",
      "Epoch 463/1000\n",
      "11/11 - 0s - loss: 14.1110 - mse: 10.8306\n",
      "Epoch 464/1000\n",
      "11/11 - 0s - loss: 14.1085 - mse: 10.8310\n",
      "Epoch 465/1000\n",
      "11/11 - 0s - loss: 14.1059 - mse: 10.8314\n",
      "Epoch 466/1000\n",
      "11/11 - 0s - loss: 14.1034 - mse: 10.8317\n",
      "Epoch 467/1000\n",
      "11/11 - 0s - loss: 14.1010 - mse: 10.8321\n",
      "Epoch 468/1000\n",
      "11/11 - 0s - loss: 14.0985 - mse: 10.8325\n",
      "Epoch 469/1000\n",
      "11/11 - 0s - loss: 14.0960 - mse: 10.8329\n",
      "Epoch 470/1000\n",
      "11/11 - 0s - loss: 14.0936 - mse: 10.8333\n",
      "Epoch 471/1000\n",
      "11/11 - 0s - loss: 14.0911 - mse: 10.8337\n",
      "Epoch 472/1000\n",
      "11/11 - 0s - loss: 14.0887 - mse: 10.8341\n",
      "Epoch 473/1000\n",
      "11/11 - 0s - loss: 14.0863 - mse: 10.8346\n",
      "Epoch 474/1000\n",
      "11/11 - 0s - loss: 14.0840 - mse: 10.8350\n",
      "Epoch 475/1000\n",
      "11/11 - 0s - loss: 14.0816 - mse: 10.8354\n",
      "Epoch 476/1000\n",
      "11/11 - 0s - loss: 14.0792 - mse: 10.8358\n",
      "Epoch 477/1000\n",
      "11/11 - 0s - loss: 14.0769 - mse: 10.8363\n",
      "Epoch 478/1000\n",
      "11/11 - 0s - loss: 14.0746 - mse: 10.8367\n",
      "Epoch 479/1000\n",
      "11/11 - 0s - loss: 14.0723 - mse: 10.8371\n",
      "Epoch 480/1000\n",
      "11/11 - 0s - loss: 14.0700 - mse: 10.8376\n",
      "Epoch 481/1000\n",
      "11/11 - 0s - loss: 14.0677 - mse: 10.8380\n",
      "Epoch 482/1000\n",
      "11/11 - 0s - loss: 14.0654 - mse: 10.8385\n",
      "Epoch 483/1000\n",
      "11/11 - 0s - loss: 14.0632 - mse: 10.8389\n",
      "Epoch 484/1000\n",
      "11/11 - 0s - loss: 14.0609 - mse: 10.8394\n",
      "Epoch 485/1000\n",
      "11/11 - 0s - loss: 14.0587 - mse: 10.8398\n",
      "Epoch 486/1000\n",
      "11/11 - 0s - loss: 14.0565 - mse: 10.8403\n",
      "Epoch 487/1000\n",
      "11/11 - 0s - loss: 14.0543 - mse: 10.8408\n",
      "Epoch 488/1000\n",
      "11/11 - 0s - loss: 14.0521 - mse: 10.8413\n",
      "Epoch 489/1000\n",
      "11/11 - 0s - loss: 14.0499 - mse: 10.8417\n",
      "Epoch 490/1000\n",
      "11/11 - 0s - loss: 14.0478 - mse: 10.8422\n",
      "Epoch 491/1000\n",
      "11/11 - 0s - loss: 14.0457 - mse: 10.8427\n",
      "Epoch 492/1000\n",
      "11/11 - 0s - loss: 14.0435 - mse: 10.8432\n",
      "Epoch 493/1000\n",
      "11/11 - 0s - loss: 14.0414 - mse: 10.8437\n",
      "Epoch 494/1000\n",
      "11/11 - 0s - loss: 14.0393 - mse: 10.8442\n",
      "Epoch 495/1000\n",
      "11/11 - 0s - loss: 14.0372 - mse: 10.8447\n",
      "Epoch 496/1000\n",
      "11/11 - 0s - loss: 14.0352 - mse: 10.8452\n",
      "Epoch 497/1000\n",
      "11/11 - 0s - loss: 14.0331 - mse: 10.8457\n",
      "Epoch 498/1000\n",
      "11/11 - 0s - loss: 14.0311 - mse: 10.8462\n",
      "Epoch 499/1000\n",
      "11/11 - 0s - loss: 14.0290 - mse: 10.8467\n",
      "Epoch 500/1000\n",
      "11/11 - 0s - loss: 14.0270 - mse: 10.8472\n",
      "Epoch 501/1000\n",
      "11/11 - 0s - loss: 14.0250 - mse: 10.8477\n",
      "Epoch 502/1000\n",
      "11/11 - 0s - loss: 14.0230 - mse: 10.8482\n",
      "Epoch 503/1000\n",
      "11/11 - 0s - loss: 14.0211 - mse: 10.8488\n",
      "Epoch 504/1000\n",
      "11/11 - 0s - loss: 14.0191 - mse: 10.8493\n",
      "Epoch 505/1000\n",
      "11/11 - 0s - loss: 14.0172 - mse: 10.8498\n",
      "Epoch 506/1000\n",
      "11/11 - 0s - loss: 14.0152 - mse: 10.8504\n",
      "Epoch 507/1000\n",
      "11/11 - 0s - loss: 14.0133 - mse: 10.8509\n",
      "Epoch 508/1000\n",
      "11/11 - 0s - loss: 14.0114 - mse: 10.8514\n",
      "Epoch 509/1000\n",
      "11/11 - 0s - loss: 14.0095 - mse: 10.8520\n",
      "Epoch 510/1000\n",
      "11/11 - 0s - loss: 14.0076 - mse: 10.8525\n",
      "Epoch 511/1000\n",
      "11/11 - 0s - loss: 14.0057 - mse: 10.8531\n",
      "Epoch 512/1000\n",
      "11/11 - 0s - loss: 14.0039 - mse: 10.8536\n",
      "Epoch 513/1000\n",
      "11/11 - 0s - loss: 14.0020 - mse: 10.8542\n",
      "Epoch 514/1000\n",
      "11/11 - 0s - loss: 14.0002 - mse: 10.8547\n",
      "Epoch 515/1000\n",
      "11/11 - 0s - loss: 13.9984 - mse: 10.8553\n",
      "Epoch 516/1000\n",
      "11/11 - 0s - loss: 13.9966 - mse: 10.8558\n",
      "Epoch 517/1000\n",
      "11/11 - 0s - loss: 13.9948 - mse: 10.8564\n",
      "Epoch 518/1000\n",
      "11/11 - 0s - loss: 13.9930 - mse: 10.8570\n",
      "Epoch 519/1000\n",
      "11/11 - 0s - loss: 13.9912 - mse: 10.8575\n",
      "Epoch 520/1000\n",
      "11/11 - 0s - loss: 13.9895 - mse: 10.8581\n",
      "Epoch 521/1000\n",
      "11/11 - 0s - loss: 13.9877 - mse: 10.8587\n",
      "Epoch 522/1000\n",
      "11/11 - 0s - loss: 13.9860 - mse: 10.8593\n",
      "Epoch 523/1000\n",
      "11/11 - 0s - loss: 13.9843 - mse: 10.8598\n",
      "Epoch 524/1000\n",
      "11/11 - 0s - loss: 13.9826 - mse: 10.8604\n",
      "Epoch 525/1000\n",
      "11/11 - 0s - loss: 13.9809 - mse: 10.8610\n",
      "Epoch 526/1000\n",
      "11/11 - 0s - loss: 13.9792 - mse: 10.8616\n",
      "Epoch 527/1000\n",
      "11/11 - 0s - loss: 13.9775 - mse: 10.8622\n",
      "Epoch 528/1000\n",
      "11/11 - 0s - loss: 13.9759 - mse: 10.8628\n",
      "Epoch 529/1000\n",
      "11/11 - 0s - loss: 13.9742 - mse: 10.8634\n",
      "Epoch 530/1000\n",
      "11/11 - 0s - loss: 13.9726 - mse: 10.8640\n",
      "Epoch 531/1000\n",
      "11/11 - 0s - loss: 13.9710 - mse: 10.8646\n",
      "Epoch 532/1000\n",
      "11/11 - 0s - loss: 13.9694 - mse: 10.8652\n",
      "Epoch 533/1000\n",
      "11/11 - 0s - loss: 13.9678 - mse: 10.8658\n",
      "Epoch 534/1000\n",
      "11/11 - 0s - loss: 13.9662 - mse: 10.8664\n",
      "Epoch 535/1000\n",
      "11/11 - 0s - loss: 13.9646 - mse: 10.8670\n",
      "Epoch 536/1000\n",
      "11/11 - 0s - loss: 13.9630 - mse: 10.8676\n",
      "Epoch 537/1000\n",
      "11/11 - 0s - loss: 13.9615 - mse: 10.8682\n",
      "Epoch 538/1000\n",
      "11/11 - 0s - loss: 13.9599 - mse: 10.8688\n",
      "Epoch 539/1000\n",
      "11/11 - 0s - loss: 13.9584 - mse: 10.8694\n",
      "Epoch 540/1000\n",
      "11/11 - 0s - loss: 13.9569 - mse: 10.8700\n",
      "Epoch 541/1000\n",
      "11/11 - 0s - loss: 13.9554 - mse: 10.8706\n",
      "Epoch 542/1000\n",
      "11/11 - 0s - loss: 13.9539 - mse: 10.8713\n",
      "Epoch 543/1000\n",
      "11/11 - 0s - loss: 13.9524 - mse: 10.8719\n",
      "Epoch 544/1000\n",
      "11/11 - 0s - loss: 13.9509 - mse: 10.8725\n",
      "Epoch 545/1000\n",
      "11/11 - 0s - loss: 13.9495 - mse: 10.8731\n",
      "Epoch 546/1000\n",
      "11/11 - 0s - loss: 13.9480 - mse: 10.8738\n",
      "Epoch 547/1000\n",
      "11/11 - 0s - loss: 13.9466 - mse: 10.8744\n",
      "Epoch 548/1000\n",
      "11/11 - 0s - loss: 13.9451 - mse: 10.8750\n",
      "Epoch 549/1000\n",
      "11/11 - 0s - loss: 13.9437 - mse: 10.8757\n",
      "Epoch 550/1000\n",
      "11/11 - 0s - loss: 13.9423 - mse: 10.8763\n",
      "Epoch 551/1000\n",
      "11/11 - 0s - loss: 13.9409 - mse: 10.8769\n",
      "Epoch 552/1000\n",
      "11/11 - 0s - loss: 13.9395 - mse: 10.8776\n",
      "Epoch 553/1000\n",
      "11/11 - 0s - loss: 13.9382 - mse: 10.8782\n",
      "Epoch 554/1000\n",
      "11/11 - 0s - loss: 13.9368 - mse: 10.8788\n",
      "Epoch 555/1000\n",
      "11/11 - 0s - loss: 13.9354 - mse: 10.8795\n",
      "Epoch 556/1000\n",
      "11/11 - 0s - loss: 13.9341 - mse: 10.8801\n",
      "Epoch 557/1000\n",
      "11/11 - 0s - loss: 13.9328 - mse: 10.8808\n",
      "Epoch 558/1000\n",
      "11/11 - 0s - loss: 13.9314 - mse: 10.8814\n",
      "Epoch 559/1000\n",
      "11/11 - 0s - loss: 13.9301 - mse: 10.8821\n",
      "Epoch 560/1000\n",
      "11/11 - 0s - loss: 13.9288 - mse: 10.8827\n",
      "Epoch 561/1000\n",
      "11/11 - 0s - loss: 13.9275 - mse: 10.8834\n",
      "Epoch 562/1000\n",
      "11/11 - 0s - loss: 13.9263 - mse: 10.8840\n",
      "Epoch 563/1000\n",
      "11/11 - 0s - loss: 13.9250 - mse: 10.8847\n",
      "Epoch 564/1000\n",
      "11/11 - 0s - loss: 13.9237 - mse: 10.8853\n",
      "Epoch 565/1000\n",
      "11/11 - 0s - loss: 13.9225 - mse: 10.8860\n",
      "Epoch 566/1000\n",
      "11/11 - 0s - loss: 13.9212 - mse: 10.8866\n",
      "Epoch 567/1000\n",
      "11/11 - 0s - loss: 13.9200 - mse: 10.8873\n",
      "Epoch 568/1000\n",
      "11/11 - 0s - loss: 13.9188 - mse: 10.8879\n",
      "Epoch 569/1000\n",
      "11/11 - 0s - loss: 13.9176 - mse: 10.8886\n",
      "Epoch 570/1000\n",
      "11/11 - 0s - loss: 13.9164 - mse: 10.8892\n",
      "Epoch 571/1000\n",
      "11/11 - 0s - loss: 13.9152 - mse: 10.8899\n",
      "Epoch 572/1000\n",
      "11/11 - 0s - loss: 13.9140 - mse: 10.8906\n",
      "Epoch 573/1000\n",
      "11/11 - 0s - loss: 13.9128 - mse: 10.8912\n",
      "Epoch 574/1000\n",
      "11/11 - 0s - loss: 13.9116 - mse: 10.8919\n",
      "Epoch 575/1000\n",
      "11/11 - 0s - loss: 13.9105 - mse: 10.8925\n",
      "Epoch 576/1000\n",
      "11/11 - 0s - loss: 13.9093 - mse: 10.8932\n",
      "Epoch 577/1000\n",
      "11/11 - 0s - loss: 13.9082 - mse: 10.8939\n",
      "Epoch 578/1000\n",
      "11/11 - 0s - loss: 13.9071 - mse: 10.8945\n",
      "Epoch 579/1000\n",
      "11/11 - 0s - loss: 13.9060 - mse: 10.8952\n",
      "Epoch 580/1000\n",
      "11/11 - 0s - loss: 13.9049 - mse: 10.8959\n",
      "Epoch 581/1000\n",
      "11/11 - 0s - loss: 13.9038 - mse: 10.8965\n",
      "Epoch 582/1000\n",
      "11/11 - 0s - loss: 13.9027 - mse: 10.8972\n",
      "Epoch 583/1000\n",
      "11/11 - 0s - loss: 13.9016 - mse: 10.8979\n",
      "Epoch 584/1000\n",
      "11/11 - 0s - loss: 13.9005 - mse: 10.8985\n",
      "Epoch 585/1000\n",
      "11/11 - 0s - loss: 13.8994 - mse: 10.8992\n",
      "Epoch 586/1000\n",
      "11/11 - 0s - loss: 13.8984 - mse: 10.8999\n",
      "Epoch 587/1000\n",
      "11/11 - 0s - loss: 13.8973 - mse: 10.9006\n",
      "Epoch 588/1000\n",
      "11/11 - 0s - loss: 13.8963 - mse: 10.9012\n",
      "Epoch 589/1000\n",
      "11/11 - 0s - loss: 13.8953 - mse: 10.9019\n",
      "Epoch 590/1000\n",
      "11/11 - 0s - loss: 13.8943 - mse: 10.9026\n",
      "Epoch 591/1000\n",
      "11/11 - 0s - loss: 13.8933 - mse: 10.9032\n",
      "Epoch 592/1000\n",
      "11/11 - 0s - loss: 13.8923 - mse: 10.9039\n",
      "Epoch 593/1000\n",
      "11/11 - 0s - loss: 13.8913 - mse: 10.9046\n",
      "Epoch 594/1000\n",
      "11/11 - 0s - loss: 13.8903 - mse: 10.9053\n",
      "Epoch 595/1000\n",
      "11/11 - 0s - loss: 13.8893 - mse: 10.9059\n",
      "Epoch 596/1000\n",
      "11/11 - 0s - loss: 13.8883 - mse: 10.9066\n",
      "Epoch 597/1000\n",
      "11/11 - 0s - loss: 13.8874 - mse: 10.9073\n",
      "Epoch 598/1000\n",
      "11/11 - 0s - loss: 13.8864 - mse: 10.9080\n",
      "Epoch 599/1000\n",
      "11/11 - 0s - loss: 13.8855 - mse: 10.9086\n",
      "Epoch 600/1000\n",
      "11/11 - 0s - loss: 13.8845 - mse: 10.9093\n",
      "Epoch 601/1000\n",
      "11/11 - 0s - loss: 13.8836 - mse: 10.9100\n",
      "Epoch 602/1000\n",
      "11/11 - 0s - loss: 13.8827 - mse: 10.9106\n",
      "Epoch 603/1000\n",
      "11/11 - 0s - loss: 13.8818 - mse: 10.9113\n",
      "Epoch 604/1000\n",
      "11/11 - 0s - loss: 13.8809 - mse: 10.9120\n",
      "Epoch 605/1000\n",
      "11/11 - 0s - loss: 13.8800 - mse: 10.9127\n",
      "Epoch 606/1000\n",
      "11/11 - 0s - loss: 13.8791 - mse: 10.9133\n",
      "Epoch 607/1000\n",
      "11/11 - 0s - loss: 13.8782 - mse: 10.9140\n",
      "Epoch 608/1000\n",
      "11/11 - 0s - loss: 13.8773 - mse: 10.9147\n",
      "Epoch 609/1000\n",
      "11/11 - 0s - loss: 13.8765 - mse: 10.9154\n",
      "Epoch 610/1000\n",
      "11/11 - 0s - loss: 13.8756 - mse: 10.9160\n",
      "Epoch 611/1000\n",
      "11/11 - 0s - loss: 13.8748 - mse: 10.9167\n",
      "Epoch 612/1000\n",
      "11/11 - 0s - loss: 13.8739 - mse: 10.9174\n",
      "Epoch 613/1000\n",
      "11/11 - 0s - loss: 13.8731 - mse: 10.9181\n",
      "Epoch 614/1000\n",
      "11/11 - 0s - loss: 13.8723 - mse: 10.9187\n",
      "Epoch 615/1000\n",
      "11/11 - 0s - loss: 13.8715 - mse: 10.9194\n",
      "Epoch 616/1000\n",
      "11/11 - 0s - loss: 13.8706 - mse: 10.9201\n",
      "Epoch 617/1000\n",
      "11/11 - 0s - loss: 13.8698 - mse: 10.9208\n",
      "Epoch 618/1000\n",
      "11/11 - 0s - loss: 13.8690 - mse: 10.9214\n",
      "Epoch 619/1000\n",
      "11/11 - 0s - loss: 13.8683 - mse: 10.9221\n",
      "Epoch 620/1000\n",
      "11/11 - 0s - loss: 13.8675 - mse: 10.9228\n",
      "Epoch 621/1000\n",
      "11/11 - 0s - loss: 13.8667 - mse: 10.9234\n",
      "Epoch 622/1000\n",
      "11/11 - 0s - loss: 13.8659 - mse: 10.9241\n",
      "Epoch 623/1000\n",
      "11/11 - 0s - loss: 13.8652 - mse: 10.9248\n",
      "Epoch 624/1000\n",
      "11/11 - 0s - loss: 13.8644 - mse: 10.9254\n",
      "Epoch 625/1000\n",
      "11/11 - 0s - loss: 13.8637 - mse: 10.9261\n",
      "Epoch 626/1000\n",
      "11/11 - 0s - loss: 13.8629 - mse: 10.9268\n",
      "Epoch 627/1000\n",
      "11/11 - 0s - loss: 13.8622 - mse: 10.9275\n",
      "Epoch 628/1000\n",
      "11/11 - 0s - loss: 13.8615 - mse: 10.9281\n",
      "Epoch 629/1000\n",
      "11/11 - 0s - loss: 13.8607 - mse: 10.9288\n",
      "Epoch 630/1000\n",
      "11/11 - 0s - loss: 13.8600 - mse: 10.9295\n",
      "Epoch 631/1000\n",
      "11/11 - 0s - loss: 13.8593 - mse: 10.9301\n",
      "Epoch 632/1000\n",
      "11/11 - 0s - loss: 13.8586 - mse: 10.9308\n",
      "Epoch 633/1000\n",
      "11/11 - 0s - loss: 13.8579 - mse: 10.9315\n",
      "Epoch 634/1000\n",
      "11/11 - 0s - loss: 13.8572 - mse: 10.9321\n",
      "Epoch 635/1000\n",
      "11/11 - 0s - loss: 13.8566 - mse: 10.9328\n",
      "Epoch 636/1000\n",
      "11/11 - 0s - loss: 13.8559 - mse: 10.9334\n",
      "Epoch 637/1000\n",
      "11/11 - 0s - loss: 13.8552 - mse: 10.9341\n",
      "Epoch 638/1000\n",
      "11/11 - 0s - loss: 13.8546 - mse: 10.9348\n",
      "Epoch 639/1000\n",
      "11/11 - 0s - loss: 13.8539 - mse: 10.9354\n",
      "Epoch 640/1000\n",
      "11/11 - 0s - loss: 13.8532 - mse: 10.9361\n",
      "Epoch 641/1000\n",
      "11/11 - 0s - loss: 13.8526 - mse: 10.9367\n",
      "Epoch 642/1000\n",
      "11/11 - 0s - loss: 13.8520 - mse: 10.9374\n",
      "Epoch 643/1000\n",
      "11/11 - 0s - loss: 13.8513 - mse: 10.9381\n",
      "Epoch 644/1000\n",
      "11/11 - 0s - loss: 13.8507 - mse: 10.9387\n",
      "Epoch 645/1000\n",
      "11/11 - 0s - loss: 13.8501 - mse: 10.9394\n",
      "Epoch 646/1000\n",
      "11/11 - 0s - loss: 13.8495 - mse: 10.9400\n",
      "Epoch 647/1000\n",
      "11/11 - 0s - loss: 13.8489 - mse: 10.9407\n",
      "Epoch 648/1000\n",
      "11/11 - 0s - loss: 13.8483 - mse: 10.9413\n",
      "Epoch 649/1000\n",
      "11/11 - 0s - loss: 13.8477 - mse: 10.9420\n",
      "Epoch 650/1000\n",
      "11/11 - 0s - loss: 13.8471 - mse: 10.9426\n",
      "Epoch 651/1000\n",
      "11/11 - 0s - loss: 13.8465 - mse: 10.9433\n",
      "Epoch 652/1000\n",
      "11/11 - 0s - loss: 13.8459 - mse: 10.9439\n",
      "Epoch 653/1000\n",
      "11/11 - 0s - loss: 13.8453 - mse: 10.9446\n",
      "Epoch 654/1000\n",
      "11/11 - 0s - loss: 13.8448 - mse: 10.9452\n",
      "Epoch 655/1000\n",
      "11/11 - 0s - loss: 13.8442 - mse: 10.9459\n",
      "Epoch 656/1000\n",
      "11/11 - 0s - loss: 13.8437 - mse: 10.9465\n",
      "Epoch 657/1000\n",
      "11/11 - 0s - loss: 13.8431 - mse: 10.9472\n",
      "Epoch 658/1000\n",
      "11/11 - 0s - loss: 13.8426 - mse: 10.9478\n",
      "Epoch 659/1000\n",
      "11/11 - 0s - loss: 13.8420 - mse: 10.9485\n",
      "Epoch 660/1000\n",
      "11/11 - 0s - loss: 13.8415 - mse: 10.9491\n",
      "Epoch 661/1000\n",
      "11/11 - 0s - loss: 13.8410 - mse: 10.9497\n",
      "Epoch 662/1000\n",
      "11/11 - 0s - loss: 13.8404 - mse: 10.9504\n",
      "Epoch 663/1000\n",
      "11/11 - 0s - loss: 13.8399 - mse: 10.9510\n",
      "Epoch 664/1000\n",
      "11/11 - 0s - loss: 13.8394 - mse: 10.9516\n",
      "Epoch 665/1000\n",
      "11/11 - 0s - loss: 13.8389 - mse: 10.9523\n",
      "Epoch 666/1000\n",
      "11/11 - 0s - loss: 13.8384 - mse: 10.9529\n",
      "Epoch 667/1000\n",
      "11/11 - 0s - loss: 13.8379 - mse: 10.9535\n",
      "Epoch 668/1000\n",
      "11/11 - 0s - loss: 13.8374 - mse: 10.9542\n",
      "Epoch 669/1000\n",
      "11/11 - 0s - loss: 13.8369 - mse: 10.9548\n",
      "Epoch 670/1000\n",
      "11/11 - 0s - loss: 13.8364 - mse: 10.9554\n",
      "Epoch 671/1000\n",
      "11/11 - 0s - loss: 13.8359 - mse: 10.9561\n",
      "Epoch 672/1000\n",
      "11/11 - 0s - loss: 13.8355 - mse: 10.9567\n",
      "Epoch 673/1000\n",
      "11/11 - 0s - loss: 13.8350 - mse: 10.9573\n",
      "Epoch 674/1000\n",
      "11/11 - 0s - loss: 13.8345 - mse: 10.9579\n",
      "Epoch 675/1000\n",
      "11/11 - 0s - loss: 13.8341 - mse: 10.9585\n",
      "Epoch 676/1000\n",
      "11/11 - 0s - loss: 13.8336 - mse: 10.9592\n",
      "Epoch 677/1000\n",
      "11/11 - 0s - loss: 13.8332 - mse: 10.9598\n",
      "Epoch 678/1000\n",
      "11/11 - 0s - loss: 13.8327 - mse: 10.9604\n",
      "Epoch 679/1000\n",
      "11/11 - 0s - loss: 13.8323 - mse: 10.9610\n",
      "Epoch 680/1000\n",
      "11/11 - 0s - loss: 13.8319 - mse: 10.9616\n",
      "Epoch 681/1000\n",
      "11/11 - 0s - loss: 13.8314 - mse: 10.9623\n",
      "Epoch 682/1000\n",
      "11/11 - 0s - loss: 13.8310 - mse: 10.9629\n",
      "Epoch 683/1000\n",
      "11/11 - 0s - loss: 13.8306 - mse: 10.9635\n",
      "Epoch 684/1000\n",
      "11/11 - 0s - loss: 13.8302 - mse: 10.9641\n",
      "Epoch 685/1000\n",
      "11/11 - 0s - loss: 13.8297 - mse: 10.9647\n",
      "Epoch 686/1000\n",
      "11/11 - 0s - loss: 13.8293 - mse: 10.9653\n",
      "Epoch 687/1000\n",
      "11/11 - 0s - loss: 13.8289 - mse: 10.9659\n",
      "Epoch 688/1000\n",
      "11/11 - 0s - loss: 13.8285 - mse: 10.9665\n",
      "Epoch 689/1000\n",
      "11/11 - 0s - loss: 13.8281 - mse: 10.9671\n",
      "Epoch 690/1000\n",
      "11/11 - 0s - loss: 13.8277 - mse: 10.9677\n",
      "Epoch 691/1000\n",
      "11/11 - 0s - loss: 13.8273 - mse: 10.9683\n",
      "Epoch 692/1000\n",
      "11/11 - 0s - loss: 13.8270 - mse: 10.9689\n",
      "Epoch 693/1000\n",
      "11/11 - 0s - loss: 13.8266 - mse: 10.9695\n",
      "Epoch 694/1000\n",
      "11/11 - 0s - loss: 13.8262 - mse: 10.9701\n",
      "Epoch 695/1000\n",
      "11/11 - 0s - loss: 13.8258 - mse: 10.9707\n",
      "Epoch 696/1000\n",
      "11/11 - 0s - loss: 13.8255 - mse: 10.9713\n",
      "Epoch 697/1000\n",
      "11/11 - 0s - loss: 13.8251 - mse: 10.9719\n",
      "Epoch 698/1000\n",
      "11/11 - 0s - loss: 13.8247 - mse: 10.9725\n",
      "Epoch 699/1000\n",
      "11/11 - 0s - loss: 13.8244 - mse: 10.9730\n",
      "Epoch 700/1000\n",
      "11/11 - 0s - loss: 13.8240 - mse: 10.9736\n",
      "Epoch 701/1000\n",
      "11/11 - 0s - loss: 13.8237 - mse: 10.9742\n",
      "Epoch 702/1000\n",
      "11/11 - 0s - loss: 13.8233 - mse: 10.9748\n",
      "Epoch 703/1000\n",
      "11/11 - 0s - loss: 13.8230 - mse: 10.9754\n",
      "Epoch 704/1000\n",
      "11/11 - 0s - loss: 13.8226 - mse: 10.9759\n",
      "Epoch 705/1000\n",
      "11/11 - 0s - loss: 13.8223 - mse: 10.9765\n",
      "Epoch 706/1000\n",
      "11/11 - 0s - loss: 13.8220 - mse: 10.9771\n",
      "Epoch 707/1000\n",
      "11/11 - 0s - loss: 13.8217 - mse: 10.9777\n",
      "Epoch 708/1000\n",
      "11/11 - 0s - loss: 13.8213 - mse: 10.9782\n",
      "Epoch 709/1000\n",
      "11/11 - 0s - loss: 13.8210 - mse: 10.9788\n",
      "Epoch 710/1000\n",
      "11/11 - 0s - loss: 13.8207 - mse: 10.9794\n",
      "Epoch 711/1000\n",
      "11/11 - 0s - loss: 13.8204 - mse: 10.9799\n",
      "Epoch 712/1000\n",
      "11/11 - 0s - loss: 13.8201 - mse: 10.9805\n",
      "Epoch 713/1000\n",
      "11/11 - 0s - loss: 13.8198 - mse: 10.9811\n",
      "Epoch 714/1000\n",
      "11/11 - 0s - loss: 13.8195 - mse: 10.9816\n",
      "Epoch 715/1000\n",
      "11/11 - 0s - loss: 13.8192 - mse: 10.9822\n",
      "Epoch 716/1000\n",
      "11/11 - 0s - loss: 13.8189 - mse: 10.9828\n",
      "Epoch 717/1000\n",
      "11/11 - 0s - loss: 13.8186 - mse: 10.9833\n",
      "Epoch 718/1000\n",
      "11/11 - 0s - loss: 13.8183 - mse: 10.9839\n",
      "Epoch 719/1000\n",
      "11/11 - 0s - loss: 13.8180 - mse: 10.9844\n",
      "Epoch 720/1000\n",
      "11/11 - 0s - loss: 13.8177 - mse: 10.9850\n",
      "Epoch 721/1000\n",
      "11/11 - 0s - loss: 13.8174 - mse: 10.9855\n",
      "Epoch 722/1000\n",
      "11/11 - 0s - loss: 13.8171 - mse: 10.9861\n",
      "Epoch 723/1000\n",
      "11/11 - 0s - loss: 13.8169 - mse: 10.9866\n",
      "Epoch 724/1000\n",
      "11/11 - 0s - loss: 13.8166 - mse: 10.9872\n",
      "Epoch 725/1000\n",
      "11/11 - 0s - loss: 13.8163 - mse: 10.9877\n",
      "Epoch 726/1000\n",
      "11/11 - 0s - loss: 13.8161 - mse: 10.9882\n",
      "Epoch 727/1000\n",
      "11/11 - 0s - loss: 13.8158 - mse: 10.9888\n",
      "Epoch 728/1000\n",
      "11/11 - 0s - loss: 13.8155 - mse: 10.9893\n",
      "Epoch 729/1000\n",
      "11/11 - 0s - loss: 13.8153 - mse: 10.9899\n",
      "Epoch 730/1000\n",
      "11/11 - 0s - loss: 13.8150 - mse: 10.9904\n",
      "Epoch 731/1000\n",
      "11/11 - 0s - loss: 13.8148 - mse: 10.9909\n",
      "Epoch 732/1000\n",
      "11/11 - 0s - loss: 13.8145 - mse: 10.9915\n",
      "Epoch 733/1000\n",
      "11/11 - 0s - loss: 13.8143 - mse: 10.9920\n",
      "Epoch 734/1000\n",
      "11/11 - 0s - loss: 13.8140 - mse: 10.9925\n",
      "Epoch 735/1000\n",
      "11/11 - 0s - loss: 13.8138 - mse: 10.9930\n",
      "Epoch 736/1000\n",
      "11/11 - 0s - loss: 13.8136 - mse: 10.9936\n",
      "Epoch 737/1000\n",
      "11/11 - 0s - loss: 13.8133 - mse: 10.9941\n",
      "Epoch 738/1000\n",
      "11/11 - 0s - loss: 13.8131 - mse: 10.9946\n",
      "Epoch 739/1000\n",
      "11/11 - 0s - loss: 13.8129 - mse: 10.9951\n",
      "Epoch 740/1000\n",
      "11/11 - 0s - loss: 13.8127 - mse: 10.9956\n",
      "Epoch 741/1000\n",
      "11/11 - 0s - loss: 13.8124 - mse: 10.9961\n",
      "Epoch 742/1000\n",
      "11/11 - 0s - loss: 13.8122 - mse: 10.9967\n",
      "Epoch 743/1000\n",
      "11/11 - 0s - loss: 13.8120 - mse: 10.9972\n",
      "Epoch 744/1000\n",
      "11/11 - 0s - loss: 13.8118 - mse: 10.9977\n",
      "Epoch 745/1000\n",
      "11/11 - 0s - loss: 13.8116 - mse: 10.9982\n",
      "Epoch 746/1000\n",
      "11/11 - 0s - loss: 13.8113 - mse: 10.9987\n",
      "Epoch 747/1000\n",
      "11/11 - 0s - loss: 13.8111 - mse: 10.9992\n",
      "Epoch 748/1000\n",
      "11/11 - 0s - loss: 13.8109 - mse: 10.9997\n",
      "Epoch 749/1000\n",
      "11/11 - 0s - loss: 13.8107 - mse: 11.0002\n",
      "Epoch 750/1000\n",
      "11/11 - 0s - loss: 13.8105 - mse: 11.0007\n",
      "Epoch 751/1000\n",
      "11/11 - 0s - loss: 13.8103 - mse: 11.0012\n",
      "Epoch 752/1000\n",
      "11/11 - 0s - loss: 13.8101 - mse: 11.0017\n",
      "Epoch 753/1000\n",
      "11/11 - 0s - loss: 13.8099 - mse: 11.0022\n",
      "Epoch 754/1000\n",
      "11/11 - 0s - loss: 13.8097 - mse: 11.0027\n",
      "Epoch 755/1000\n",
      "11/11 - 0s - loss: 13.8096 - mse: 11.0031\n",
      "Epoch 756/1000\n",
      "11/11 - 0s - loss: 13.8094 - mse: 11.0036\n",
      "Epoch 757/1000\n",
      "11/11 - 0s - loss: 13.8092 - mse: 11.0041\n",
      "Epoch 758/1000\n",
      "11/11 - 0s - loss: 13.8090 - mse: 11.0046\n",
      "Epoch 759/1000\n",
      "11/11 - 0s - loss: 13.8088 - mse: 11.0051\n",
      "Epoch 760/1000\n",
      "11/11 - 0s - loss: 13.8086 - mse: 11.0056\n",
      "Epoch 761/1000\n",
      "11/11 - 0s - loss: 13.8085 - mse: 11.0060\n",
      "Epoch 762/1000\n",
      "11/11 - 0s - loss: 13.8083 - mse: 11.0065\n",
      "Epoch 763/1000\n",
      "11/11 - 0s - loss: 13.8081 - mse: 11.0070\n",
      "Epoch 764/1000\n",
      "11/11 - 0s - loss: 13.8079 - mse: 11.0075\n",
      "Epoch 765/1000\n",
      "11/11 - 0s - loss: 13.8078 - mse: 11.0079\n",
      "Epoch 766/1000\n",
      "11/11 - 0s - loss: 13.8076 - mse: 11.0084\n",
      "Epoch 767/1000\n",
      "11/11 - 0s - loss: 13.8074 - mse: 11.0089\n",
      "Epoch 768/1000\n",
      "11/11 - 0s - loss: 13.8073 - mse: 11.0093\n",
      "Epoch 769/1000\n",
      "11/11 - 0s - loss: 13.8071 - mse: 11.0098\n",
      "Epoch 770/1000\n",
      "11/11 - 0s - loss: 13.8070 - mse: 11.0103\n",
      "Epoch 771/1000\n",
      "11/11 - 0s - loss: 13.8068 - mse: 11.0107\n",
      "Epoch 772/1000\n",
      "11/11 - 0s - loss: 13.8067 - mse: 11.0112\n",
      "Epoch 773/1000\n",
      "11/11 - 0s - loss: 13.8065 - mse: 11.0116\n",
      "Epoch 774/1000\n",
      "11/11 - 0s - loss: 13.8063 - mse: 11.0121\n",
      "Epoch 775/1000\n",
      "11/11 - 0s - loss: 13.8062 - mse: 11.0125\n",
      "Epoch 776/1000\n",
      "11/11 - 0s - loss: 13.8061 - mse: 11.0130\n",
      "Epoch 777/1000\n",
      "11/11 - 0s - loss: 13.8059 - mse: 11.0134\n",
      "Epoch 778/1000\n",
      "11/11 - 0s - loss: 13.8058 - mse: 11.0139\n",
      "Epoch 779/1000\n",
      "11/11 - 0s - loss: 13.8056 - mse: 11.0143\n",
      "Epoch 780/1000\n",
      "11/11 - 0s - loss: 13.8055 - mse: 11.0148\n",
      "Epoch 781/1000\n",
      "11/11 - 0s - loss: 13.8053 - mse: 11.0152\n",
      "Epoch 782/1000\n",
      "11/11 - 0s - loss: 13.8052 - mse: 11.0156\n",
      "Epoch 783/1000\n",
      "11/11 - 0s - loss: 13.8051 - mse: 11.0161\n",
      "Epoch 784/1000\n",
      "11/11 - 0s - loss: 13.8049 - mse: 11.0165\n",
      "Epoch 785/1000\n",
      "11/11 - 0s - loss: 13.8048 - mse: 11.0169\n",
      "Epoch 786/1000\n",
      "11/11 - 0s - loss: 13.8047 - mse: 11.0174\n",
      "Epoch 787/1000\n",
      "11/11 - 0s - loss: 13.8045 - mse: 11.0178\n",
      "Epoch 788/1000\n",
      "11/11 - 0s - loss: 13.8044 - mse: 11.0182\n",
      "Epoch 789/1000\n",
      "11/11 - 0s - loss: 13.8043 - mse: 11.0187\n",
      "Epoch 790/1000\n",
      "11/11 - 0s - loss: 13.8042 - mse: 11.0191\n",
      "Epoch 791/1000\n",
      "11/11 - 0s - loss: 13.8040 - mse: 11.0195\n",
      "Epoch 792/1000\n",
      "11/11 - 0s - loss: 13.8039 - mse: 11.0199\n",
      "Epoch 793/1000\n",
      "11/11 - 0s - loss: 13.8038 - mse: 11.0204\n",
      "Epoch 794/1000\n",
      "11/11 - 0s - loss: 13.8037 - mse: 11.0208\n",
      "Epoch 795/1000\n",
      "11/11 - 0s - loss: 13.8036 - mse: 11.0212\n",
      "Epoch 796/1000\n",
      "11/11 - 0s - loss: 13.8035 - mse: 11.0216\n",
      "Epoch 797/1000\n",
      "11/11 - 0s - loss: 13.8033 - mse: 11.0220\n",
      "Epoch 798/1000\n",
      "11/11 - 0s - loss: 13.8032 - mse: 11.0224\n",
      "Epoch 799/1000\n",
      "11/11 - 0s - loss: 13.8031 - mse: 11.0228\n",
      "Epoch 800/1000\n",
      "11/11 - 0s - loss: 13.8030 - mse: 11.0232\n",
      "Epoch 801/1000\n",
      "11/11 - 0s - loss: 13.8029 - mse: 11.0236\n",
      "Epoch 802/1000\n",
      "11/11 - 0s - loss: 13.8028 - mse: 11.0240\n",
      "Epoch 803/1000\n",
      "11/11 - 0s - loss: 13.8027 - mse: 11.0244\n",
      "Epoch 804/1000\n",
      "11/11 - 0s - loss: 13.8026 - mse: 11.0248\n",
      "Epoch 805/1000\n",
      "11/11 - 0s - loss: 13.8025 - mse: 11.0252\n",
      "Epoch 806/1000\n",
      "11/11 - 0s - loss: 13.8024 - mse: 11.0256\n",
      "Epoch 807/1000\n",
      "11/11 - 0s - loss: 13.8023 - mse: 11.0260\n",
      "Epoch 808/1000\n",
      "11/11 - 0s - loss: 13.8022 - mse: 11.0264\n",
      "Epoch 809/1000\n",
      "11/11 - 0s - loss: 13.8021 - mse: 11.0268\n",
      "Epoch 810/1000\n",
      "11/11 - 0s - loss: 13.8020 - mse: 11.0272\n",
      "Epoch 811/1000\n",
      "11/11 - 0s - loss: 13.8019 - mse: 11.0276\n",
      "Epoch 812/1000\n",
      "11/11 - 0s - loss: 13.8018 - mse: 11.0280\n",
      "Epoch 813/1000\n",
      "11/11 - 0s - loss: 13.8017 - mse: 11.0284\n",
      "Epoch 814/1000\n",
      "11/11 - 0s - loss: 13.8016 - mse: 11.0287\n",
      "Epoch 815/1000\n",
      "11/11 - 0s - loss: 13.8015 - mse: 11.0291\n",
      "Epoch 816/1000\n",
      "11/11 - 0s - loss: 13.8014 - mse: 11.0295\n",
      "Epoch 817/1000\n",
      "11/11 - 0s - loss: 13.8014 - mse: 11.0299\n",
      "Epoch 818/1000\n",
      "11/11 - 0s - loss: 13.8013 - mse: 11.0302\n",
      "Epoch 819/1000\n",
      "11/11 - 0s - loss: 13.8012 - mse: 11.0306\n",
      "Epoch 820/1000\n",
      "11/11 - 0s - loss: 13.8011 - mse: 11.0310\n",
      "Epoch 821/1000\n",
      "11/11 - 0s - loss: 13.8010 - mse: 11.0314\n",
      "Epoch 822/1000\n",
      "11/11 - 0s - loss: 13.8009 - mse: 11.0317\n",
      "Epoch 823/1000\n",
      "11/11 - 0s - loss: 13.8009 - mse: 11.0321\n",
      "Epoch 824/1000\n",
      "11/11 - 0s - loss: 13.8008 - mse: 11.0325\n",
      "Epoch 825/1000\n",
      "11/11 - 0s - loss: 13.8007 - mse: 11.0328\n",
      "Epoch 826/1000\n",
      "11/11 - 0s - loss: 13.8006 - mse: 11.0332\n",
      "Epoch 827/1000\n",
      "11/11 - 0s - loss: 13.8005 - mse: 11.0335\n",
      "Epoch 828/1000\n",
      "11/11 - 0s - loss: 13.8005 - mse: 11.0339\n",
      "Epoch 829/1000\n",
      "11/11 - 0s - loss: 13.8004 - mse: 11.0343\n",
      "Epoch 830/1000\n",
      "11/11 - 0s - loss: 13.8003 - mse: 11.0346\n",
      "Epoch 831/1000\n",
      "11/11 - 0s - loss: 13.8002 - mse: 11.0350\n",
      "Epoch 832/1000\n",
      "11/11 - 0s - loss: 13.8002 - mse: 11.0353\n",
      "Epoch 833/1000\n",
      "11/11 - 0s - loss: 13.8001 - mse: 11.0357\n",
      "Epoch 834/1000\n",
      "11/11 - 0s - loss: 13.8000 - mse: 11.0360\n",
      "Epoch 835/1000\n",
      "11/11 - 0s - loss: 13.8000 - mse: 11.0364\n",
      "Epoch 836/1000\n",
      "11/11 - 0s - loss: 13.7999 - mse: 11.0367\n",
      "Epoch 837/1000\n",
      "11/11 - 0s - loss: 13.7998 - mse: 11.0370\n",
      "Epoch 838/1000\n",
      "11/11 - 0s - loss: 13.7998 - mse: 11.0374\n",
      "Epoch 839/1000\n",
      "11/11 - 0s - loss: 13.7997 - mse: 11.0377\n",
      "Epoch 840/1000\n",
      "11/11 - 0s - loss: 13.7996 - mse: 11.0381\n",
      "Epoch 841/1000\n",
      "11/11 - 0s - loss: 13.7996 - mse: 11.0384\n",
      "Epoch 842/1000\n",
      "11/11 - 0s - loss: 13.7995 - mse: 11.0387\n",
      "Epoch 843/1000\n",
      "11/11 - 0s - loss: 13.7994 - mse: 11.0391\n",
      "Epoch 844/1000\n",
      "11/11 - 0s - loss: 13.7994 - mse: 11.0394\n",
      "Epoch 845/1000\n",
      "11/11 - 0s - loss: 13.7993 - mse: 11.0397\n",
      "Epoch 846/1000\n",
      "11/11 - 0s - loss: 13.7993 - mse: 11.0401\n",
      "Epoch 847/1000\n",
      "11/11 - 0s - loss: 13.7992 - mse: 11.0404\n",
      "Epoch 848/1000\n",
      "11/11 - 0s - loss: 13.7992 - mse: 11.0407\n",
      "Epoch 849/1000\n",
      "11/11 - 0s - loss: 13.7991 - mse: 11.0410\n",
      "Epoch 850/1000\n",
      "11/11 - 0s - loss: 13.7990 - mse: 11.0414\n",
      "Epoch 851/1000\n",
      "11/11 - 0s - loss: 13.7990 - mse: 11.0417\n",
      "Epoch 852/1000\n",
      "11/11 - 0s - loss: 13.7989 - mse: 11.0420\n",
      "Epoch 853/1000\n",
      "11/11 - 0s - loss: 13.7989 - mse: 11.0423\n",
      "Epoch 854/1000\n",
      "11/11 - 0s - loss: 13.7988 - mse: 11.0426\n",
      "Epoch 855/1000\n",
      "11/11 - 0s - loss: 13.7988 - mse: 11.0430\n",
      "Epoch 856/1000\n",
      "11/11 - 0s - loss: 13.7987 - mse: 11.0433\n",
      "Epoch 857/1000\n",
      "11/11 - 0s - loss: 13.7987 - mse: 11.0436\n",
      "Epoch 858/1000\n",
      "11/11 - 0s - loss: 13.7986 - mse: 11.0439\n",
      "Epoch 859/1000\n",
      "11/11 - 0s - loss: 13.7986 - mse: 11.0442\n",
      "Epoch 860/1000\n",
      "11/11 - 0s - loss: 13.7985 - mse: 11.0445\n",
      "Epoch 861/1000\n",
      "11/11 - 0s - loss: 13.7985 - mse: 11.0448\n",
      "Epoch 862/1000\n",
      "11/11 - 0s - loss: 13.7984 - mse: 11.0451\n",
      "Epoch 863/1000\n",
      "11/11 - 0s - loss: 13.7984 - mse: 11.0454\n",
      "Epoch 864/1000\n",
      "11/11 - 0s - loss: 13.7983 - mse: 11.0457\n",
      "Epoch 865/1000\n",
      "11/11 - 0s - loss: 13.7983 - mse: 11.0460\n",
      "Epoch 866/1000\n",
      "11/11 - 0s - loss: 13.7982 - mse: 11.0463\n",
      "Epoch 867/1000\n",
      "11/11 - 0s - loss: 13.7982 - mse: 11.0466\n",
      "Epoch 868/1000\n",
      "11/11 - 0s - loss: 13.7982 - mse: 11.0469\n",
      "Epoch 869/1000\n",
      "11/11 - 0s - loss: 13.7981 - mse: 11.0472\n",
      "Epoch 870/1000\n",
      "11/11 - 0s - loss: 13.7981 - mse: 11.0475\n",
      "Epoch 871/1000\n",
      "11/11 - 0s - loss: 13.7980 - mse: 11.0478\n",
      "Epoch 872/1000\n",
      "11/11 - 0s - loss: 13.7980 - mse: 11.0481\n",
      "Epoch 873/1000\n",
      "11/11 - 0s - loss: 13.7980 - mse: 11.0484\n",
      "Epoch 874/1000\n",
      "11/11 - 0s - loss: 13.7979 - mse: 11.0486\n",
      "Epoch 875/1000\n",
      "11/11 - 0s - loss: 13.7979 - mse: 11.0489\n",
      "Epoch 876/1000\n",
      "11/11 - 0s - loss: 13.7978 - mse: 11.0492\n",
      "Epoch 877/1000\n",
      "11/11 - 0s - loss: 13.7978 - mse: 11.0495\n",
      "Epoch 878/1000\n",
      "11/11 - 0s - loss: 13.7978 - mse: 11.0498\n",
      "Epoch 879/1000\n",
      "11/11 - 0s - loss: 13.7977 - mse: 11.0500\n",
      "Epoch 880/1000\n",
      "11/11 - 0s - loss: 13.7977 - mse: 11.0503\n",
      "Epoch 881/1000\n",
      "11/11 - 0s - loss: 13.7977 - mse: 11.0506\n",
      "Epoch 882/1000\n",
      "11/11 - 0s - loss: 13.7976 - mse: 11.0509\n",
      "Epoch 883/1000\n",
      "11/11 - 0s - loss: 13.7976 - mse: 11.0512\n",
      "Epoch 884/1000\n",
      "11/11 - 0s - loss: 13.7975 - mse: 11.0514\n",
      "Epoch 885/1000\n",
      "11/11 - 0s - loss: 13.7975 - mse: 11.0517\n",
      "Epoch 886/1000\n",
      "11/11 - 0s - loss: 13.7975 - mse: 11.0520\n",
      "Epoch 887/1000\n",
      "11/11 - 0s - loss: 13.7975 - mse: 11.0522\n",
      "Epoch 888/1000\n",
      "11/11 - 0s - loss: 13.7974 - mse: 11.0525\n",
      "Epoch 889/1000\n",
      "11/11 - 0s - loss: 13.7974 - mse: 11.0528\n",
      "Epoch 890/1000\n",
      "11/11 - 0s - loss: 13.7974 - mse: 11.0530\n",
      "Epoch 891/1000\n",
      "11/11 - 0s - loss: 13.7973 - mse: 11.0533\n",
      "Epoch 892/1000\n",
      "11/11 - 0s - loss: 13.7973 - mse: 11.0535\n",
      "Epoch 893/1000\n",
      "11/11 - 0s - loss: 13.7973 - mse: 11.0538\n",
      "Epoch 894/1000\n",
      "11/11 - 0s - loss: 13.7972 - mse: 11.0541\n",
      "Epoch 895/1000\n",
      "11/11 - 0s - loss: 13.7972 - mse: 11.0543\n",
      "Epoch 896/1000\n",
      "11/11 - 0s - loss: 13.7972 - mse: 11.0546\n",
      "Epoch 897/1000\n",
      "11/11 - 0s - loss: 13.7972 - mse: 11.0548\n",
      "Epoch 898/1000\n",
      "11/11 - 0s - loss: 13.7971 - mse: 11.0551\n",
      "Epoch 899/1000\n",
      "11/11 - 0s - loss: 13.7971 - mse: 11.0553\n",
      "Epoch 900/1000\n",
      "11/11 - 0s - loss: 13.7971 - mse: 11.0556\n",
      "Epoch 901/1000\n",
      "11/11 - 0s - loss: 13.7970 - mse: 11.0558\n",
      "Epoch 902/1000\n",
      "11/11 - 0s - loss: 13.7970 - mse: 11.0561\n",
      "Epoch 903/1000\n",
      "11/11 - 0s - loss: 13.7970 - mse: 11.0563\n",
      "Epoch 904/1000\n",
      "11/11 - 0s - loss: 13.7970 - mse: 11.0566\n",
      "Epoch 905/1000\n",
      "11/11 - 0s - loss: 13.7969 - mse: 11.0568\n",
      "Epoch 906/1000\n",
      "11/11 - 0s - loss: 13.7969 - mse: 11.0571\n",
      "Epoch 907/1000\n",
      "11/11 - 0s - loss: 13.7969 - mse: 11.0573\n",
      "Epoch 908/1000\n",
      "11/11 - 0s - loss: 13.7969 - mse: 11.0575\n",
      "Epoch 909/1000\n",
      "11/11 - 0s - loss: 13.7969 - mse: 11.0578\n",
      "Epoch 910/1000\n",
      "11/11 - 0s - loss: 13.7968 - mse: 11.0580\n",
      "Epoch 911/1000\n",
      "11/11 - 0s - loss: 13.7968 - mse: 11.0583\n",
      "Epoch 912/1000\n",
      "11/11 - 0s - loss: 13.7968 - mse: 11.0585\n",
      "Epoch 913/1000\n",
      "11/11 - 0s - loss: 13.7968 - mse: 11.0587\n",
      "Epoch 914/1000\n",
      "11/11 - 0s - loss: 13.7967 - mse: 11.0590\n",
      "Epoch 915/1000\n",
      "11/11 - 0s - loss: 13.7967 - mse: 11.0592\n",
      "Epoch 916/1000\n",
      "11/11 - 0s - loss: 13.7967 - mse: 11.0594\n",
      "Epoch 917/1000\n",
      "11/11 - 0s - loss: 13.7967 - mse: 11.0596\n",
      "Epoch 918/1000\n",
      "11/11 - 0s - loss: 13.7967 - mse: 11.0599\n",
      "Epoch 919/1000\n",
      "11/11 - 0s - loss: 13.7966 - mse: 11.0601\n",
      "Epoch 920/1000\n",
      "11/11 - 0s - loss: 13.7966 - mse: 11.0603\n",
      "Epoch 921/1000\n",
      "11/11 - 0s - loss: 13.7966 - mse: 11.0605\n",
      "Epoch 922/1000\n",
      "11/11 - 0s - loss: 13.7966 - mse: 11.0608\n",
      "Epoch 923/1000\n",
      "11/11 - 0s - loss: 13.7966 - mse: 11.0610\n",
      "Epoch 924/1000\n",
      "11/11 - 0s - loss: 13.7965 - mse: 11.0612\n",
      "Epoch 925/1000\n",
      "11/11 - 0s - loss: 13.7965 - mse: 11.0614\n",
      "Epoch 926/1000\n",
      "11/11 - 0s - loss: 13.7965 - mse: 11.0617\n",
      "Epoch 927/1000\n",
      "11/11 - 0s - loss: 13.7965 - mse: 11.0619\n",
      "Epoch 928/1000\n",
      "11/11 - 0s - loss: 13.7965 - mse: 11.0621\n",
      "Epoch 929/1000\n",
      "11/11 - 0s - loss: 13.7965 - mse: 11.0623\n",
      "Epoch 930/1000\n",
      "11/11 - 0s - loss: 13.7964 - mse: 11.0625\n",
      "Epoch 931/1000\n",
      "11/11 - 0s - loss: 13.7964 - mse: 11.0627\n",
      "Epoch 932/1000\n",
      "11/11 - 0s - loss: 13.7964 - mse: 11.0629\n",
      "Epoch 933/1000\n",
      "11/11 - 0s - loss: 13.7964 - mse: 11.0631\n",
      "Epoch 934/1000\n",
      "11/11 - 0s - loss: 13.7964 - mse: 11.0634\n",
      "Epoch 935/1000\n",
      "11/11 - 0s - loss: 13.7964 - mse: 11.0636\n",
      "Epoch 936/1000\n",
      "11/11 - 0s - loss: 13.7964 - mse: 11.0638\n",
      "Epoch 937/1000\n",
      "11/11 - 0s - loss: 13.7963 - mse: 11.0640\n",
      "Epoch 938/1000\n",
      "11/11 - 0s - loss: 13.7963 - mse: 11.0642\n",
      "Epoch 939/1000\n",
      "11/11 - 0s - loss: 13.7963 - mse: 11.0644\n",
      "Epoch 940/1000\n",
      "11/11 - 0s - loss: 13.7963 - mse: 11.0646\n",
      "Epoch 941/1000\n",
      "11/11 - 0s - loss: 13.7963 - mse: 11.0648\n",
      "Epoch 942/1000\n",
      "11/11 - 0s - loss: 13.7963 - mse: 11.0650\n",
      "Epoch 943/1000\n",
      "11/11 - 0s - loss: 13.7963 - mse: 11.0652\n",
      "Epoch 944/1000\n",
      "11/11 - 0s - loss: 13.7962 - mse: 11.0654\n",
      "Epoch 945/1000\n",
      "11/11 - 0s - loss: 13.7962 - mse: 11.0656\n",
      "Epoch 946/1000\n",
      "11/11 - 0s - loss: 13.7962 - mse: 11.0658\n",
      "Epoch 947/1000\n",
      "11/11 - 0s - loss: 13.7962 - mse: 11.0660\n",
      "Epoch 948/1000\n",
      "11/11 - 0s - loss: 13.7962 - mse: 11.0662\n",
      "Epoch 949/1000\n",
      "11/11 - 0s - loss: 13.7962 - mse: 11.0664\n",
      "Epoch 950/1000\n",
      "11/11 - 0s - loss: 13.7962 - mse: 11.0666\n",
      "Epoch 951/1000\n",
      "11/11 - 0s - loss: 13.7962 - mse: 11.0667\n",
      "Epoch 952/1000\n",
      "11/11 - 0s - loss: 13.7962 - mse: 11.0669\n",
      "Epoch 953/1000\n",
      "11/11 - 0s - loss: 13.7961 - mse: 11.0671\n",
      "Epoch 954/1000\n",
      "11/11 - 0s - loss: 13.7961 - mse: 11.0673\n",
      "Epoch 955/1000\n",
      "11/11 - 0s - loss: 13.7961 - mse: 11.0675\n",
      "Epoch 956/1000\n",
      "11/11 - 0s - loss: 13.7961 - mse: 11.0677\n",
      "Epoch 957/1000\n",
      "11/11 - 0s - loss: 13.7961 - mse: 11.0679\n",
      "Epoch 958/1000\n",
      "11/11 - 0s - loss: 13.7961 - mse: 11.0680\n",
      "Epoch 959/1000\n",
      "11/11 - 0s - loss: 13.7961 - mse: 11.0682\n",
      "Epoch 960/1000\n",
      "11/11 - 0s - loss: 13.7961 - mse: 11.0684\n",
      "Epoch 961/1000\n",
      "11/11 - 0s - loss: 13.7961 - mse: 11.0686\n",
      "Epoch 962/1000\n",
      "11/11 - 0s - loss: 13.7961 - mse: 11.0688\n",
      "Epoch 963/1000\n",
      "11/11 - 0s - loss: 13.7961 - mse: 11.0689\n",
      "Epoch 964/1000\n",
      "11/11 - 0s - loss: 13.7960 - mse: 11.0691\n",
      "Epoch 965/1000\n",
      "11/11 - 0s - loss: 13.7960 - mse: 11.0693\n",
      "Epoch 966/1000\n",
      "11/11 - 0s - loss: 13.7960 - mse: 11.0695\n",
      "Epoch 967/1000\n",
      "11/11 - 0s - loss: 13.7960 - mse: 11.0697\n",
      "Epoch 968/1000\n",
      "11/11 - 0s - loss: 13.7960 - mse: 11.0698\n",
      "Epoch 969/1000\n",
      "11/11 - 0s - loss: 13.7960 - mse: 11.0700\n",
      "Epoch 970/1000\n",
      "11/11 - 0s - loss: 13.7960 - mse: 11.0702\n",
      "Epoch 971/1000\n",
      "11/11 - 0s - loss: 13.7960 - mse: 11.0703\n",
      "Epoch 972/1000\n",
      "11/11 - 0s - loss: 13.7960 - mse: 11.0705\n",
      "Epoch 973/1000\n",
      "11/11 - 0s - loss: 13.7960 - mse: 11.0707\n",
      "Epoch 974/1000\n",
      "11/11 - 0s - loss: 13.7960 - mse: 11.0708\n",
      "Epoch 975/1000\n",
      "11/11 - 0s - loss: 13.7960 - mse: 11.0710\n",
      "Epoch 976/1000\n",
      "11/11 - 0s - loss: 13.7960 - mse: 11.0712\n",
      "Epoch 977/1000\n",
      "11/11 - 0s - loss: 13.7960 - mse: 11.0713\n",
      "Epoch 978/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0715\n",
      "Epoch 979/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0717\n",
      "Epoch 980/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0718\n",
      "Epoch 981/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0720\n",
      "Epoch 982/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0722\n",
      "Epoch 983/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0723\n",
      "Epoch 984/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0725\n",
      "Epoch 985/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0726\n",
      "Epoch 986/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0728\n",
      "Epoch 987/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0729\n",
      "Epoch 988/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0731\n",
      "Epoch 989/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0733\n",
      "Epoch 990/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0734\n",
      "Epoch 991/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0736\n",
      "Epoch 992/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0737\n",
      "Epoch 993/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0739\n",
      "Epoch 994/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0740\n",
      "Epoch 995/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0742\n",
      "Epoch 996/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0743\n",
      "Epoch 997/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0745\n",
      "Epoch 998/1000\n",
      "11/11 - 0s - loss: 13.7959 - mse: 11.0746\n",
      "Epoch 999/1000\n",
      "11/11 - 0s - loss: 13.7958 - mse: 11.0748\n",
      "Epoch 1000/1000\n",
      "11/11 - 0s - loss: 13.7958 - mse: 11.0749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21585cafc70>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.astype('float64')\n",
    "y_train = y_train.astype('float64')\n",
    "x_test = x_test.astype('float64')\n",
    "y_test = y_test.astype('float64')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=x_train.shape[1], activation='linear', kernel_regularizer = make_reg_function(20.0)))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "model.fit(x_train,y_train, batch_size=int(x_train.shape[0]/10), epochs=1000, shuffle=False, verbose = 2)\n",
    "# make and fit a model with MUCH more regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.3635911 ],\n",
       "        [ 0.24955693],\n",
       "        [ 0.10539468],\n",
       "        [-0.5656785 ],\n",
       "        [ 0.53672856],\n",
       "        [-0.205129  ]], dtype=float32),\n",
       " array([-4.89898], dtype=float32)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the coefs\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 767us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.231797634773105"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate MSE\n",
    "y_pred = model.predict(x_test,verbose=1)\n",
    "mean_squared_error(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 94.2546 - mse: 75.2632\n",
      "Epoch 2/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 92.0250 - mse: 73.5810\n",
      "Epoch 3/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 89.8866 - mse: 71.9570\n",
      "Epoch 4/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 87.8409 - mse: 70.3999\n",
      "Epoch 5/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 85.8874 - mse: 68.9178\n",
      "Epoch 6/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 84.0225 - mse: 67.5145\n",
      "Epoch 7/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 82.2411 - mse: 66.1907\n",
      "Epoch 8/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 80.5377 - mse: 64.9447\n",
      "Epoch 9/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 78.9070 - mse: 63.7726\n",
      "Epoch 10/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 77.3442 - mse: 62.6694\n",
      "Epoch 11/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 75.8452 - mse: 61.6289\n",
      "Epoch 12/1000\n",
      "11/11 [==============================] - 0s 560us/step - loss: 74.4065 - mse: 60.6447\n",
      "Epoch 13/1000\n",
      "11/11 [==============================] - 0s 752us/step - loss: 73.0251 - mse: 59.7105\n",
      "Epoch 14/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 71.6985 - mse: 58.8206\n",
      "Epoch 15/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 70.4244 - mse: 57.9696\n",
      "Epoch 16/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 69.2007 - mse: 57.1531\n",
      "Epoch 17/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 68.0256 - mse: 56.3675\n",
      "Epoch 18/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 66.8972 - mse: 55.6096\n",
      "Epoch 19/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 65.8137 - mse: 54.8771\n",
      "Epoch 20/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 64.7734 - mse: 54.1678\n",
      "Epoch 21/1000\n",
      "11/11 [==============================] - 0s 997us/step - loss: 63.7745 - mse: 53.4803\n",
      "Epoch 22/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 62.8155 - mse: 52.8131\n",
      "Epoch 23/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 61.8946 - mse: 52.1652\n",
      "Epoch 24/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 61.0102 - mse: 51.5357\n",
      "Epoch 25/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 60.1609 - mse: 50.9239\n",
      "Epoch 26/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 59.3449 - mse: 50.3289\n",
      "Epoch 27/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 58.5609 - mse: 49.7504\n",
      "Epoch 28/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 57.8073 - mse: 49.1876\n",
      "Epoch 29/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 57.0828 - mse: 48.6402\n",
      "Epoch 30/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 56.3859 - mse: 48.1076\n",
      "Epoch 31/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 55.7154 - mse: 47.5894\n",
      "Epoch 32/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 55.0700 - mse: 47.0852\n",
      "Epoch 33/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 54.4485 - mse: 46.5946\n",
      "Epoch 34/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 53.8496 - mse: 46.1171\n",
      "Epoch 35/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 53.2723 - mse: 45.6524\n",
      "Epoch 36/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 52.7155 - mse: 45.2001\n",
      "Epoch 37/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 52.1781 - mse: 44.7597\n",
      "Epoch 38/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 51.6593 - mse: 44.3310\n",
      "Epoch 39/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 51.1580 - mse: 43.9135\n",
      "Epoch 40/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 50.6735 - mse: 43.5069\n",
      "Epoch 41/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 50.2048 - mse: 43.1108\n",
      "Epoch 42/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 49.7513 - mse: 42.7249\n",
      "Epoch 43/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 49.3121 - mse: 42.3488\n",
      "Epoch 44/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 48.8866 - mse: 41.9821\n",
      "Epoch 45/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 48.4741 - mse: 41.6247\n",
      "Epoch 46/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 48.0740 - mse: 41.2760\n",
      "Epoch 47/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 47.6857 - mse: 40.9359\n",
      "Epoch 48/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 47.3086 - mse: 40.6040\n",
      "Epoch 49/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 46.9423 - mse: 40.2800\n",
      "Epoch 50/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 46.5862 - mse: 39.9637\n",
      "Epoch 51/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 46.2399 - mse: 39.6547\n",
      "Epoch 52/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 45.9029 - mse: 39.3528\n",
      "Epoch 53/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 45.5748 - mse: 39.0578\n",
      "Epoch 54/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 45.2553 - mse: 38.7694\n",
      "Epoch 55/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 44.9439 - mse: 38.4873\n",
      "Epoch 56/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 44.6403 - mse: 38.2114\n",
      "Epoch 57/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 44.3442 - mse: 37.9414\n",
      "Epoch 58/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 44.0553 - mse: 37.6771\n",
      "Epoch 59/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 43.7731 - mse: 37.4182\n",
      "Epoch 60/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 43.4976 - mse: 37.1647\n",
      "Epoch 61/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 43.2283 - mse: 36.9164\n",
      "Epoch 62/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 42.9651 - mse: 36.6729\n",
      "Epoch 63/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 42.7077 - mse: 36.4343\n",
      "Epoch 64/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 42.4557 - mse: 36.2003\n",
      "Epoch 65/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 42.2091 - mse: 35.9707\n",
      "Epoch 66/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 41.9676 - mse: 35.7455\n",
      "Epoch 67/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 41.7310 - mse: 35.5244\n",
      "Epoch 68/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 41.4991 - mse: 35.3074\n",
      "Epoch 69/1000\n",
      "11/11 [==============================] - 0s 907us/step - loss: 41.2717 - mse: 35.0943\n",
      "Epoch 70/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 41.0486 - mse: 34.8850\n",
      "Epoch 71/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 40.8296 - mse: 34.6793\n",
      "Epoch 72/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 40.6146 - mse: 34.4773\n",
      "Epoch 73/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 40.4034 - mse: 34.2787\n",
      "Epoch 74/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 40.1959 - mse: 34.0834\n",
      "Epoch 75/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 39.9919 - mse: 33.8915\n",
      "Epoch 76/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 39.7912 - mse: 33.7027\n",
      "Epoch 77/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 39.5938 - mse: 33.5169\n",
      "Epoch 78/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 39.3996 - mse: 33.3342\n",
      "Epoch 79/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 39.2083 - mse: 33.1544\n",
      "Epoch 80/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 39.0198 - mse: 32.9774\n",
      "Epoch 81/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 38.8342 - mse: 32.8032\n",
      "Epoch 82/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 38.6511 - mse: 32.6317\n",
      "Epoch 83/1000\n",
      "11/11 [==============================] - 0s 726us/step - loss: 38.4707 - mse: 32.4628\n",
      "Epoch 84/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 38.2926 - mse: 32.2965\n",
      "Epoch 85/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 38.1170 - mse: 32.1326\n",
      "Epoch 86/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 37.9436 - mse: 31.9712\n",
      "Epoch 87/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 37.7723 - mse: 31.8122\n",
      "Epoch 88/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 37.6032 - mse: 31.6555\n",
      "Epoch 89/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 37.4361 - mse: 31.5011\n",
      "Epoch 90/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 37.2710 - mse: 31.3489\n",
      "Epoch 91/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 37.1077 - mse: 31.1988\n",
      "Epoch 92/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 36.9463 - mse: 31.0509\n",
      "Epoch 93/1000\n",
      "11/11 [==============================] - 0s 658us/step - loss: 36.7866 - mse: 30.9050\n",
      "Epoch 94/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 36.6287 - mse: 30.7612\n",
      "Epoch 95/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 36.4724 - mse: 30.6193\n",
      "Epoch 96/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 36.3176 - mse: 30.4794\n",
      "Epoch 97/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 36.1644 - mse: 30.3414\n",
      "Epoch 98/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 36.0127 - mse: 30.2053\n",
      "Epoch 99/1000\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 35.8625 - mse: 30.0709\n",
      "Epoch 100/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 35.7137 - mse: 29.9384\n",
      "Epoch 101/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 35.5662 - mse: 29.8076\n",
      "Epoch 102/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 35.4201 - mse: 29.6785\n",
      "Epoch 103/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 35.2753 - mse: 29.5511\n",
      "Epoch 104/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 35.1317 - mse: 29.4254\n",
      "Epoch 105/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 34.9894 - mse: 29.3012\n",
      "Epoch 106/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 34.8483 - mse: 29.1787\n",
      "Epoch 107/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 34.7083 - mse: 29.0577\n",
      "Epoch 108/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 34.5696 - mse: 28.9383\n",
      "Epoch 109/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 34.4319 - mse: 28.8203\n",
      "Epoch 110/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 34.2954 - mse: 28.7038\n",
      "Epoch 111/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 34.1600 - mse: 28.5888\n",
      "Epoch 112/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 34.0256 - mse: 28.4752\n",
      "Epoch 113/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 33.8923 - mse: 28.3630\n",
      "Epoch 114/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 33.7600 - mse: 28.2522\n",
      "Epoch 115/1000\n",
      "11/11 [==============================] - ETA: 0s - loss: 35.9822 - mse: 30.486 - 0s 635us/step - loss: 33.6288 - mse: 28.1427\n",
      "Epoch 116/1000\n",
      "11/11 [==============================] - 0s 997us/step - loss: 33.4985 - mse: 28.0345\n",
      "Epoch 117/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 33.3692 - mse: 27.9277\n",
      "Epoch 118/1000\n",
      "11/11 [==============================] - 0s 906us/step - loss: 33.2410 - mse: 27.8221\n",
      "Epoch 119/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 33.1137 - mse: 27.7178\n",
      "Epoch 120/1000\n",
      "11/11 [==============================] - 0s 907us/step - loss: 32.9873 - mse: 27.6147\n",
      "Epoch 121/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 32.8619 - mse: 27.5129\n",
      "Epoch 122/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 32.7375 - mse: 27.4123\n",
      "Epoch 123/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 32.6139 - mse: 27.3128\n",
      "Epoch 124/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 32.4913 - mse: 27.2145\n",
      "Epoch 125/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 32.3696 - mse: 27.1174\n",
      "Epoch 126/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 32.2488 - mse: 27.0214\n",
      "Epoch 127/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 32.1290 - mse: 26.9266\n",
      "Epoch 128/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 32.0100 - mse: 26.8328\n",
      "Epoch 129/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 31.8919 - mse: 26.7401\n",
      "Epoch 130/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 31.7747 - mse: 26.6485\n",
      "Epoch 131/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 31.6583 - mse: 26.5579\n",
      "Epoch 132/1000\n",
      "11/11 [==============================] - 0s 997us/step - loss: 31.5429 - mse: 26.4684\n",
      "Epoch 133/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 31.4283 - mse: 26.3799\n",
      "Epoch 134/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 31.3146 - mse: 26.2924\n",
      "Epoch 135/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 31.2017 - mse: 26.2059\n",
      "Epoch 136/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 31.0897 - mse: 26.1204\n",
      "Epoch 137/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 30.9786 - mse: 26.0358\n",
      "Epoch 138/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 30.8683 - mse: 25.9522\n",
      "Epoch 139/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 30.7589 - mse: 25.8696\n",
      "Epoch 140/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 30.6504 - mse: 25.7879\n",
      "Epoch 141/1000\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 30.5426 - mse: 25.7072\n",
      "Epoch 142/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 30.4358 - mse: 25.6273\n",
      "Epoch 143/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 30.3297 - mse: 25.5484\n",
      "Epoch 144/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 30.2245 - mse: 25.4703\n",
      "Epoch 145/1000\n",
      "11/11 [==============================] - 0s 719us/step - loss: 30.1202 - mse: 25.3932\n",
      "Epoch 146/1000\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 30.0167 - mse: 25.3169\n",
      "Epoch 147/1000\n",
      "11/11 [==============================] - 0s 907us/step - loss: 29.9140 - mse: 25.2414\n",
      "Epoch 148/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 29.8121 - mse: 25.1668\n",
      "Epoch 149/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 29.7111 - mse: 25.0931\n",
      "Epoch 150/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 29.6109 - mse: 25.0202\n",
      "Epoch 151/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 29.5116 - mse: 24.9481\n",
      "Epoch 152/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 29.4130 - mse: 24.8769\n",
      "Epoch 153/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 29.3153 - mse: 24.8064\n",
      "Epoch 154/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 29.2184 - mse: 24.7368\n",
      "Epoch 155/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 29.1224 - mse: 24.6679\n",
      "Epoch 156/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 29.0271 - mse: 24.5998\n",
      "Epoch 157/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 28.9327 - mse: 24.5325\n",
      "Epoch 158/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 28.8390 - mse: 24.4660\n",
      "Epoch 159/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 28.7462 - mse: 24.4002\n",
      "Epoch 160/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 28.6542 - mse: 24.3352\n",
      "Epoch 161/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 28.5630 - mse: 24.2709\n",
      "Epoch 162/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 28.4726 - mse: 24.2074\n",
      "Epoch 163/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 28.3830 - mse: 24.1446\n",
      "Epoch 164/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 28.2942 - mse: 24.0825\n",
      "Epoch 165/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 28.2062 - mse: 24.0212\n",
      "Epoch 166/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 28.1190 - mse: 23.9605\n",
      "Epoch 167/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 28.0326 - mse: 23.9006\n",
      "Epoch 168/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 27.9469 - mse: 23.8413\n",
      "Epoch 169/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 27.8621 - mse: 23.7828\n",
      "Epoch 170/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 27.7780 - mse: 23.7249\n",
      "Epoch 171/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 27.6948 - mse: 23.6677\n",
      "Epoch 172/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 27.6123 - mse: 23.6112\n",
      "Epoch 173/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 27.5305 - mse: 23.5554\n",
      "Epoch 174/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 27.4496 - mse: 23.5002\n",
      "Epoch 175/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 27.3694 - mse: 23.4457\n",
      "Epoch 176/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 27.2900 - mse: 23.3918\n",
      "Epoch 177/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 27.2113 - mse: 23.3386\n",
      "Epoch 178/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 27.1334 - mse: 23.2860\n",
      "Epoch 179/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 27.0563 - mse: 23.2340\n",
      "Epoch 180/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 26.9799 - mse: 23.1827\n",
      "Epoch 181/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 26.9042 - mse: 23.1320\n",
      "Epoch 182/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 26.8294 - mse: 23.0819\n",
      "Epoch 183/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 26.7552 - mse: 23.0324\n",
      "Epoch 184/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 26.6818 - mse: 22.9836\n",
      "Epoch 185/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 26.6091 - mse: 22.9353\n",
      "Epoch 186/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 26.5371 - mse: 22.8876\n",
      "Epoch 187/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 26.4659 - mse: 22.8405\n",
      "Epoch 188/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 26.3954 - mse: 22.7940\n",
      "Epoch 189/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 26.3256 - mse: 22.7481\n",
      "Epoch 190/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 26.2566 - mse: 22.7028\n",
      "Epoch 191/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 26.1882 - mse: 22.6580\n",
      "Epoch 192/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 26.1206 - mse: 22.6138\n",
      "Epoch 193/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 26.0536 - mse: 22.5701\n",
      "Epoch 194/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 25.9874 - mse: 22.5270\n",
      "Epoch 195/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 25.9218 - mse: 22.4845\n",
      "Epoch 196/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 25.8569 - mse: 22.4425\n",
      "Epoch 197/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 25.7927 - mse: 22.4010\n",
      "Epoch 198/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 25.7292 - mse: 22.3601\n",
      "Epoch 199/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 25.6664 - mse: 22.3197\n",
      "Epoch 200/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 25.6043 - mse: 22.2798\n",
      "Epoch 201/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 25.5428 - mse: 22.2404\n",
      "Epoch 202/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 25.4819 - mse: 22.2016\n",
      "Epoch 203/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 25.4218 - mse: 22.1632\n",
      "Epoch 204/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 25.3623 - mse: 22.1254\n",
      "Epoch 205/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 25.3034 - mse: 22.0881\n",
      "Epoch 206/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 25.2452 - mse: 22.0512\n",
      "Epoch 207/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 25.1876 - mse: 22.0149\n",
      "Epoch 208/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 25.1307 - mse: 21.9790\n",
      "Epoch 209/1000\n",
      "11/11 [==============================] - 0s 634us/step - loss: 25.0743 - mse: 21.9436\n",
      "Epoch 210/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 25.0187 - mse: 21.9087\n",
      "Epoch 211/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 24.9636 - mse: 21.8743\n",
      "Epoch 212/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 24.9091 - mse: 21.8403\n",
      "Epoch 213/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 24.8553 - mse: 21.8068\n",
      "Epoch 214/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 24.8020 - mse: 21.7737\n",
      "Epoch 215/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 24.7494 - mse: 21.7411\n",
      "Epoch 216/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 24.6973 - mse: 21.7089\n",
      "Epoch 217/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 24.6459 - mse: 21.6772\n",
      "Epoch 218/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 24.5950 - mse: 21.6459\n",
      "Epoch 219/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 24.5447 - mse: 21.6151\n",
      "Epoch 220/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 24.4950 - mse: 21.5846\n",
      "Epoch 221/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 24.4458 - mse: 21.5546\n",
      "Epoch 222/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 24.3972 - mse: 21.5250\n",
      "Epoch 223/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 24.3492 - mse: 21.4958\n",
      "Epoch 224/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 24.3017 - mse: 21.4671\n",
      "Epoch 225/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 24.2548 - mse: 21.4387\n",
      "Epoch 226/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 24.2084 - mse: 21.4107\n",
      "Epoch 227/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 24.1625 - mse: 21.3831\n",
      "Epoch 228/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 24.1172 - mse: 21.3559\n",
      "Epoch 229/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 24.0724 - mse: 21.3291\n",
      "Epoch 230/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 24.0281 - mse: 21.3026\n",
      "Epoch 231/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.9844 - mse: 21.2766\n",
      "Epoch 232/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 23.9411 - mse: 21.2509\n",
      "Epoch 233/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 23.8984 - mse: 21.2255\n",
      "Epoch 234/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.8561 - mse: 21.2005\n",
      "Epoch 235/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.8144 - mse: 21.1759\n",
      "Epoch 236/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.7731 - mse: 21.1516\n",
      "Epoch 237/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.7324 - mse: 21.1277\n",
      "Epoch 238/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 23.6921 - mse: 21.1041\n",
      "Epoch 239/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.6523 - mse: 21.0808\n",
      "Epoch 240/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.6129 - mse: 21.0579\n",
      "Epoch 241/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.5740 - mse: 21.0353\n",
      "Epoch 242/1000\n",
      "11/11 [==============================] - 0s 454us/step - loss: 23.5356 - mse: 21.0130\n",
      "Epoch 243/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.4976 - mse: 20.9911\n",
      "Epoch 244/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.4601 - mse: 20.9694\n",
      "Epoch 245/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 23.4230 - mse: 20.9480\n",
      "Epoch 246/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.3863 - mse: 20.9270\n",
      "Epoch 247/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.3501 - mse: 20.9062\n",
      "Epoch 248/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.3143 - mse: 20.8858\n",
      "Epoch 249/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.2789 - mse: 20.8656\n",
      "Epoch 250/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.2440 - mse: 20.8457\n",
      "Epoch 251/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.2094 - mse: 20.8261\n",
      "Epoch 252/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.1753 - mse: 20.8068\n",
      "Epoch 253/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.1415 - mse: 20.7877\n",
      "Epoch 254/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 23.1082 - mse: 20.7690\n",
      "Epoch 255/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.0752 - mse: 20.7504\n",
      "Epoch 256/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.0426 - mse: 20.7321\n",
      "Epoch 257/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 23.0104 - mse: 20.7141\n",
      "Epoch 258/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.9786 - mse: 20.6964\n",
      "Epoch 259/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.9472 - mse: 20.6788\n",
      "Epoch 260/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.9161 - mse: 20.6615\n",
      "Epoch 261/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.8853 - mse: 20.6445\n",
      "Epoch 262/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 22.8550 - mse: 20.6277\n",
      "Epoch 263/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.8249 - mse: 20.6111\n",
      "Epoch 264/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.7952 - mse: 20.5947\n",
      "Epoch 265/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.7659 - mse: 20.5786\n",
      "Epoch 266/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 22.7369 - mse: 20.5626\n",
      "Epoch 267/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 22.7082 - mse: 20.5469\n",
      "Epoch 268/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.6799 - mse: 20.5314\n",
      "Epoch 269/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 22.6518 - mse: 20.5161\n",
      "Epoch 270/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.6241 - mse: 20.5009\n",
      "Epoch 271/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.5967 - mse: 20.4860\n",
      "Epoch 272/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.5696 - mse: 20.4713\n",
      "Epoch 273/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.5428 - mse: 20.4568\n",
      "Epoch 274/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 22.5163 - mse: 20.4424\n",
      "Epoch 275/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.4901 - mse: 20.4282\n",
      "Epoch 276/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.4641 - mse: 20.4142\n",
      "Epoch 277/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.4385 - mse: 20.4004\n",
      "Epoch 278/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.4131 - mse: 20.3867\n",
      "Epoch 279/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 22.3880 - mse: 20.3732\n",
      "Epoch 280/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 22.3632 - mse: 20.3599\n",
      "Epoch 281/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.3386 - mse: 20.3467\n",
      "Epoch 282/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.3143 - mse: 20.3337\n",
      "Epoch 283/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.2903 - mse: 20.3208\n",
      "Epoch 284/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.2665 - mse: 20.3081\n",
      "Epoch 285/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.2429 - mse: 20.2955\n",
      "Epoch 286/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.2196 - mse: 20.2830\n",
      "Epoch 287/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.1966 - mse: 20.2707\n",
      "Epoch 288/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 22.1737 - mse: 20.2585\n",
      "Epoch 289/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.1511 - mse: 20.2465\n",
      "Epoch 290/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 22.1288 - mse: 20.2346\n",
      "Epoch 291/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.1066 - mse: 20.2228\n",
      "Epoch 292/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 22.0847 - mse: 20.2111\n",
      "Epoch 293/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.0630 - mse: 20.1995\n",
      "Epoch 294/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.0415 - mse: 20.1881\n",
      "Epoch 295/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 22.0202 - mse: 20.1768\n",
      "Epoch 296/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.9991 - mse: 20.1655\n",
      "Epoch 297/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.9782 - mse: 20.1544\n",
      "Epoch 298/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.9575 - mse: 20.1434\n",
      "Epoch 299/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.9370 - mse: 20.1325\n",
      "Epoch 300/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.9167 - mse: 20.1217\n",
      "Epoch 301/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.8965 - mse: 20.1109\n",
      "Epoch 302/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.8766 - mse: 20.1003\n",
      "Epoch 303/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.8568 - mse: 20.0897\n",
      "Epoch 304/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 21.8372 - mse: 20.0793\n",
      "Epoch 305/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 21.8178 - mse: 20.0689\n",
      "Epoch 306/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 21.7985 - mse: 20.0586\n",
      "Epoch 307/1000\n",
      "11/11 [==============================] - 0s 634us/step - loss: 21.7794 - mse: 20.0484\n",
      "Epoch 308/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.7605 - mse: 20.0382\n",
      "Epoch 309/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 21.7417 - mse: 20.0282\n",
      "Epoch 310/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.7231 - mse: 20.0182\n",
      "Epoch 311/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.7046 - mse: 20.0082\n",
      "Epoch 312/1000\n",
      "11/11 [==============================] - ETA: 0s - loss: 24.0533 - mse: 22.360 - 0s 635us/step - loss: 21.6863 - mse: 19.9984\n",
      "Epoch 313/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 21.6681 - mse: 19.9886\n",
      "Epoch 314/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 21.6501 - mse: 19.9788\n",
      "Epoch 315/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.6322 - mse: 19.9691\n",
      "Epoch 316/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 21.6145 - mse: 19.9595\n",
      "Epoch 317/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 21.5968 - mse: 19.9499\n",
      "Epoch 318/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.5793 - mse: 19.9404\n",
      "Epoch 319/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.5620 - mse: 19.9309\n",
      "Epoch 320/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.5447 - mse: 19.9215\n",
      "Epoch 321/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.5276 - mse: 19.9121\n",
      "Epoch 322/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.5106 - mse: 19.9028\n",
      "Epoch 323/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 21.4937 - mse: 19.8935\n",
      "Epoch 324/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 21.4770 - mse: 19.8843\n",
      "Epoch 325/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.4603 - mse: 19.8751\n",
      "Epoch 326/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.4438 - mse: 19.8659\n",
      "Epoch 327/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.4273 - mse: 19.8568\n",
      "Epoch 328/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.4110 - mse: 19.8477\n",
      "Epoch 329/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.3948 - mse: 19.8386\n",
      "Epoch 330/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.3787 - mse: 19.8296\n",
      "Epoch 331/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.3626 - mse: 19.8206\n",
      "Epoch 332/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.3467 - mse: 19.8116\n",
      "Epoch 333/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 21.3308 - mse: 19.8026\n",
      "Epoch 334/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.3151 - mse: 19.7937\n",
      "Epoch 335/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.2994 - mse: 19.7848\n",
      "Epoch 336/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.2839 - mse: 19.7759\n",
      "Epoch 337/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.2684 - mse: 19.7671\n",
      "Epoch 338/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 21.2530 - mse: 19.7583\n",
      "Epoch 339/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 21.2376 - mse: 19.7494\n",
      "Epoch 340/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.2224 - mse: 19.7406\n",
      "Epoch 341/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 21.2072 - mse: 19.7318\n",
      "Epoch 342/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 21.1921 - mse: 19.7231\n",
      "Epoch 343/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.1771 - mse: 19.7143\n",
      "Epoch 344/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.1622 - mse: 19.7056\n",
      "Epoch 345/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.1473 - mse: 19.6968\n",
      "Epoch 346/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.1325 - mse: 19.6881\n",
      "Epoch 347/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.1177 - mse: 19.6794\n",
      "Epoch 348/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.1030 - mse: 19.6707\n",
      "Epoch 349/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.0884 - mse: 19.6620\n",
      "Epoch 350/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.0739 - mse: 19.6533\n",
      "Epoch 351/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.0594 - mse: 19.6446\n",
      "Epoch 352/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.0450 - mse: 19.6360\n",
      "Epoch 353/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 21.0306 - mse: 19.6273\n",
      "Epoch 354/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.0163 - mse: 19.6186\n",
      "Epoch 355/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 21.0020 - mse: 19.6099\n",
      "Epoch 356/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.9878 - mse: 19.6013\n",
      "Epoch 357/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 20.9736 - mse: 19.5926\n",
      "Epoch 358/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 20.9595 - mse: 19.5839\n",
      "Epoch 359/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 20.9455 - mse: 19.5753\n",
      "Epoch 360/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 20.9315 - mse: 19.5666\n",
      "Epoch 361/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.9175 - mse: 19.5580\n",
      "Epoch 362/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.9036 - mse: 19.5493\n",
      "Epoch 363/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.8897 - mse: 19.5406\n",
      "Epoch 364/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.8759 - mse: 19.5319\n",
      "Epoch 365/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.8621 - mse: 19.5233\n",
      "Epoch 366/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 20.8484 - mse: 19.5146\n",
      "Epoch 367/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 20.8347 - mse: 19.5059\n",
      "Epoch 368/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.8210 - mse: 19.4972\n",
      "Epoch 369/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.8074 - mse: 19.4885\n",
      "Epoch 370/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.7938 - mse: 19.4798\n",
      "Epoch 371/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.7803 - mse: 19.4711\n",
      "Epoch 372/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.7668 - mse: 19.4624\n",
      "Epoch 373/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.7533 - mse: 19.4537\n",
      "Epoch 374/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.7399 - mse: 19.4450\n",
      "Epoch 375/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 20.7264 - mse: 19.4362\n",
      "Epoch 376/1000\n",
      "11/11 [==============================] - 0s 490us/step - loss: 20.7131 - mse: 19.4275\n",
      "Epoch 377/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.6997 - mse: 19.4188\n",
      "Epoch 378/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.6864 - mse: 19.4100\n",
      "Epoch 379/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.6732 - mse: 19.4013\n",
      "Epoch 380/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.6599 - mse: 19.3925\n",
      "Epoch 381/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.6467 - mse: 19.3837\n",
      "Epoch 382/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 20.6335 - mse: 19.3749\n",
      "Epoch 383/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 20.6203 - mse: 19.3661\n",
      "Epoch 384/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.6072 - mse: 19.3573\n",
      "Epoch 385/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.5941 - mse: 19.3485\n",
      "Epoch 386/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.5810 - mse: 19.3397\n",
      "Epoch 387/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.5680 - mse: 19.3309\n",
      "Epoch 388/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 20.5550 - mse: 19.3221\n",
      "Epoch 389/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 20.5420 - mse: 19.3132\n",
      "Epoch 390/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.5290 - mse: 19.3044\n",
      "Epoch 391/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.5160 - mse: 19.2955\n",
      "Epoch 392/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 20.5031 - mse: 19.2867\n",
      "Epoch 393/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.4902 - mse: 19.2778\n",
      "Epoch 394/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.4773 - mse: 19.2689\n",
      "Epoch 395/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.4645 - mse: 19.2600\n",
      "Epoch 396/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.4516 - mse: 19.2511\n",
      "Epoch 397/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.4388 - mse: 19.2422\n",
      "Epoch 398/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.4260 - mse: 19.2333\n",
      "Epoch 399/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.4132 - mse: 19.2244\n",
      "Epoch 400/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.4005 - mse: 19.2154\n",
      "Epoch 401/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 20.3878 - mse: 19.2065\n",
      "Epoch 402/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 20.3750 - mse: 19.1975\n",
      "Epoch 403/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.3624 - mse: 19.1886\n",
      "Epoch 404/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.3497 - mse: 19.1796\n",
      "Epoch 405/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.3370 - mse: 19.1706\n",
      "Epoch 406/1000\n",
      "11/11 [==============================] - 0s 473us/step - loss: 20.3244 - mse: 19.1617\n",
      "Epoch 407/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.3118 - mse: 19.1527\n",
      "Epoch 408/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.2992 - mse: 19.1437\n",
      "Epoch 409/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.2866 - mse: 19.1347\n",
      "Epoch 410/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.2741 - mse: 19.1257\n",
      "Epoch 411/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.2615 - mse: 19.1167\n",
      "Epoch 412/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.2490 - mse: 19.1076\n",
      "Epoch 413/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 20.2365 - mse: 19.0986\n",
      "Epoch 414/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 20.2240 - mse: 19.0896\n",
      "Epoch 415/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 20.2115 - mse: 19.0806\n",
      "Epoch 416/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.1991 - mse: 19.0715\n",
      "Epoch 417/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.1867 - mse: 19.0625\n",
      "Epoch 418/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.1743 - mse: 19.0534\n",
      "Epoch 419/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.1619 - mse: 19.0444\n",
      "Epoch 420/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.1495 - mse: 19.0353\n",
      "Epoch 421/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.1371 - mse: 19.0262\n",
      "Epoch 422/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 20.1248 - mse: 19.0172\n",
      "Epoch 423/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.1125 - mse: 19.0081\n",
      "Epoch 424/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.1001 - mse: 18.9990\n",
      "Epoch 425/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.0879 - mse: 18.9900\n",
      "Epoch 426/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 20.0756 - mse: 18.9809\n",
      "Epoch 427/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.0633 - mse: 18.9718\n",
      "Epoch 428/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.0511 - mse: 18.9627\n",
      "Epoch 429/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.0389 - mse: 18.9536\n",
      "Epoch 430/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.0266 - mse: 18.9445\n",
      "Epoch 431/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 20.0145 - mse: 18.9355\n",
      "Epoch 432/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 20.0023 - mse: 18.9264\n",
      "Epoch 433/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.9901 - mse: 18.9173\n",
      "Epoch 434/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.9780 - mse: 18.9082\n",
      "Epoch 435/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 19.9659 - mse: 18.8991\n",
      "Epoch 436/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.9538 - mse: 18.8900\n",
      "Epoch 437/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.9417 - mse: 18.8809\n",
      "Epoch 438/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.9296 - mse: 18.8718\n",
      "Epoch 439/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.9176 - mse: 18.8627\n",
      "Epoch 440/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.9055 - mse: 18.8536\n",
      "Epoch 441/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.8935 - mse: 18.8446\n",
      "Epoch 442/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 19.8815 - mse: 18.8355\n",
      "Epoch 443/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.8695 - mse: 18.8264\n",
      "Epoch 444/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.8576 - mse: 18.8173\n",
      "Epoch 445/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.8456 - mse: 18.8082\n",
      "Epoch 446/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.8337 - mse: 18.7992\n",
      "Epoch 447/1000\n",
      "11/11 [==============================] - 0s 472us/step - loss: 19.8218 - mse: 18.7901\n",
      "Epoch 448/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.8099 - mse: 18.7810\n",
      "Epoch 449/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.7980 - mse: 18.7720\n",
      "Epoch 450/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.7862 - mse: 18.7629\n",
      "Epoch 451/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.7743 - mse: 18.7538\n",
      "Epoch 452/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.7625 - mse: 18.7448\n",
      "Epoch 453/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.7507 - mse: 18.7358\n",
      "Epoch 454/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.7389 - mse: 18.7267\n",
      "Epoch 455/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 19.7272 - mse: 18.7177\n",
      "Epoch 456/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.7154 - mse: 18.7087\n",
      "Epoch 457/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.7037 - mse: 18.6996\n",
      "Epoch 458/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.6920 - mse: 18.6906\n",
      "Epoch 459/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.6803 - mse: 18.6816\n",
      "Epoch 460/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.6686 - mse: 18.6726\n",
      "Epoch 461/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.6570 - mse: 18.6636\n",
      "Epoch 462/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 19.6453 - mse: 18.6546\n",
      "Epoch 463/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.6337 - mse: 18.6457\n",
      "Epoch 464/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.6222 - mse: 18.6367\n",
      "Epoch 465/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.6106 - mse: 18.6277\n",
      "Epoch 466/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.5990 - mse: 18.6188\n",
      "Epoch 467/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.5875 - mse: 18.6098\n",
      "Epoch 468/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.5760 - mse: 18.6009\n",
      "Epoch 469/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.5645 - mse: 18.5920\n",
      "Epoch 470/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.5530 - mse: 18.5831\n",
      "Epoch 471/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.5416 - mse: 18.5742\n",
      "Epoch 472/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.5302 - mse: 18.5653\n",
      "Epoch 473/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.5188 - mse: 18.5564\n",
      "Epoch 474/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 19.5074 - mse: 18.5475\n",
      "Epoch 475/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.4960 - mse: 18.5387\n",
      "Epoch 476/1000\n",
      "11/11 [==============================] - 0s 907us/step - loss: 19.4847 - mse: 18.5298\n",
      "Epoch 477/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.4734 - mse: 18.5210\n",
      "Epoch 478/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 19.4621 - mse: 18.5122\n",
      "Epoch 479/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 19.4508 - mse: 18.5033\n",
      "Epoch 480/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.4395 - mse: 18.4945\n",
      "Epoch 481/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 19.4283 - mse: 18.4858\n",
      "Epoch 482/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 19.4171 - mse: 18.4770\n",
      "Epoch 483/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.4059 - mse: 18.4682\n",
      "Epoch 484/1000\n",
      "11/11 [==============================] - 0s 907us/step - loss: 19.3947 - mse: 18.4595\n",
      "Epoch 485/1000\n",
      "11/11 [==============================] - 0s 614us/step - loss: 19.3836 - mse: 18.4507\n",
      "Epoch 486/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.3725 - mse: 18.4420\n",
      "Epoch 487/1000\n",
      "11/11 [==============================] - 0s 907us/step - loss: 19.3614 - mse: 18.4333\n",
      "Epoch 488/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.3503 - mse: 18.4246\n",
      "Epoch 489/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 19.3393 - mse: 18.4160\n",
      "Epoch 490/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 19.3282 - mse: 18.4073\n",
      "Epoch 491/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 19.3172 - mse: 18.3987\n",
      "Epoch 492/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.3063 - mse: 18.3900\n",
      "Epoch 493/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 19.2953 - mse: 18.3814\n",
      "Epoch 494/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.2844 - mse: 18.3728\n",
      "Epoch 495/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 19.2735 - mse: 18.3642\n",
      "Epoch 496/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.2626 - mse: 18.3557\n",
      "Epoch 497/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 19.2517 - mse: 18.3471\n",
      "Epoch 498/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.2409 - mse: 18.3386\n",
      "Epoch 499/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.2301 - mse: 18.3301\n",
      "Epoch 500/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.2193 - mse: 18.3216\n",
      "Epoch 501/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.2086 - mse: 18.3131\n",
      "Epoch 502/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 19.1978 - mse: 18.3046\n",
      "Epoch 503/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.1871 - mse: 18.2962\n",
      "Epoch 504/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.1764 - mse: 18.2878\n",
      "Epoch 505/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.1658 - mse: 18.2793\n",
      "Epoch 506/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.1551 - mse: 18.2710\n",
      "Epoch 507/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.1445 - mse: 18.2626\n",
      "Epoch 508/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.1340 - mse: 18.2542\n",
      "Epoch 509/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.1234 - mse: 18.2459\n",
      "Epoch 510/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.1129 - mse: 18.2376\n",
      "Epoch 511/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.1024 - mse: 18.2293\n",
      "Epoch 512/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.0919 - mse: 18.2210\n",
      "Epoch 513/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.0815 - mse: 18.2127\n",
      "Epoch 514/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.0710 - mse: 18.2045\n",
      "Epoch 515/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.0607 - mse: 18.1963\n",
      "Epoch 516/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 19.0503 - mse: 18.1881\n",
      "Epoch 517/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.0399 - mse: 18.1799\n",
      "Epoch 518/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.0296 - mse: 18.1718\n",
      "Epoch 519/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.0193 - mse: 18.1636\n",
      "Epoch 520/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 19.0091 - mse: 18.1555\n",
      "Epoch 521/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.9989 - mse: 18.1474\n",
      "Epoch 522/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.9887 - mse: 18.1393\n",
      "Epoch 523/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.9785 - mse: 18.1313\n",
      "Epoch 524/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.9683 - mse: 18.1233\n",
      "Epoch 525/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.9582 - mse: 18.1152\n",
      "Epoch 526/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.9481 - mse: 18.1073\n",
      "Epoch 527/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.9381 - mse: 18.0993\n",
      "Epoch 528/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.9280 - mse: 18.0913\n",
      "Epoch 529/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.9180 - mse: 18.0834\n",
      "Epoch 530/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.9081 - mse: 18.0755\n",
      "Epoch 531/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.8981 - mse: 18.0676\n",
      "Epoch 532/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.8882 - mse: 18.0598\n",
      "Epoch 533/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.8783 - mse: 18.0520\n",
      "Epoch 534/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.8685 - mse: 18.0442\n",
      "Epoch 535/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.8586 - mse: 18.0364\n",
      "Epoch 536/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.8488 - mse: 18.0286\n",
      "Epoch 537/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.8391 - mse: 18.0209\n",
      "Epoch 538/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.8293 - mse: 18.0132\n",
      "Epoch 539/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.8196 - mse: 18.0055\n",
      "Epoch 540/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.8099 - mse: 17.9978\n",
      "Epoch 541/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.8003 - mse: 17.9901\n",
      "Epoch 542/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.7906 - mse: 17.9825\n",
      "Epoch 543/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.7811 - mse: 17.9749\n",
      "Epoch 544/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.7715 - mse: 17.9674\n",
      "Epoch 545/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.7620 - mse: 17.9598\n",
      "Epoch 546/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.7525 - mse: 17.9523\n",
      "Epoch 547/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.7430 - mse: 17.9448\n",
      "Epoch 548/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.7335 - mse: 17.9373\n",
      "Epoch 549/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.7241 - mse: 17.9299\n",
      "Epoch 550/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.7148 - mse: 17.9224\n",
      "Epoch 551/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.7054 - mse: 17.9150\n",
      "Epoch 552/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.6961 - mse: 17.9076\n",
      "Epoch 553/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.6868 - mse: 17.9003\n",
      "Epoch 554/1000\n",
      "11/11 [==============================] - 0s 363us/step - loss: 18.6776 - mse: 17.8930\n",
      "Epoch 555/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.6683 - mse: 17.8857\n",
      "Epoch 556/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.6591 - mse: 17.8784\n",
      "Epoch 557/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.6500 - mse: 17.8711\n",
      "Epoch 558/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.6408 - mse: 17.8639\n",
      "Epoch 559/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.6317 - mse: 17.8567\n",
      "Epoch 560/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.6227 - mse: 17.8495\n",
      "Epoch 561/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.6136 - mse: 17.8424\n",
      "Epoch 562/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.6046 - mse: 17.8353\n",
      "Epoch 563/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.5957 - mse: 17.8282\n",
      "Epoch 564/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.5867 - mse: 17.8211\n",
      "Epoch 565/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.5778 - mse: 17.8140\n",
      "Epoch 566/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.5689 - mse: 17.8070\n",
      "Epoch 567/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.5601 - mse: 17.8000\n",
      "Epoch 568/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.5513 - mse: 17.7931\n",
      "Epoch 569/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.5425 - mse: 17.7861\n",
      "Epoch 570/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.5337 - mse: 17.7792\n",
      "Epoch 571/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.5250 - mse: 17.7723\n",
      "Epoch 572/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.5163 - mse: 17.7654\n",
      "Epoch 573/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.5077 - mse: 17.7586\n",
      "Epoch 574/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.4991 - mse: 17.7518\n",
      "Epoch 575/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.4905 - mse: 17.7450\n",
      "Epoch 576/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.4819 - mse: 17.7383\n",
      "Epoch 577/1000\n",
      "11/11 [==============================] - 0s 634us/step - loss: 18.4734 - mse: 17.7315\n",
      "Epoch 578/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.4649 - mse: 17.7248\n",
      "Epoch 579/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.4565 - mse: 17.7181\n",
      "Epoch 580/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.4480 - mse: 17.7115\n",
      "Epoch 581/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.4396 - mse: 17.7049\n",
      "Epoch 582/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.4313 - mse: 17.6983\n",
      "Epoch 583/1000\n",
      "11/11 [==============================] - 0s 614us/step - loss: 18.4230 - mse: 17.6917\n",
      "Epoch 584/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.4147 - mse: 17.6852\n",
      "Epoch 585/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.4064 - mse: 17.6787\n",
      "Epoch 586/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.3982 - mse: 17.6722\n",
      "Epoch 587/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.3900 - mse: 17.6657\n",
      "Epoch 588/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.3818 - mse: 17.6593\n",
      "Epoch 589/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.3737 - mse: 17.6529\n",
      "Epoch 590/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.3656 - mse: 17.6465\n",
      "Epoch 591/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.3575 - mse: 17.6401\n",
      "Epoch 592/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.3495 - mse: 17.6338\n",
      "Epoch 593/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.3415 - mse: 17.6275\n",
      "Epoch 594/1000\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.3335 - mse: 17.6212\n",
      "Epoch 595/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 18.3256 - mse: 17.6150\n",
      "Epoch 596/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 18.3177 - mse: 17.6088\n",
      "Epoch 597/1000\n",
      "11/11 [==============================] - 0s 997us/step - loss: 18.3099 - mse: 17.6026\n",
      "Epoch 598/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 18.3020 - mse: 17.5964\n",
      "Epoch 599/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.2942 - mse: 17.5903\n",
      "Epoch 600/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 18.2865 - mse: 17.5842\n",
      "Epoch 601/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.2787 - mse: 17.5781\n",
      "Epoch 602/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.2710 - mse: 17.5721\n",
      "Epoch 603/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.2634 - mse: 17.5660\n",
      "Epoch 604/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.2558 - mse: 17.5600\n",
      "Epoch 605/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.2482 - mse: 17.5541\n",
      "Epoch 606/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.2406 - mse: 17.5481\n",
      "Epoch 607/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.2331 - mse: 17.5422\n",
      "Epoch 608/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.2256 - mse: 17.5363\n",
      "Epoch 609/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.2181 - mse: 17.5305\n",
      "Epoch 610/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.2107 - mse: 17.5247\n",
      "Epoch 611/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 18.2033 - mse: 17.5189\n",
      "Epoch 612/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.1959 - mse: 17.5131\n",
      "Epoch 613/1000\n",
      "11/11 [==============================] - 0s 543us/step - loss: 18.1886 - mse: 17.5073\n",
      "Epoch 614/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.1813 - mse: 17.5016\n",
      "Epoch 615/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.1740 - mse: 17.4959\n",
      "Epoch 616/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.1668 - mse: 17.4903\n",
      "Epoch 617/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.1596 - mse: 17.4846\n",
      "Epoch 618/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.1525 - mse: 17.4790\n",
      "Epoch 619/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.1453 - mse: 17.4734\n",
      "Epoch 620/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.1382 - mse: 17.4679\n",
      "Epoch 621/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.1312 - mse: 17.4623\n",
      "Epoch 622/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.1241 - mse: 17.4568\n",
      "Epoch 623/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.1172 - mse: 17.4514\n",
      "Epoch 624/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.1102 - mse: 17.4459\n",
      "Epoch 625/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.1033 - mse: 17.4405\n",
      "Epoch 626/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 18.0964 - mse: 17.4351\n",
      "Epoch 627/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 18.0895 - mse: 17.4298\n",
      "Epoch 628/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 18.0827 - mse: 17.4244\n",
      "Epoch 629/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 18.0759 - mse: 17.4191\n",
      "Epoch 630/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 18.0691 - mse: 17.4139\n",
      "Epoch 631/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.0624 - mse: 17.4086\n",
      "Epoch 632/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.0557 - mse: 17.4034\n",
      "Epoch 633/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.0491 - mse: 17.3982\n",
      "Epoch 634/1000\n",
      "11/11 [==============================] - 0s 540us/step - loss: 18.0424 - mse: 17.3930\n",
      "Epoch 635/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.0358 - mse: 17.3879\n",
      "Epoch 636/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.0293 - mse: 17.3828\n",
      "Epoch 637/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 18.0227 - mse: 17.3777\n",
      "Epoch 638/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 18.0163 - mse: 17.3726\n",
      "Epoch 639/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.0098 - mse: 17.3676\n",
      "Epoch 640/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 18.0034 - mse: 17.3626\n",
      "Epoch 641/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 17.9970 - mse: 17.3576\n",
      "Epoch 642/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.9906 - mse: 17.3526\n",
      "Epoch 643/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.9843 - mse: 17.3477\n",
      "Epoch 644/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 17.9780 - mse: 17.3428\n",
      "Epoch 645/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.9717 - mse: 17.3380\n",
      "Epoch 646/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.9655 - mse: 17.3331\n",
      "Epoch 647/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.9593 - mse: 17.3283\n",
      "Epoch 648/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.9531 - mse: 17.3235\n",
      "Epoch 649/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.9470 - mse: 17.3188\n",
      "Epoch 650/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.9409 - mse: 17.3140\n",
      "Epoch 651/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.9348 - mse: 17.3093\n",
      "Epoch 652/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.9288 - mse: 17.3046\n",
      "Epoch 653/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.9228 - mse: 17.3000\n",
      "Epoch 654/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.9168 - mse: 17.2954\n",
      "Epoch 655/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.9109 - mse: 17.2908\n",
      "Epoch 656/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.9050 - mse: 17.2862\n",
      "Epoch 657/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.8991 - mse: 17.2817\n",
      "Epoch 658/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.8933 - mse: 17.2771\n",
      "Epoch 659/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.8875 - mse: 17.2726\n",
      "Epoch 660/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 17.8817 - mse: 17.2682\n",
      "Epoch 661/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 17.8760 - mse: 17.2637\n",
      "Epoch 662/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.8703 - mse: 17.2593\n",
      "Epoch 663/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.8646 - mse: 17.2549\n",
      "Epoch 664/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.8590 - mse: 17.2506\n",
      "Epoch 665/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.8534 - mse: 17.2463\n",
      "Epoch 666/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.8478 - mse: 17.2419\n",
      "Epoch 667/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 17.8422 - mse: 17.2377\n",
      "Epoch 668/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.8367 - mse: 17.2334\n",
      "Epoch 669/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.8312 - mse: 17.2292\n",
      "Epoch 670/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.8258 - mse: 17.2250\n",
      "Epoch 671/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 17.8204 - mse: 17.2208\n",
      "Epoch 672/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.8150 - mse: 17.2167\n",
      "Epoch 673/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.8096 - mse: 17.2125\n",
      "Epoch 674/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.8043 - mse: 17.2084\n",
      "Epoch 675/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.7990 - mse: 17.2044\n",
      "Epoch 676/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.7937 - mse: 17.2003\n",
      "Epoch 677/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.7885 - mse: 17.1963\n",
      "Epoch 678/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 17.7833 - mse: 17.1923\n",
      "Epoch 679/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.7781 - mse: 17.1883\n",
      "Epoch 680/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.7730 - mse: 17.1844\n",
      "Epoch 681/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.7679 - mse: 17.1805\n",
      "Epoch 682/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.7628 - mse: 17.1766\n",
      "Epoch 683/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.7578 - mse: 17.1727\n",
      "Epoch 684/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.7528 - mse: 17.1689\n",
      "Epoch 685/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.7478 - mse: 17.1651\n",
      "Epoch 686/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.7428 - mse: 17.1613\n",
      "Epoch 687/1000\n",
      "11/11 [==============================] - 0s 656us/step - loss: 17.7379 - mse: 17.1575\n",
      "Epoch 688/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.7330 - mse: 17.1538\n",
      "Epoch 689/1000\n",
      "11/11 [==============================] - 0s 907us/step - loss: 17.7282 - mse: 17.1501\n",
      "Epoch 690/1000\n",
      "11/11 [==============================] - 0s 633us/step - loss: 17.7234 - mse: 17.1464\n",
      "Epoch 691/1000\n",
      "11/11 [==============================] - 0s 549us/step - loss: 17.7186 - mse: 17.1427\n",
      "Epoch 692/1000\n",
      "11/11 [==============================] - 0s 907us/step - loss: 17.7138 - mse: 17.1391\n",
      "Epoch 693/1000\n",
      "11/11 [==============================] - 0s 636us/step - loss: 17.7090 - mse: 17.1354\n",
      "Epoch 694/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.7043 - mse: 17.1318\n",
      "Epoch 695/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.6997 - mse: 17.1283\n",
      "Epoch 696/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.6950 - mse: 17.1247\n",
      "Epoch 697/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.6904 - mse: 17.1212\n",
      "Epoch 698/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.6858 - mse: 17.1177\n",
      "Epoch 699/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.6813 - mse: 17.1143\n",
      "Epoch 700/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.6767 - mse: 17.1108\n",
      "Epoch 701/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.6722 - mse: 17.1074\n",
      "Epoch 702/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.6678 - mse: 17.1040\n",
      "Epoch 703/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.6633 - mse: 17.1006\n",
      "Epoch 704/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.6589 - mse: 17.0973\n",
      "Epoch 705/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.6546 - mse: 17.0939\n",
      "Epoch 706/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.6502 - mse: 17.0906\n",
      "Epoch 707/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.6459 - mse: 17.0874\n",
      "Epoch 708/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.6416 - mse: 17.0841\n",
      "Epoch 709/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.6373 - mse: 17.0809\n",
      "Epoch 710/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.6331 - mse: 17.0777\n",
      "Epoch 711/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 17.6289 - mse: 17.0745\n",
      "Epoch 712/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.6247 - mse: 17.0713\n",
      "Epoch 713/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 17.6206 - mse: 17.0682\n",
      "Epoch 714/1000\n",
      "11/11 [==============================] - 0s 997us/step - loss: 17.6164 - mse: 17.0651\n",
      "Epoch 715/1000\n",
      "11/11 [==============================] - 0s 997us/step - loss: 17.6124 - mse: 17.0620\n",
      "Epoch 716/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 17.6083 - mse: 17.0589\n",
      "Epoch 717/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 17.6043 - mse: 17.0559\n",
      "Epoch 718/1000\n",
      "11/11 [==============================] - 0s 638us/step - loss: 17.6003 - mse: 17.0529\n",
      "Epoch 719/1000\n",
      "11/11 [==============================] - 0s 997us/step - loss: 17.5963 - mse: 17.0499\n",
      "Epoch 720/1000\n",
      "11/11 [==============================] - 0s 907us/step - loss: 17.5923 - mse: 17.0469\n",
      "Epoch 721/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.5884 - mse: 17.0439\n",
      "Epoch 722/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.5845 - mse: 17.0410\n",
      "Epoch 723/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.5806 - mse: 17.0381\n",
      "Epoch 724/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.5768 - mse: 17.0352\n",
      "Epoch 725/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 17.5730 - mse: 17.0323\n",
      "Epoch 726/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.5692 - mse: 17.0295\n",
      "Epoch 727/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 17.5655 - mse: 17.0267\n",
      "Epoch 728/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.5617 - mse: 17.0239\n",
      "Epoch 729/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.5580 - mse: 17.0211\n",
      "Epoch 730/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.5544 - mse: 17.0183\n",
      "Epoch 731/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.5507 - mse: 17.0156\n",
      "Epoch 732/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.5471 - mse: 17.0129\n",
      "Epoch 733/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.5435 - mse: 17.0102\n",
      "Epoch 734/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.5399 - mse: 17.0075\n",
      "Epoch 735/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.5364 - mse: 17.0049\n",
      "Epoch 736/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.5329 - mse: 17.0023\n",
      "Epoch 737/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.5294 - mse: 16.9997\n",
      "Epoch 738/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.5259 - mse: 16.9971\n",
      "Epoch 739/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.5225 - mse: 16.9945\n",
      "Epoch 740/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.5191 - mse: 16.9920\n",
      "Epoch 741/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.5157 - mse: 16.9895\n",
      "Epoch 742/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.5123 - mse: 16.9870\n",
      "Epoch 743/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.5090 - mse: 16.9845\n",
      "Epoch 744/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.5057 - mse: 16.9820\n",
      "Epoch 745/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.5024 - mse: 16.9796\n",
      "Epoch 746/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.4991 - mse: 16.9772\n",
      "Epoch 747/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.4959 - mse: 16.9748\n",
      "Epoch 748/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.4927 - mse: 16.9724\n",
      "Epoch 749/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.4895 - mse: 16.9700\n",
      "Epoch 750/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.4864 - mse: 16.9677\n",
      "Epoch 751/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.4832 - mse: 16.9654\n",
      "Epoch 752/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.4801 - mse: 16.9631\n",
      "Epoch 753/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.4771 - mse: 16.9608\n",
      "Epoch 754/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.4740 - mse: 16.9586\n",
      "Epoch 755/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.4710 - mse: 16.9563\n",
      "Epoch 756/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.4680 - mse: 16.9541\n",
      "Epoch 757/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.4650 - mse: 16.9519\n",
      "Epoch 758/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.4620 - mse: 16.9497\n",
      "Epoch 759/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.4591 - mse: 16.9476\n",
      "Epoch 760/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.4562 - mse: 16.9454\n",
      "Epoch 761/1000\n",
      "11/11 [==============================] - 0s 726us/step - loss: 17.4533 - mse: 16.9433\n",
      "Epoch 762/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 17.4504 - mse: 16.9412\n",
      "Epoch 763/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 17.4476 - mse: 16.9391\n",
      "Epoch 764/1000\n",
      "11/11 [==============================] - 0s 907us/step - loss: 17.4447 - mse: 16.9370\n",
      "Epoch 765/1000\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 17.4420 - mse: 16.9350\n",
      "Epoch 766/1000\n",
      "11/11 [==============================] - 0s 907us/step - loss: 17.4392 - mse: 16.9330\n",
      "Epoch 767/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 17.4364 - mse: 16.9310\n",
      "Epoch 768/1000\n",
      "11/11 [==============================] - 0s 907us/step - loss: 17.4337 - mse: 16.9290\n",
      "Epoch 769/1000\n",
      "11/11 [==============================] - 0s 997us/step - loss: 17.4310 - mse: 16.9270\n",
      "Epoch 770/1000\n",
      "11/11 [==============================] - 0s 726us/step - loss: 17.4283 - mse: 16.9250\n",
      "Epoch 771/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.4257 - mse: 16.9231\n",
      "Epoch 772/1000\n",
      "11/11 [==============================] - 0s 906us/step - loss: 17.4230 - mse: 16.9212\n",
      "Epoch 773/1000\n",
      "11/11 [==============================] - 0s 997us/step - loss: 17.4204 - mse: 16.9193\n",
      "Epoch 774/1000\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 17.4178 - mse: 16.9174\n",
      "Epoch 775/1000\n",
      "11/11 [==============================] - 0s 907us/step - loss: 17.4153 - mse: 16.9155\n",
      "Epoch 776/1000\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 17.4127 - mse: 16.9137\n",
      "Epoch 777/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.4102 - mse: 16.9118\n",
      "Epoch 778/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 17.4077 - mse: 16.9100\n",
      "Epoch 779/1000\n",
      "11/11 [==============================] - 0s 772us/step - loss: 17.4052 - mse: 16.9082\n",
      "Epoch 780/1000\n",
      "11/11 [==============================] - 0s 998us/step - loss: 17.4027 - mse: 16.9065\n",
      "Epoch 781/1000\n",
      "11/11 [==============================] - 0s 998us/step - loss: 17.4003 - mse: 16.9047\n",
      "Epoch 782/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.3979 - mse: 16.9029\n",
      "Epoch 783/1000\n",
      "11/11 [==============================] - 0s 726us/step - loss: 17.3955 - mse: 16.9012\n",
      "Epoch 784/1000\n",
      "11/11 [==============================] - 0s 944us/step - loss: 17.3931 - mse: 16.8995\n",
      "Epoch 785/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 17.3908 - mse: 16.8978\n",
      "Epoch 786/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.3884 - mse: 16.8961\n",
      "Epoch 787/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.3861 - mse: 16.8945\n",
      "Epoch 788/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.3838 - mse: 16.8928\n",
      "Epoch 789/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.3816 - mse: 16.8912\n",
      "Epoch 790/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.3793 - mse: 16.8896\n",
      "Epoch 791/1000\n",
      "11/11 [==============================] - 0s 634us/step - loss: 17.3771 - mse: 16.8880\n",
      "Epoch 792/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.3749 - mse: 16.8864\n",
      "Epoch 793/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.3727 - mse: 16.8848\n",
      "Epoch 794/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.3705 - mse: 16.8833\n",
      "Epoch 795/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.3683 - mse: 16.8817\n",
      "Epoch 796/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.3662 - mse: 16.8802\n",
      "Epoch 797/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.3641 - mse: 16.8787\n",
      "Epoch 798/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.3620 - mse: 16.8772\n",
      "Epoch 799/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.3599 - mse: 16.8758\n",
      "Epoch 800/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.3579 - mse: 16.8743\n",
      "Epoch 801/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 17.3559 - mse: 16.8729\n",
      "Epoch 802/1000\n",
      "11/11 [==============================] - 0s 569us/step - loss: 17.3538 - mse: 16.8714\n",
      "Epoch 803/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.3518 - mse: 16.8700\n",
      "Epoch 804/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.3499 - mse: 16.8686\n",
      "Epoch 805/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.3479 - mse: 16.8672\n",
      "Epoch 806/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 17.3460 - mse: 16.8659\n",
      "Epoch 807/1000\n",
      "11/11 [==============================] - 0s 998us/step - loss: 17.3440 - mse: 16.8645\n",
      "Epoch 808/1000\n",
      "11/11 [==============================] - 0s 823us/step - loss: 17.3421 - mse: 16.8632\n",
      "Epoch 809/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 17.3403 - mse: 16.8618\n",
      "Epoch 810/1000\n",
      "11/11 [==============================] - 0s 726us/step - loss: 17.3384 - mse: 16.8605\n",
      "Epoch 811/1000\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 17.3365 - mse: 16.8592\n",
      "Epoch 812/1000\n",
      "11/11 [==============================] - 0s 997us/step - loss: 17.3347 - mse: 16.8579\n",
      "Epoch 813/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.3329 - mse: 16.8567\n",
      "Epoch 814/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 17.3311 - mse: 16.8554\n",
      "Epoch 815/1000\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 17.3293 - mse: 16.8542\n",
      "Epoch 816/1000\n",
      "11/11 [==============================] - 0s 997us/step - loss: 17.3276 - mse: 16.8529\n",
      "Epoch 817/1000\n",
      "11/11 [==============================] - 0s 907us/step - loss: 17.3258 - mse: 16.8517\n",
      "Epoch 818/1000\n",
      "11/11 [==============================] - 0s 997us/step - loss: 17.3241 - mse: 16.8505\n",
      "Epoch 819/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.3224 - mse: 16.8493\n",
      "Epoch 820/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 17.3207 - mse: 16.8482\n",
      "Epoch 821/1000\n",
      "11/11 [==============================] - 0s 906us/step - loss: 17.3190 - mse: 16.8470\n",
      "Epoch 822/1000\n",
      "11/11 [==============================] - 0s 726us/step - loss: 17.3173 - mse: 16.8458\n",
      "Epoch 823/1000\n",
      "11/11 [==============================] - ETA: 0s - loss: 19.2708 - mse: 18.797 - 0s 725us/step - loss: 17.3157 - mse: 16.8447\n",
      "Epoch 824/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.3141 - mse: 16.8436\n",
      "Epoch 825/1000\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 17.3125 - mse: 16.8425\n",
      "Epoch 826/1000\n",
      "11/11 [==============================] - 0s 995us/step - loss: 17.3109 - mse: 16.8414\n",
      "Epoch 827/1000\n",
      "11/11 [==============================] - 0s 907us/step - loss: 17.3093 - mse: 16.8403\n",
      "Epoch 828/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.3077 - mse: 16.8392\n",
      "Epoch 829/1000\n",
      "11/11 [==============================] - 0s 634us/step - loss: 17.3062 - mse: 16.8381\n",
      "Epoch 830/1000\n",
      "11/11 [==============================] - 0s 634us/step - loss: 17.3047 - mse: 16.8371\n",
      "Epoch 831/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.3031 - mse: 16.8360\n",
      "Epoch 832/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.3016 - mse: 16.8350\n",
      "Epoch 833/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.3001 - mse: 16.8340\n",
      "Epoch 834/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2987 - mse: 16.8330\n",
      "Epoch 835/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2972 - mse: 16.8320\n",
      "Epoch 836/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2958 - mse: 16.8310\n",
      "Epoch 837/1000\n",
      "11/11 [==============================] - ETA: 0s - loss: 19.2422 - mse: 18.776 - 0s 635us/step - loss: 17.2944 - mse: 16.8301\n",
      "Epoch 838/1000\n",
      "11/11 [==============================] - 0s 634us/step - loss: 17.2930 - mse: 16.8291\n",
      "Epoch 839/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2916 - mse: 16.8282\n",
      "Epoch 840/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2902 - mse: 16.8272\n",
      "Epoch 841/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2888 - mse: 16.8263\n",
      "Epoch 842/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2875 - mse: 16.8254\n",
      "Epoch 843/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2861 - mse: 16.8245\n",
      "Epoch 844/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2848 - mse: 16.8236\n",
      "Epoch 845/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2835 - mse: 16.8227\n",
      "Epoch 846/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2822 - mse: 16.8218\n",
      "Epoch 847/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2809 - mse: 16.8210\n",
      "Epoch 848/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2797 - mse: 16.8201\n",
      "Epoch 849/1000\n",
      "11/11 [==============================] - 0s 907us/step - loss: 17.2784 - mse: 16.8193\n",
      "Epoch 850/1000\n",
      "11/11 [==============================] - 0s 634us/step - loss: 17.2772 - mse: 16.8185\n",
      "Epoch 851/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 17.2759 - mse: 16.8177\n",
      "Epoch 852/1000\n",
      "11/11 [==============================] - 0s 816us/step - loss: 17.2747 - mse: 16.8169\n",
      "Epoch 853/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2735 - mse: 16.8161\n",
      "Epoch 854/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.2723 - mse: 16.8153\n",
      "Epoch 855/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.2712 - mse: 16.8145\n",
      "Epoch 856/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.2700 - mse: 16.8137\n",
      "Epoch 857/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.2689 - mse: 16.8130\n",
      "Epoch 858/1000\n",
      "11/11 [==============================] - 0s 907us/step - loss: 17.2677 - mse: 16.8122\n",
      "Epoch 859/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.2666 - mse: 16.8115\n",
      "Epoch 860/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.2655 - mse: 16.8108\n",
      "Epoch 861/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.2644 - mse: 16.8100\n",
      "Epoch 862/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.2633 - mse: 16.8093\n",
      "Epoch 863/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.2622 - mse: 16.8086\n",
      "Epoch 864/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2612 - mse: 16.8079\n",
      "Epoch 865/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2601 - mse: 16.8072\n",
      "Epoch 866/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2591 - mse: 16.8066\n",
      "Epoch 867/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2581 - mse: 16.8059\n",
      "Epoch 868/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2570 - mse: 16.8052\n",
      "Epoch 869/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2560 - mse: 16.8046\n",
      "Epoch 870/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2550 - mse: 16.8040\n",
      "Epoch 871/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2541 - mse: 16.8033\n",
      "Epoch 872/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2531 - mse: 16.8027\n",
      "Epoch 873/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2521 - mse: 16.8021\n",
      "Epoch 874/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2512 - mse: 16.8015\n",
      "Epoch 875/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 17.2503 - mse: 16.8009\n",
      "Epoch 876/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2493 - mse: 16.8003\n",
      "Epoch 877/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2484 - mse: 16.7997\n",
      "Epoch 878/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2475 - mse: 16.7991\n",
      "Epoch 879/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2466 - mse: 16.7986\n",
      "Epoch 880/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2457 - mse: 16.7980\n",
      "Epoch 881/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2449 - mse: 16.7975\n",
      "Epoch 882/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2440 - mse: 16.7969\n",
      "Epoch 883/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2431 - mse: 16.7964\n",
      "Epoch 884/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2423 - mse: 16.7959\n",
      "Epoch 885/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2415 - mse: 16.7953\n",
      "Epoch 886/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2406 - mse: 16.7948\n",
      "Epoch 887/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2398 - mse: 16.7943\n",
      "Epoch 888/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2390 - mse: 16.7938\n",
      "Epoch 889/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2382 - mse: 16.7933\n",
      "Epoch 890/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2375 - mse: 16.7928\n",
      "Epoch 891/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2367 - mse: 16.7924\n",
      "Epoch 892/1000\n",
      "11/11 [==============================] - 0s 634us/step - loss: 17.2359 - mse: 16.7919\n",
      "Epoch 893/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2352 - mse: 16.7914\n",
      "Epoch 894/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2344 - mse: 16.7910\n",
      "Epoch 895/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2337 - mse: 16.7905\n",
      "Epoch 896/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2329 - mse: 16.7901\n",
      "Epoch 897/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2322 - mse: 16.7896\n",
      "Epoch 898/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2315 - mse: 16.7892\n",
      "Epoch 899/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2308 - mse: 16.7888\n",
      "Epoch 900/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2301 - mse: 16.7884\n",
      "Epoch 901/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2294 - mse: 16.7880\n",
      "Epoch 902/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.2288 - mse: 16.7876\n",
      "Epoch 903/1000\n",
      "11/11 [==============================] - 0s 725us/step - loss: 17.2281 - mse: 16.7871\n",
      "Epoch 904/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2274 - mse: 16.7868\n",
      "Epoch 905/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2268 - mse: 16.7864\n",
      "Epoch 906/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2261 - mse: 16.7860\n",
      "Epoch 907/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2255 - mse: 16.7856\n",
      "Epoch 908/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2249 - mse: 16.7852\n",
      "Epoch 909/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2242 - mse: 16.7849\n",
      "Epoch 910/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2236 - mse: 16.7845\n",
      "Epoch 911/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2230 - mse: 16.7842\n",
      "Epoch 912/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2224 - mse: 16.7838\n",
      "Epoch 913/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2218 - mse: 16.7835\n",
      "Epoch 914/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2213 - mse: 16.7831\n",
      "Epoch 915/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2207 - mse: 16.7828\n",
      "Epoch 916/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2201 - mse: 16.7825\n",
      "Epoch 917/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2196 - mse: 16.7821\n",
      "Epoch 918/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2190 - mse: 16.7818\n",
      "Epoch 919/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2185 - mse: 16.7815\n",
      "Epoch 920/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2179 - mse: 16.7812\n",
      "Epoch 921/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 17.2174 - mse: 16.7809\n",
      "Epoch 922/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 17.2169 - mse: 16.7806\n",
      "Epoch 923/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2163 - mse: 16.7803\n",
      "Epoch 924/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2158 - mse: 16.7800\n",
      "Epoch 925/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 17.2153 - mse: 16.7797\n",
      "Epoch 926/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2148 - mse: 16.7795\n",
      "Epoch 927/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2143 - mse: 16.7792\n",
      "Epoch 928/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2138 - mse: 16.7789\n",
      "Epoch 929/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2134 - mse: 16.7787\n",
      "Epoch 930/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2129 - mse: 16.7784\n",
      "Epoch 931/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2124 - mse: 16.7781\n",
      "Epoch 932/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 17.2120 - mse: 16.7779\n",
      "Epoch 933/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2115 - mse: 16.7776\n",
      "Epoch 934/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2111 - mse: 16.7774\n",
      "Epoch 935/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2106 - mse: 16.7772\n",
      "Epoch 936/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2102 - mse: 16.7769\n",
      "Epoch 937/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2097 - mse: 16.7767\n",
      "Epoch 938/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2093 - mse: 16.7765\n",
      "Epoch 939/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2089 - mse: 16.7762\n",
      "Epoch 940/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2085 - mse: 16.7760\n",
      "Epoch 941/1000\n",
      "11/11 [==============================] - 0s 634us/step - loss: 17.2081 - mse: 16.7758\n",
      "Epoch 942/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2077 - mse: 16.7756\n",
      "Epoch 943/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2073 - mse: 16.7754\n",
      "Epoch 944/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2069 - mse: 16.7752\n",
      "Epoch 945/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2065 - mse: 16.7750\n",
      "Epoch 946/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2061 - mse: 16.7748\n",
      "Epoch 947/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2057 - mse: 16.7746\n",
      "Epoch 948/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2054 - mse: 16.7744\n",
      "Epoch 949/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2050 - mse: 16.7742\n",
      "Epoch 950/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2046 - mse: 16.7740\n",
      "Epoch 951/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2043 - mse: 16.7738\n",
      "Epoch 952/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2039 - mse: 16.7737\n",
      "Epoch 953/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2036 - mse: 16.7735\n",
      "Epoch 954/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2032 - mse: 16.7733\n",
      "Epoch 955/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2029 - mse: 16.7732\n",
      "Epoch 956/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2026 - mse: 16.7730\n",
      "Epoch 957/1000\n",
      "11/11 [==============================] - 0s 634us/step - loss: 17.2022 - mse: 16.7728\n",
      "Epoch 958/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2019 - mse: 16.7727\n",
      "Epoch 959/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 17.2016 - mse: 16.7725\n",
      "Epoch 960/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2013 - mse: 16.7724\n",
      "Epoch 961/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2010 - mse: 16.7722\n",
      "Epoch 962/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2007 - mse: 16.7721\n",
      "Epoch 963/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.2004 - mse: 16.7719\n",
      "Epoch 964/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.2001 - mse: 16.7718\n",
      "Epoch 965/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1998 - mse: 16.7716\n",
      "Epoch 966/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1995 - mse: 16.7715\n",
      "Epoch 967/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1992 - mse: 16.7714\n",
      "Epoch 968/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1989 - mse: 16.7712\n",
      "Epoch 969/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1986 - mse: 16.7711\n",
      "Epoch 970/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1984 - mse: 16.7710\n",
      "Epoch 971/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1981 - mse: 16.7709\n",
      "Epoch 972/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1978 - mse: 16.7707\n",
      "Epoch 973/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1976 - mse: 16.7706\n",
      "Epoch 974/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1973 - mse: 16.7705\n",
      "Epoch 975/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1970 - mse: 16.7704\n",
      "Epoch 976/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1968 - mse: 16.7703\n",
      "Epoch 977/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1965 - mse: 16.7702\n",
      "Epoch 978/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 17.1963 - mse: 16.7701\n",
      "Epoch 979/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1961 - mse: 16.7700\n",
      "Epoch 980/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1958 - mse: 16.7699\n",
      "Epoch 981/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1956 - mse: 16.7698\n",
      "Epoch 982/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1954 - mse: 16.7697\n",
      "Epoch 983/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1951 - mse: 16.7696\n",
      "Epoch 984/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 17.1949 - mse: 16.7695\n",
      "Epoch 985/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1947 - mse: 16.7694\n",
      "Epoch 986/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1945 - mse: 16.7693\n",
      "Epoch 987/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1943 - mse: 16.7692\n",
      "Epoch 988/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 17.1940 - mse: 16.7691\n",
      "Epoch 989/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 17.1938 - mse: 16.7691\n",
      "Epoch 990/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.1936 - mse: 16.7690\n",
      "Epoch 991/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.1934 - mse: 16.7689\n",
      "Epoch 992/1000\n",
      "11/11 [==============================] - 0s 907us/step - loss: 17.1932 - mse: 16.7688\n",
      "Epoch 993/1000\n",
      "11/11 [==============================] - 0s 998us/step - loss: 17.1930 - mse: 16.7687\n",
      "Epoch 994/1000\n",
      "11/11 [==============================] - 0s 600us/step - loss: 17.1928 - mse: 16.7687\n",
      "Epoch 995/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.1926 - mse: 16.7686\n",
      "Epoch 996/1000\n",
      "11/11 [==============================] - 0s 635us/step - loss: 17.1925 - mse: 16.7685\n",
      "Epoch 997/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 17.1923 - mse: 16.7685\n",
      "Epoch 998/1000\n",
      "11/11 [==============================] - 0s 544us/step - loss: 17.1921 - mse: 16.7684\n",
      "Epoch 999/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 17.1919 - mse: 16.7683\n",
      "Epoch 1000/1000\n",
      "11/11 [==============================] - 0s 453us/step - loss: 17.1917 - mse: 16.7683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2158a4c0a90>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=x_train.shape[1], activation='linear', kernel_regularizer = make_reg_function(200.0)))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "model.fit(x_train,y_train, batch_size=int(x_train.shape[0]/10), epochs=1000, shuffle=False, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.1542807 ],\n",
       "        [0.13888358],\n",
       "        [0.12113725],\n",
       "        [0.03677516],\n",
       "        [0.1721866 ],\n",
       "        [0.07985572]], dtype=float32),\n",
       " array([-5.904023], dtype=float32)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the coefs\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 384us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.04423177310637"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate MSE\n",
    "y_pred = model.predict(x_test,verbose=1)\n",
    "mean_squared_error(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is not bad.\n",
    " - What would the MSE of the best CONSTANT guess for `y`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.834741308052283"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "y_pred = np.array([y_train.mean()]*len(y_test))\n",
    "mean_squared_error(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "We have looked at the cases where wt_factor = 20 and wt_factor = 200 respectively. As the wt_factor  →+∞ , the weights tend to be 0, the constant approaches the mean of y_train, which is the best CONSTANT guess for y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
