{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Multiclass (30 %)\n",
    "### So far we have largely focused on binary classification, where the input is a document and the output is a yes or a no (or probability of yes). \n",
    "\n",
    "### In fact, more complex tasks exist where the input is a document and the output can be multiple (more than two) classes. \n",
    "\n",
    "## In this problem we'll investigate two so-called multiclass problems\n",
    "### Multiclass: an observation is assigned inclusion in ONE of a N $N>2$ categories\n",
    " - ### E.g. is this sentence positive, negative, or neutral sentiment\n",
    " - ### E.g. is this email spam or not spam\n",
    "\n",
    "\n",
    "### Multiclass-multilabel: an observation can belong to more than one of $N>=2$ categories\n",
    " - ### E.g. is this document about `{sports, current events, Steph Curry}` ( a document can be about more than one)\n",
    " - ### E.g. is this blood sample A, B, O, $+$, $-$ (blood can be `A+` or `A-`)\n",
    "\n",
    "## We will study the metrics we can use to evaluate these classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will start with multiclass by studying the 20 newsgroups data\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# Part 0: get the data\n",
    " - ### use the builtin function `from sklearn.datasets import fetch_20newsgroups`\n",
    " - ### NB: look at the docs and use the `remove` kwarg in order to get cleaned data\n",
    "\n",
    "## TODO\n",
    " - ## fetch the data separately for the train and test data\n",
    " - ## How many classes are present? \n",
    " - ## What is the most common class- please give the name and not the number.\n",
    " - ## What is the accuracy of the best constant guess in the train set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(...\n",
    "data_test = fetch_20newsgroups(...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_counts = ...\n",
    "print('found {} classes'.format(val_counts.shape[0]))\n",
    "most_common_class = ...\n",
    "print('most common class: {}'.format(most_common_class))\n",
    "\n",
    "dummy_acc = accuracy_score(...\n",
    "print('constant guess acc: {:.3f}'.format(dummy_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: fit a model\n",
    "## As we saw with mnist, logistic regression is capable of fitting multi-class data.\n",
    " - ## Encode the text with as a bag of words and fit logistic regression to the data\n",
    " - ## Calcuate the out of sample accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo\n",
    "# 1. make a count vectorizer with max_features=20000\n",
    "# 2. fit it\n",
    "# 3. transform the train and test data into number\n",
    "vec = ..\n",
    "\n",
    "# your code here\n",
    "xtr = ... # train data\n",
    "xte = ... # test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. fit logistic regression\n",
    "# 2. compute accuracy score\n",
    "\n",
    "# your code here\n",
    "accuracy_score(..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Evaluate metrics\n",
    "### As we have seen previously, while accuracy is useful, it does not always capture all the behavior we want in a metric.\n",
    "\n",
    "### Here we will extend the concept of f1 score to the multiclass setting. There are several ways to do this\n",
    " - report a different f1 score for every class (no averaging)\n",
    " - report the mean f1 score over all classes\n",
    " - report a weighted f1 score weighted by class prevelance. \n",
    "\n",
    "### For each of these three types of f1\n",
    " - calculate the score(s) without the help of scikit learn\n",
    " - compare it to the corresponding f1 score evaluated with scikit-learn (NB you'll need to read the docs for `f1_score`. \n",
    " - Write down the pros and cons for this method of calculating multiclass f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr.predict(xte)\n",
    "\n",
    "f1s = []\n",
    "for label_index, label_name in enumerate(data_train.target_names):\n",
    "    # calculate the f1 score of one (label_index) vs rest\n",
    "    # your code here...\n",
    "    f1s.append(...\n",
    "\n",
    "for label_name, fs in zip(data_train.target_names, f1s):\n",
    "    print('fscore for {} \\t = {:.3f}'.format(label_name, fs))\n",
    "\n",
    "print('\\n\\n')\n",
    "# compare to sklearn\n",
    "success = (f1s == f1_score(data_test.target, preds, average=None)).all()\n",
    "if success:\n",
    "    print('sklearn builtin matches results')\n",
    "else:\n",
    "    print('scores do not match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pros are ...\n",
    "# The cons are ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the macro f1 (the mean of the f1s for each class)\n",
    "f1_macro = ... # calculate without sklearn\n",
    "f1_macro_sk = f1_score(... # calculate with sklearn\n",
    "assert(f1_macro == f1_macro_sk)\n",
    "print('macro f1: {} \\t sklearn macro f1 {}'.format(\n",
    "    f1_macro, \n",
    "    f1_macro_sk\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pros are ...\n",
    "# The cons are ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now weighted by class prevalence\n",
    "# TODO:\n",
    "#  - calculate the frequency of each class\n",
    "#  - take a weighted average of the f1s, weighted by these weights\n",
    "#  - compare to sklearn\n",
    "wts = ...\n",
    "weighted_f1 = # without sklearn\n",
    "weighted_f1_sk = f1_score(... # with sklearn\n",
    "\n",
    "print('weighted f1 {} \\t sklearn weighted f1 {}'.format(weighted_f1, weighted_f1_sk))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pros are ...\n",
    "# The cons are ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Confusion Matrix\n",
    "## The confusion matrix is a handy way to understand errors in classification problems.  It is a 2-D grid of what values were predicted and what the actual values were. \n",
    "\n",
    "See [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) in the sklearn docs. \n",
    "\n",
    "## Create a confusion matrix for the 20-newsgroups dataset and comment on the most common failure modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# your code here\n",
    "# NB: it's handy to call `pd.DataFrame` on the confusion matrix to print it out nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Multiclass Multilabel Problems (20 %)\n",
    "### In this problem we'll examine academic articles from the [arXiv](www.arxiv.org).\n",
    "### Authors who submit articles can attach one or more categories to the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Load the data\n",
    "## TODO\n",
    " - ### load the data\n",
    " - ### compute all of the unique categories in the train data\n",
    " - ### What are the 10 most common categories which occur together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9923 2481\n"
     ]
    }
   ],
   "source": [
    "with open('./data/arXiv/arxiv-qfin-train.json') as fi:\n",
    "    data_train = json.load(fi)\n",
    "\n",
    "with open('./data/arXiv/arxiv-qfin-test.json') as fi:\n",
    "    data_test = json.load(fi)\n",
    "\n",
    "    \n",
    "print(len(data_train), len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Identifying long-term precursors of financial market crashes using\\n  correlation patterns',\n",
       " 'summary': 'The study of the critical dynamics in complex systems is always interesting\\nyet challenging. Here, we choose financial market as an example of a complex\\nsystem, and do a comparative analyses of two stock markets - the S&P 500 (USA)\\nand Nikkei 225 (JPN). Our analyses are based on the evolution of\\ncrosscorrelation structure patterns of short time-epochs for a 32-year period\\n(1985-2016). We identify \"market states\" as clusters of similar correlation\\nstructures, which occur more frequently than by pure chance (randomness). The\\ndynamical transitions between the correlation structures reflect the evolution\\nof the market states. Power mapping method from the random matrix theory is\\nused to suppress the noise on correlation patterns, and an adaptation of the\\nintra-cluster distance method is used to obtain the \"optimum\" number of market\\nstates. We find that the USA is characterized by four market states and JPN by\\nfive. We further analyze the co-occurrence of paired market states; the\\nprobability of remaining in the same state is much higher than the transition\\nto a different state. The transitions to other states mainly occur among the\\nimmediately adjacent states, with a few rare intermittent transitions to the\\nremote states. The state adjacent to the critical state (market crash) may\\nserve as an indicator or a \"precursor\" for the critical state and this novel\\nmethod of identifying the long-term precursors may be very helpful for\\nconstructing the early warning system in financial markets, as well as in other\\ncomplex systems.',\n",
       " 'category': ['q-fin.ST'],\n",
       " 'id': 'http://arxiv.org/abs/1809.00885v2',\n",
       " 'link': 'http://dx.doi.org/10.1088/1367-2630/aae7e0'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'econ.EM', 'q-fin.TR', 'q-fin.RM', 'q-fin.GN', 'q-fin.CP', 'q-fin.ST', 'q-fin.PR', 'q-fin.MF', 'math.PR', 'q-fin.PM', 'econ.GN', 'q-fin.EC'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the unique categories here\n",
    "set_cat = set()\n",
    "for val in data_train:\n",
    "    for cat in val['category']:\n",
    "        set_cat.add(cat)\n",
    "        \n",
    "print(set_cat)\n",
    "len(set_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"['econ.GN', 'q-fin.EC']\": 795,\n",
       " \"['math.PR', 'q-fin.PR']\": 281,\n",
       " \"['math.PR', 'q-fin.MF']\": 160,\n",
       " \"['q-fin.CP', 'q-fin.PR']\": 130,\n",
       " \"['q-fin.ST', 'q-fin.TR']\": 117,\n",
       " \"['math.PR', 'q-fin.RM']\": 113,\n",
       " \"['math.PR', 'q-fin.CP']\": 112,\n",
       " \"['q-fin.GN', 'q-fin.ST']\": 98,\n",
       " \"['q-fin.RM', 'q-fin.ST']\": 82,\n",
       " \"['q-fin.PM', 'q-fin.RM']\": 72}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the co-occuring categories here\n",
    "# Hint:\n",
    "#  - loop through all the train articles\n",
    "#  - loop through all the pairs of categories\n",
    "#  - keep track of the counts of every pair\n",
    "dict_cat = {}\n",
    "for val in data_train:\n",
    "    if len(val['category'])>1:\n",
    "        if str(sorted(val['category'])) in dict_cat:\n",
    "            dict_cat[str(sorted(val['category']))] += 1\n",
    "        else:\n",
    "            dict_cat[str(sorted(val['category']))] = 1\n",
    "\n",
    "\n",
    "sorted_dict = {k: v for k, v in sorted(dict_cat.items(), key=lambda item: item[1], reverse=True)}\n",
    "dict(list(sorted_dict.items())[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Encode the data\n",
    "\n",
    "## We will encode the title of each article using a bag of words (`CountVectorizer`). Try limiting the features to about 20k. \n",
    "\n",
    "## Encoding targets is as bit trickier for multilabel problems. In this case we want our target to be a matrix of $N_{samples} x N_{categories}$ but each row does not have to sum to 1.\n",
    " - ## NB: scikit learn as a `MultiLabelBinarizer` to help here. \n",
    "\n",
    "# $ \\\\ $\n",
    "## TODO\n",
    " - ## fit a `CountVectorizer` on the titles to create `x_train` and `x_test`\n",
    " - ## create `y_train` and `y_test` to be matrices of $N_{samples} x N_{categories}$ with all 0s and 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train_summ = [item['summary'] for item in data_train]\n",
    "d_test_summ = [item['summary'] for item in data_test]\n",
    "d_train_cat = [sorted(item['category']) for item in data_train]\n",
    "d_test_cat = [sorted(item['category']) for item in data_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['q-fin.CP'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.CP', 'q-fin.MF', 'q-fin.ST'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.TR'],\n",
       " ['math.PR', 'q-fin.RM'],\n",
       " [],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.MF'],\n",
       " ['math.PR', 'q-fin.PM', 'q-fin.RM'],\n",
       " [],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.PR'],\n",
       " ['math.PR', 'q-fin.PM'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.EC'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.MF', 'q-fin.TR'],\n",
       " ['math.PR', 'q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.PR'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['q-fin.GN'],\n",
       " ['math.PR', 'q-fin.MF', 'q-fin.RM'],\n",
       " ['q-fin.TR'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.GN', 'q-fin.ST'],\n",
       " ['q-fin.CP', 'q-fin.ST'],\n",
       " [],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.TR'],\n",
       " ['econ.EM', 'q-fin.PM', 'q-fin.RM'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.PM'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.GN', 'q-fin.ST'],\n",
       " ['q-fin.PM', 'q-fin.TR'],\n",
       " ['q-fin.GN', 'q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['math.PR', 'q-fin.GN', 'q-fin.RM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.PM'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['econ.EM', 'econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.GN', 'q-fin.TR'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['math.PR', 'q-fin.GN'],\n",
       " ['q-fin.CP', 'q-fin.PR'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.GN', 'q-fin.ST'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.EC', 'q-fin.ST'],\n",
       " ['econ.EM', 'q-fin.ST'],\n",
       " ['q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['math.PR', 'q-fin.MF', 'q-fin.PM'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.PR'],\n",
       " ['econ.GN', 'q-fin.EC', 'q-fin.GN'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.GN', 'q-fin.EC', 'q-fin.MF'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.PM'],\n",
       " ['math.PR', 'q-fin.RM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.PR'],\n",
       " ['econ.EM'],\n",
       " ['math.PR', 'q-fin.MF', 'q-fin.PR'],\n",
       " ['q-fin.TR'],\n",
       " [],\n",
       " ['q-fin.MF'],\n",
       " [],\n",
       " ['econ.GN', 'q-fin.EC', 'q-fin.GN', 'q-fin.TR'],\n",
       " ['math.PR', 'q-fin.RM'],\n",
       " ['econ.EM'],\n",
       " ['math.PR', 'q-fin.GN'],\n",
       " ['q-fin.PR', 'q-fin.RM'],\n",
       " ['q-fin.ST'],\n",
       " [],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.GN'],\n",
       " [],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.GN', 'q-fin.ST'],\n",
       " ['q-fin.PM'],\n",
       " [],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.PR'],\n",
       " ['math.PR', 'q-fin.MF', 'q-fin.RM'],\n",
       " ['q-fin.EC'],\n",
       " ['q-fin.EC'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.RM'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.PM', 'q-fin.RM'],\n",
       " ['q-fin.GN', 'q-fin.ST'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.TR'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.PR', 'q-fin.RM'],\n",
       " ['econ.EM'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.PR'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['math.PR', 'q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.PR'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.MF', 'q-fin.PR'],\n",
       " ['q-fin.GN'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['q-fin.GN'],\n",
       " [],\n",
       " ['q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.EC'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.RM', 'q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.CP', 'q-fin.TR'],\n",
       " ['q-fin.EC'],\n",
       " ['q-fin.ST', 'q-fin.TR'],\n",
       " ['q-fin.EC'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['q-fin.GN', 'q-fin.RM'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.ST', 'q-fin.TR'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.PM', 'q-fin.RM'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.TR'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.TR'],\n",
       " ['econ.EM', 'q-fin.GN'],\n",
       " ['q-fin.PM', 'q-fin.RM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.GN'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.MF', 'q-fin.PR'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.EC', 'q-fin.GN'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.CP', 'q-fin.TR'],\n",
       " ['q-fin.GN', 'q-fin.PM'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.CP', 'q-fin.PR'],\n",
       " ['math.PR', 'q-fin.CP'],\n",
       " [],\n",
       " ['econ.EM'],\n",
       " ['q-fin.GN', 'q-fin.PR'],\n",
       " ['q-fin.PR'],\n",
       " ['econ.EM'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.PR'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['math.PR', 'q-fin.CP'],\n",
       " ['q-fin.GN'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['q-fin.ST'],\n",
       " [],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.CP', 'q-fin.ST'],\n",
       " ['q-fin.MF', 'q-fin.PR'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.RM', 'q-fin.ST'],\n",
       " ['q-fin.CP', 'q-fin.TR'],\n",
       " ['q-fin.PM', 'q-fin.RM'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.GN', 'q-fin.TR'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['q-fin.ST', 'q-fin.TR'],\n",
       " ['econ.GN', 'math.PR', 'q-fin.EC'],\n",
       " ['q-fin.TR'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.ST', 'q-fin.TR'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.CP', 'q-fin.ST'],\n",
       " ['math.PR', 'q-fin.CP'],\n",
       " ['q-fin.GN', 'q-fin.ST'],\n",
       " ['q-fin.MF', 'q-fin.PR'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.CP'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.GN', 'q-fin.ST'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.ST'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['econ.EM', 'q-fin.GN'],\n",
       " ['math.PR', 'q-fin.RM'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['math.PR', 'q-fin.CP', 'q-fin.PR'],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.CP'],\n",
       " ['math.PR', 'q-fin.MF', 'q-fin.PR'],\n",
       " ['q-fin.MF', 'q-fin.RM'],\n",
       " ['q-fin.EC', 'q-fin.GN', 'q-fin.ST'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.MF', 'q-fin.ST'],\n",
       " ['q-fin.PM', 'q-fin.PR'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.CP', 'q-fin.PR'],\n",
       " ['q-fin.CP', 'q-fin.TR'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.PM', 'q-fin.RM'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.MF'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['econ.EM', 'q-fin.GN'],\n",
       " ['math.PR', 'q-fin.CP'],\n",
       " ['q-fin.MF', 'q-fin.TR'],\n",
       " ['econ.EM'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.ST'],\n",
       " ['math.PR', 'q-fin.GN'],\n",
       " ['q-fin.GN'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.ST'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.CP', 'q-fin.PR', 'q-fin.RM'],\n",
       " ['q-fin.EC'],\n",
       " ['q-fin.RM', 'q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.GN', 'q-fin.EC', 'q-fin.GN'],\n",
       " ['q-fin.ST'],\n",
       " [],\n",
       " ['math.PR', 'q-fin.PM'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.MF'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.PM', 'q-fin.RM'],\n",
       " ['math.PR', 'q-fin.RM'],\n",
       " ['q-fin.ST'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.ST'],\n",
       " ['math.PR', 'q-fin.GN', 'q-fin.RM'],\n",
       " ['econ.EM'],\n",
       " [],\n",
       " ['q-fin.PM', 'q-fin.RM'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.EC'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['q-fin.EC'],\n",
       " ['q-fin.ST', 'q-fin.TR'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.PR'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['math.PR', 'q-fin.CP', 'q-fin.TR'],\n",
       " ['q-fin.EC'],\n",
       " ['q-fin.EC', 'q-fin.GN'],\n",
       " ['econ.GN', 'q-fin.EC', 'q-fin.GN'],\n",
       " ['q-fin.PM'],\n",
       " ['math.PR', 'q-fin.PM'],\n",
       " ['q-fin.PM'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.MF', 'q-fin.TR'],\n",
       " ['q-fin.GN'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.GN', 'q-fin.EC', 'q-fin.GN'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.EM', 'q-fin.ST'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.PM'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.GN'],\n",
       " [],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.EC', 'q-fin.GN'],\n",
       " ['q-fin.CP', 'q-fin.PR'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.PM'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.ST'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST', 'q-fin.TR'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['math.PR', 'q-fin.GN'],\n",
       " ['q-fin.MF', 'q-fin.PM'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.EM', 'econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.PR'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.RM'],\n",
       " [],\n",
       " [],\n",
       " ['q-fin.RM'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['math.PR'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.ST'],\n",
       " [],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.CP', 'q-fin.PR'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.MF'],\n",
       " ['math.PR', 'q-fin.CP'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.PM', 'q-fin.RM'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.EM'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST', 'q-fin.TR'],\n",
       " ['q-fin.GN', 'q-fin.PR'],\n",
       " ['q-fin.PM', 'q-fin.ST'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.PR'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.GN', 'q-fin.CP', 'q-fin.EC', 'q-fin.GN', 'q-fin.PM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST', 'q-fin.TR'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.RM'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.CP', 'q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.MF'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.GN', 'q-fin.CP', 'q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.PM', 'q-fin.RM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.RM', 'q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.GN'],\n",
       " ['math.PR', 'q-fin.RM'],\n",
       " ['econ.GN', 'math.PR', 'q-fin.EC'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.CP', 'q-fin.RM'],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.EC'],\n",
       " ['q-fin.MF', 'q-fin.RM'],\n",
       " ['math.PR', 'q-fin.MF', 'q-fin.PM', 'q-fin.TR'],\n",
       " ['econ.EM', 'econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.CP', 'q-fin.PM', 'q-fin.TR'],\n",
       " ['q-fin.PM'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.EC', 'q-fin.TR'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.EC', 'q-fin.RM'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.PR', 'q-fin.ST'],\n",
       " ['math.PR', 'q-fin.PM'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['q-fin.PR'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.ST', 'q-fin.TR'],\n",
       " ['q-fin.PR', 'q-fin.ST'],\n",
       " ['q-fin.CP', 'q-fin.PM'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.GN', 'q-fin.ST'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['math.PR', 'q-fin.RM'],\n",
       " ['econ.EM'],\n",
       " ['math.PR', 'q-fin.ST'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['q-fin.TR'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.PR', 'q-fin.ST'],\n",
       " ['math.PR', 'q-fin.CP', 'q-fin.PR', 'q-fin.RM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.PM', 'q-fin.PR'],\n",
       " ['q-fin.EC', 'q-fin.MF', 'q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.TR'],\n",
       " ['math.PR', 'q-fin.CP'],\n",
       " ['q-fin.RM'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.GN'],\n",
       " ['math.PR', 'q-fin.RM'],\n",
       " ['q-fin.ST'],\n",
       " [],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.TR'],\n",
       " ['econ.GN', 'q-fin.EC', 'q-fin.GN'],\n",
       " ['econ.EM'],\n",
       " ['econ.EM', 'q-fin.GN'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.RM'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.CP', 'q-fin.MF'],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.ST', 'q-fin.TR'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.CP', 'q-fin.MF', 'q-fin.PR', 'q-fin.RM'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['q-fin.GN', 'q-fin.PR'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.EM', 'q-fin.GN'],\n",
       " ['q-fin.CP', 'q-fin.RM'],\n",
       " ['q-fin.CP', 'q-fin.PR'],\n",
       " ['q-fin.ST', 'q-fin.TR'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.TR'],\n",
       " ['econ.GN', 'q-fin.EC', 'q-fin.MF'],\n",
       " ['q-fin.MF', 'q-fin.RM'],\n",
       " ['q-fin.PR'],\n",
       " ['math.PR', 'q-fin.RM'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.EC', 'q-fin.GN'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.MF'],\n",
       " ['econ.EM'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.PM', 'q-fin.RM'],\n",
       " ['q-fin.PR', 'q-fin.RM'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.MF', 'q-fin.RM'],\n",
       " ['q-fin.MF', 'q-fin.RM'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.PM'],\n",
       " ['math.PR', 'q-fin.CP'],\n",
       " ['q-fin.CP', 'q-fin.PM', 'q-fin.RM'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.CP', 'q-fin.PR'],\n",
       " ['q-fin.GN', 'q-fin.ST'],\n",
       " ['q-fin.RM'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.TR'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.CP', 'q-fin.MF'],\n",
       " ['q-fin.PM', 'q-fin.RM'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.TR'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.CP'],\n",
       " ['econ.EM'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.CP', 'q-fin.TR'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.CP', 'q-fin.GN', 'q-fin.PM', 'q-fin.RM'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.CP', 'q-fin.PM'],\n",
       " ['q-fin.MF', 'q-fin.PR'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.PM', 'q-fin.RM'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.EC'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.GN', 'q-fin.PR'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.EC'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['math.PR', 'q-fin.TR'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['q-fin.GN'],\n",
       " [],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.CP', 'q-fin.PM', 'q-fin.PR', 'q-fin.RM'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.CP', 'q-fin.PM', 'q-fin.PR'],\n",
       " [],\n",
       " ['q-fin.MF', 'q-fin.PR'],\n",
       " ['q-fin.ST', 'q-fin.TR'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST', 'q-fin.TR'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.CP', 'q-fin.PR'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.CP', 'q-fin.PM', 'q-fin.PR'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.PM', 'q-fin.ST'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.EC'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.ST'],\n",
       " ['math.PR', 'q-fin.TR'],\n",
       " ['q-fin.PM'],\n",
       " ['econ.EM'],\n",
       " ['math.PR'],\n",
       " ['econ.EM', 'econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.MF', 'q-fin.PR'],\n",
       " [],\n",
       " ['econ.EM', 'q-fin.ST'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.EC', 'q-fin.GN'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.EM'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.PM', 'q-fin.RM'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.GN', 'q-fin.PR', 'q-fin.RM'],\n",
       " ['math.PR', 'q-fin.GN'],\n",
       " ['q-fin.PR', 'q-fin.RM'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.EC', 'q-fin.GN'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.ST', 'q-fin.TR'],\n",
       " ['q-fin.TR'],\n",
       " ['econ.EM'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST', 'q-fin.TR'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.CP', 'q-fin.ST'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['q-fin.GN'],\n",
       " [],\n",
       " ['q-fin.CP', 'q-fin.MF'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.GN', 'q-fin.EC', 'q-fin.GN'],\n",
       " ['q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.GN'],\n",
       " ['math.PR', 'q-fin.GN', 'q-fin.ST'],\n",
       " ['q-fin.CP', 'q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.GN', 'q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['math.PR', 'q-fin.GN'],\n",
       " ['q-fin.EC', 'q-fin.MF'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.EC', 'q-fin.GN'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.ST'],\n",
       " ['math.PR', 'q-fin.PM', 'q-fin.PR'],\n",
       " ['q-fin.MF', 'q-fin.ST', 'q-fin.TR'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['math.PR'],\n",
       " ['q-fin.CP', 'q-fin.PM', 'q-fin.PR', 'q-fin.ST'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.PM', 'q-fin.RM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.ST'],\n",
       " ['math.PR', 'q-fin.CP', 'q-fin.PR'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.GN'],\n",
       " ['math.PR', 'q-fin.RM'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.CP', 'q-fin.PM'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.EC'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['q-fin.PM', 'q-fin.ST'],\n",
       " ['q-fin.ST', 'q-fin.TR'],\n",
       " ['math.PR', 'q-fin.GN'],\n",
       " ['math.PR'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.EC'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['econ.EM', 'q-fin.ST'],\n",
       " ['math.PR', 'q-fin.PM'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.CP', 'q-fin.GN'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.GN', 'q-fin.ST'],\n",
       " ['q-fin.TR'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.EC'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.CP', 'q-fin.TR'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.TR'],\n",
       " ['econ.EM'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.CP', 'q-fin.PR'],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.CP', 'q-fin.PR'],\n",
       " ['q-fin.EC'],\n",
       " ['math.PR', 'q-fin.ST'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.RM', 'q-fin.ST'],\n",
       " ['q-fin.EC'],\n",
       " ['q-fin.MF'],\n",
       " [],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.PR', 'q-fin.RM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['math.PR', 'q-fin.CP'],\n",
       " ['q-fin.MF'],\n",
       " ['math.PR', 'q-fin.PM'],\n",
       " ['q-fin.EC'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.EM', 'econ.GN', 'q-fin.EC', 'q-fin.GN', 'q-fin.PM', 'q-fin.RM'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.EM'],\n",
       " ['econ.GN', 'q-fin.EC', 'q-fin.GN', 'q-fin.MF'],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.TR'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.GN', 'q-fin.RM'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['q-fin.PR'],\n",
       " ['econ.GN', 'q-fin.EC', 'q-fin.GN', 'q-fin.ST'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.ST'],\n",
       " [],\n",
       " ['q-fin.GN'],\n",
       " ['math.PR', 'q-fin.PR'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.CP', 'q-fin.PR'],\n",
       " ['q-fin.MF', 'q-fin.PM'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.CP', 'q-fin.PR', 'q-fin.RM', 'q-fin.ST'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.CP', 'q-fin.MF'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.EC', 'q-fin.GN'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.GN'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.PR'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.CP'],\n",
       " ['q-fin.RM'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['q-fin.CP'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.CP', 'q-fin.PM'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.RM'],\n",
       " ['math.PR', 'q-fin.RM'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.GN', 'q-fin.ST'],\n",
       " ['q-fin.EC'],\n",
       " ['q-fin.GN', 'q-fin.TR'],\n",
       " ['math.PR', 'q-fin.CP'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.CP', 'q-fin.RM'],\n",
       " ['q-fin.PR'],\n",
       " [],\n",
       " ['math.PR', 'q-fin.ST'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.GN'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.ST'],\n",
       " [],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.RM'],\n",
       " ['q-fin.PM'],\n",
       " ['q-fin.RM', 'q-fin.ST'],\n",
       " ['q-fin.EC'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " [],\n",
       " ['q-fin.PR'],\n",
       " ['math.PR', 'q-fin.CP'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.TR'],\n",
       " ['q-fin.PR', 'q-fin.RM'],\n",
       " ['econ.GN', 'q-fin.EC'],\n",
       " ['q-fin.ST'],\n",
       " ['math.PR', 'q-fin.PR', 'q-fin.RM'],\n",
       " ['q-fin.GN', 'q-fin.ST'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['math.PR', 'q-fin.PM', 'q-fin.TR'],\n",
       " ['q-fin.MF', 'q-fin.PM'],\n",
       " ['q-fin.MF'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['math.PR', 'q-fin.MF'],\n",
       " ['econ.GN', 'q-fin.EC', 'q-fin.GN'],\n",
       " ['q-fin.PR'],\n",
       " ['q-fin.ST', 'q-fin.TR'],\n",
       " ['q-fin.GN', 'q-fin.ST'],\n",
       " ['econ.EM'],\n",
       " ['econ.EM'],\n",
       " ['q-fin.ST'],\n",
       " [],\n",
       " ['q-fin.ST'],\n",
       " ['q-fin.ST'],\n",
       " ['econ.EM', 'q-fin.EC'],\n",
       " ['q-fin.TR'],\n",
       " ['econ.EM'],\n",
       " ['math.PR', 'q-fin.CP', 'q-fin.PM'],\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_train_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(lowercase=False)\n",
    "vec.fit(d_train_summ)\n",
    "x_train = vec.transform(d_train_summ)\n",
    "x_test = vec.transform(d_test_summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9923x28496 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 873196 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9923, 12) (2481, 12)\n",
      "['econ.EM', 'econ.GN', 'math.PR', 'q-fin.CP', 'q-fin.EC', 'q-fin.GN', 'q-fin.MF', 'q-fin.PM', 'q-fin.PR', 'q-fin.RM', 'q-fin.ST', 'q-fin.TR']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<9923x28496 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 873196 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "# your code here\n",
    "mlb.fit(d_train_cat)\n",
    "y_train = mlb.transform(d_train_cat)\n",
    "y_test = mlb.transform(d_test_cat)\n",
    "print(y_train.shape, y_test.shape)\n",
    "print(list(mlb.classes_))\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model the data\n",
    "### While scikit-learn can't handle multilabel data in logistic regression, keras can. \n",
    "### Create and fit a multilabel logistic regression model and fit it. \n",
    "### NB: think hard about the activation function and loss function that are appropriate in this case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import Dense, Softmax, Dropout\n",
    "import keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "doc_input = Input(shape=(x_train.shape[1],))\n",
    "output = Dense(len(mlb.classes_), activation='sigmoid')(doc_input)\n",
    "# your code here\n",
    "# dont forget to compile your model\n",
    "model = Model(doc_input, output)\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='mse', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28496)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                341964    \n",
      "=================================================================\n",
      "Total params: 341,964\n",
      "Trainable params: 341,964\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9923/9923 [==============================] - 2s 250us/step - loss: 0.1460 - accuracy: 0.2032\n",
      "Epoch 2/10\n",
      "9923/9923 [==============================] - 2s 198us/step - loss: 0.1017 - accuracy: 0.3082\n",
      "Epoch 3/10\n",
      "9923/9923 [==============================] - 2s 197us/step - loss: 0.0933 - accuracy: 0.3256\n",
      "Epoch 4/10\n",
      "9923/9923 [==============================] - 2s 201us/step - loss: 0.0854 - accuracy: 0.4180\n",
      "Epoch 5/10\n",
      "9923/9923 [==============================] - 2s 197us/step - loss: 0.0778 - accuracy: 0.4872\n",
      "Epoch 6/10\n",
      "9923/9923 [==============================] - 2s 198us/step - loss: 0.0715 - accuracy: 0.5302\n",
      "Epoch 7/10\n",
      "9923/9923 [==============================] - 2s 201us/step - loss: 0.0663 - accuracy: 0.5528\n",
      "Epoch 8/10\n",
      "9923/9923 [==============================] - 2s 198us/step - loss: 0.0619 - accuracy: 0.56690s - loss: 0.0626 - ac\n",
      "Epoch 9/10\n",
      "9923/9923 [==============================] - 2s 198us/step - loss: 0.0583 - accuracy: 0.5838\n",
      "Epoch 10/10\n",
      "9923/9923 [==============================] - 2s 198us/step - loss: 0.0551 - accuracy: 0.5960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x252c3e71518>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=512, epochs=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(model.history.history).plot(#[['val_loss', 'val_accuracy']]\n",
    "    figsize=(12,7), secondary_y='val_loss'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: f1 score\n",
    "## While modeling is more difficult in the multilabel case, the metrics are, oddly, simpler. Here, we can only compute metrics class by class.\n",
    "\n",
    "### For each class, print the accuracy and f1 score for the class. Comment on the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classecon.EM \t\tacc: 0.944 \tf1: 0.668\n",
      "classecon.GN \t\tacc: 0.942 \tf1: 0.549\n",
      "classmath.PR \t\tacc: 0.907 \tf1: 0.416\n",
      "classq-fin.CP \t\tacc: 0.916 \tf1: 0.376\n",
      "classq-fin.EC \t\tacc: 0.919 \tf1: 0.570\n",
      "classq-fin.GN \t\tacc: 0.863 \tf1: 0.452\n",
      "classq-fin.MF \t\tacc: 0.894 \tf1: 0.070\n",
      "classq-fin.PM \t\tacc: 0.925 \tf1: 0.413\n",
      "classq-fin.PR \t\tacc: 0.913 \tf1: 0.559\n",
      "classq-fin.RM \t\tacc: 0.921 \tf1: 0.467\n",
      "classq-fin.ST \t\tacc: 0.892 \tf1: 0.701\n",
      "classq-fin.TR \t\tacc: 0.936 \tf1: 0.418\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x_test)\n",
    "for i, class_ in enumerate(mlb.classes_):\n",
    "    acc = accuracy_score(y_test[:, i].astype(numpy.float32), preds[:, i].round())\n",
    "    f1 = f1_score(y_test[:, i].astype(numpy.float32), preds[:, i].round())\n",
    "    print('class{} \\t\\tacc: {:.3f} \\tf1: {:.3f}'.format(class_, acc, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9423619508262797"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test[:,1].astype(numpy.float64),preds[:,1].round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 3., 4., 5.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([0.2, 0.6, 3, 4, 5]).round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: New Metrics (30%)\n",
    "## In this problem we'll explore new metrics associated with true positives and false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Load the IMDB data and fit a model\n",
    " - ### Load the imdb data\n",
    " - ### featurize the text using TFIDF\n",
    " - ### Fit logistic regression\n",
    " - ### calculate the in-sample and out of sample accuracy and f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "found 25000 train docs and 25000 test docs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "\n",
    "def load_imdb_data_text(imdb_data_dir, random_seed=1234):\n",
    "    train_dir = os.path.join(imdb_data_dir, 'train')\n",
    "    test_dir = os.path.join(imdb_data_dir, 'test')\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    texts = []\n",
    "    targets = []\n",
    "    for label in ('pos', 'neg'):\n",
    "        data_dir = os.path.join(train_dir, label)\n",
    "        files = glob.glob(os.path.join(data_dir, '*.txt'))\n",
    "        for filename in files:\n",
    "            with open(filename) as fi:\n",
    "                text = fi.read()\n",
    "            target = (label == 'pos')\n",
    "            texts.append(text)\n",
    "            targets.append(target)\n",
    "\n",
    "    train_docs = texts\n",
    "    y_train = np.array(targets)\n",
    "\n",
    "\n",
    "    texts = []\n",
    "    targets = []\n",
    "    for label in ('pos', 'neg'):\n",
    "        data_dir = os.path.join(test_dir, label)\n",
    "        files = glob.glob(os.path.join(data_dir, '*.txt'))\n",
    "        for filename in files:\n",
    "            with open(filename) as fi:\n",
    "                text = fi.read()\n",
    "            target = (label == 'pos')\n",
    "            texts.append(text)\n",
    "            targets.append(target)\n",
    "\n",
    "    test_docs = texts\n",
    "    y_test = np.array(targets)\n",
    "\n",
    "    inds = np.arange(y_train.shape[0])\n",
    "    np.random.shuffle(inds)\n",
    "\n",
    "    train_docs = [train_docs[i] for i in inds]\n",
    "    y_train = y_train[inds]\n",
    "    \n",
    "    return (train_docs, y_train), (test_docs, y_test)\n",
    "\n",
    "(train_docs, y_train), (test_docs, y_test) = load_imdb_data_text('../../data/aclImdb/')\n",
    "print('found {} train docs and {} test docs'.format(len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer...\n",
    "# more code here\n",
    "\n",
    "\n",
    "# more code here\n",
    "preds_train = ...\n",
    "preds_test = ...\n",
    "\n",
    "print('#'*20 + ' in sample ' + '#'*20 )\n",
    "print('\\t\\taccuracy: {:.3f}'.format(accuracy_score(y_train, preds_train)))\n",
    "print('\\t\\tf1: {:.3f}'.format(f1_score(y_train, preds_train)))\n",
    "print('\\n\\n')\n",
    "print('#'*20 + ' out of sample ' + '#'*20 )\n",
    "print('\\t\\taccuracy: {:.3f}'.format(accuracy_score(y_test, preds_test)))\n",
    "print('\\t\\tf1: {:.3f}'.format(f1_score(y_test, preds_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tradeoff between true positives and false positives\n",
    "Typically we take a threshold of 0.5 probability to consider something a positive example.\n",
    "However, as we change this threshold we can change the number of true positives we get.\n",
    " - Example: at a theshold of 0.0001 we will get nearly all of the true positives\n",
    " - Example: at a threshold of 0.999 we will get almost none of the true positives\n",
    "\n",
    "Notice: as we change our threshold and increase the number of true positives we will also increase the number of false positives we pick up.\n",
    "\n",
    "In this part you will create a graph of the false positive rate on the x-axis and the true positive rate on the y-axis. This is often called the `receiver operator characteristic`. Make this curve for the out of sample data below.\n",
    "\n",
    "Note: while you can use the builtin scikit-learn functionality for this, you will __not receive credit__ if you do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# hint: \n",
    "#  - loop through the thresholds\n",
    "#  - calulcate the true positives and false positives\n",
    "\n",
    "# hint: what values for thresholds should you loop through?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(true_pos_rates, index=false_pos_rates).plot(figsize=(12,8), fontsize=16)\n",
    "plt.xlabel('False Pos Rate', fontsize=16)\n",
    "plt.ylabel('True Pos Rate', fontsize=16)\n",
    "plt.title('Receiver Operator Characteristic', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Baseline\n",
    " - What does the receiver operator curve look like for a random guessing classifier? \n",
    " - Make the same plot as above but add the random guessing curve\n",
    " - Add comments about WHY the random guessing curve looks this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.Series(true_pos_rates, index=false_pos_rates, name='logistic regression').plot(\n",
    "    figsize=(12,8), fontsize=16\n",
    ")\n",
    "baseline_series = ... # your code here for the ROC for random guessing\n",
    "baseline_series.to_frame('random guess').plot(ax=ax, fontsize=16)\n",
    "plt.xlabel('False Pos Rate', fontsize=16)\n",
    "plt.ylabel('True Pos Rate', fontsize=16)\n",
    "plt.title('Receiver Operator Characteristic', fontsize=20)\n",
    "plt.legend(fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add comments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Boiling it down to a single number\n",
    " - While the ROC is a useful curve and contains a lot of information, it is useful to distill in down to a single number. Typically, the area under the curve is used. Calculate the area under the curve and add it as the title to your previous plot. \n",
    " - Hint: think about approximations for integrals for finding area under a curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_under = # your code here\n",
    "\n",
    "\n",
    "# repeat the plotting code here\n",
    "\n",
    "plt.title('Area under the curve = {:.3f}'.format(area_under, fontsize=20))\n",
    "plt.legend(fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Check you work and comment on the results\n",
    " - \"There's gotta be a better way!\"\n",
    " - In fact, `scikit-learn` will take care of a lot of the headache here. \n",
    " - `from sklearn.metrics import plot_roc_curve`\n",
    " - read the docs and use this function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve, auc\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few comments:\n",
    " - The area under the ROC has a nice interpretation. It can be thought of as the probability that a randomly chosen positive example has a higher probability than a randomly chosen negative example.\n",
    " - This metric is also nice since it is independent of a threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Examining Coefficients (20%)\n",
    "In class we skipped an important step: we never made sure our models made sense. \n",
    "Logistic regression provides coefficients, which allow us to determine if a model\n",
    "if learning anything reasonable. \n",
    "\n",
    "In this problem, you'll load the imdb data, fit logistic regression and exmamine the coefficients. \n",
    "Print out the largest and smallest (largest negative) coefficients and comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "def load_imdb_data_text(imdb_data_dir, random_seed=1234):\n",
    "    train_dir = os.path.join(imdb_data_dir, 'train')\n",
    "    test_dir = os.path.join(imdb_data_dir, 'test')\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    texts = []\n",
    "    targets = []\n",
    "    for label in ('pos', 'neg'):\n",
    "        data_dir = os.path.join(train_dir, label)\n",
    "        files = glob.glob(os.path.join(data_dir, '*.txt'))\n",
    "        for filename in files:\n",
    "            with open(filename,'r', encoding='UTF-8') as fi:\n",
    "                text = fi.read()\n",
    "            target = (label == 'pos')\n",
    "            texts.append(text)\n",
    "            targets.append(target)\n",
    "\n",
    "    train_docs = texts\n",
    "    y_train = np.array(targets)\n",
    "\n",
    "\n",
    "    texts = []\n",
    "    targets = []\n",
    "    for label in ('pos', 'neg'):\n",
    "        data_dir = os.path.join(test_dir, label)\n",
    "        files = glob.glob(os.path.join(data_dir, '*.txt'))\n",
    "        for filename in files:\n",
    "            with open(filename,'r', encoding='UTF-8') as fi:\n",
    "                text = fi.read()\n",
    "            target = (label == 'pos')\n",
    "            texts.append(text)\n",
    "            targets.append(target)\n",
    "\n",
    "    test_docs = texts\n",
    "    y_test = np.array(targets)\n",
    "\n",
    "    inds = np.arange(y_train.shape[0])\n",
    "    np.random.shuffle(inds)\n",
    "\n",
    "    train_docs = [train_docs[i] for i in inds]\n",
    "    y_train = y_train[inds]\n",
    "    \n",
    "    return (train_docs, y_train), (test_docs, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 25000 train docs and 25000 test docs\n"
     ]
    }
   ],
   "source": [
    "(train_docs, y_train), (test_docs, y_test) = load_imdb_data_text('./data/aclImdb/')\n",
    "print('found {} train docs and {} test docs'.format(len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Michael Winner is probably best known for his revenge-themed films, such as \"Death Wish\" and \"Chato\\'s Land\", but he is equally gifted as a director of occult Horror cinema, as \"The Sentinel\" of 1977 proves. \"The Sentinel\", which is based on a novel by John Konvitz, who also wrote the screenplay, is a clever and immensely creepy religious chiller that no lover of occult Horror should consider missing. The film is obviously inspired by successful occult classics such as \"Rosemary\\'s Baby\", \"The Exorcist\" or \"The Omen\", but, as far as I am concerned, it is also easily as unsettling as these more widely acclaimed films, and probably even creepier.<br /><br />Allison Parker (Christina Raines) is a beautiful young New York model. Traumatized by events in her her past and not yet willing to marry her lawyer boyfriend (Chris Sarandon), Allison is in search for an apartment, and finds a big, incredibly nice one, which is also affordable, in an old mansion in Brooklyn. The new apartment, however, comes along with a bunch of very strange other tenants. And the sinister new neighbors soon become more than a little bothersome to Alice... This may not be an adequate plot synopsis, but I would hate to spoil any of this film\\'s great moments, so I will not give any further plot description. What I will say, however, is that \"The Sentinel\" is a very creepy and effective film that profits from a great cast as well as an often bizarre and constantly uncanny atmosphere. The fact that director Michael Winner and writer John Konnvitz also acted as producers here certainly had its influence on the outcome. The film is imaginatively photographed, and the eerie old Brooklyn mansion is a fantastic setting for this kind of film. As mentioned above, the atmosphere is obscure and creepy, and the film also includes several shock-moments and genuine scares. The film features many sinister and eccentric characters, and the cast is superb. Beautiful Christina Raines is great in her role of Allison Parker, lovable and yet on the cusp to losing her mind. Chris Sarandon is also very good as her boyfriend, a successful lawyer, and the supporting cast includes many big names, such as Christopher Walken, Jeff Goldblum, Jerry Orbach, Beverly D\\'Angelo and Tom Berenger, before becoming really famous. The cast also includes stars like Ava Gardner, Horror icon John Carradine, Burgess Meredith, and, my personal favorite, the great Eli Wallach as a cynical homicide detective. I\\'ve been a great fan of director Michael Winner for a long time, mostly for films like \"Death Wish\" and \"Chato\\'s Land\". \"The Sentinel\" is yet another great film in Winner\\'s repertoire, and also the proof that the man is not only a master of hard-boiled revenge-cinema, but also of atmospheric occult Horror. All in all, \"The Sentinel\" is a creepy, intelligent, and amazingly bizarre occult chiller that is highly recommended to all Horror fans!'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_docs' (list)\n"
     ]
    }
   ],
   "source": [
    "%store train_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'y_train' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "%store y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'test_docs' (list)\n"
     ]
    }
   ],
   "source": [
    "%store test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'y_test' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "%store y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r train_docs\n",
    "%store -r y_train\n",
    "%store -r test_docs\n",
    "%store -r y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vec = TfidfVectorizer(lowercase=True, max_features=10000)\n",
    "x_train = vec.fit_transform(thing for thing in train_docs)\n",
    "x_test = vec.fit_transform(thing for thing in test_docs)\n",
    "# more code here\n",
    "                      \n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "test_preds = lr.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53296"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = np.squeeze(lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 10000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.todense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: you can call `vec.get_feature_names` to get the words in order\n",
    "# that correspond to the columns of the TFIDF matrix \n",
    "# This is useful to pass to the index of a pd.Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame({'coef': coef, 'token': word}, columns=['coef', 'token']).sort_values(by='coef', ascending=False)\n",
    "\n",
    "# NB: to get the largest items in a series by abs try\n",
    "#coefs.loc[coefs.abs().nlargest(20).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>7.066042</td>\n",
       "      <td>grow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3209</th>\n",
       "      <td>6.072309</td>\n",
       "      <td>existenz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>4.932215</td>\n",
       "      <td>biggest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553</th>\n",
       "      <td>4.835743</td>\n",
       "      <td>phase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9877</th>\n",
       "      <td>4.604285</td>\n",
       "      <td>wonder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>4.043686</td>\n",
       "      <td>ambitious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9743</th>\n",
       "      <td>3.772415</td>\n",
       "      <td>weber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>3.753873</td>\n",
       "      <td>tits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3406</th>\n",
       "      <td>3.675948</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5397</th>\n",
       "      <td>3.671688</td>\n",
       "      <td>lush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3735</th>\n",
       "      <td>3.632001</td>\n",
       "      <td>galaxy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3080</th>\n",
       "      <td>3.433716</td>\n",
       "      <td>environmental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5396</th>\n",
       "      <td>3.393050</td>\n",
       "      <td>lurking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4281</th>\n",
       "      <td>3.305913</td>\n",
       "      <td>holds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8717</th>\n",
       "      <td>3.255022</td>\n",
       "      <td>support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>3.221957</td>\n",
       "      <td>bubble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>3.061595</td>\n",
       "      <td>jasper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>3.014215</td>\n",
       "      <td>deniro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>2.959590</td>\n",
       "      <td>blamed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4878</th>\n",
       "      <td>2.900542</td>\n",
       "      <td>judge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          coef          token\n",
       "3997  7.066042           grow\n",
       "3209  6.072309       existenz\n",
       "988   4.932215        biggest\n",
       "6553  4.835743          phase\n",
       "9877  4.604285         wonder\n",
       "439   4.043686      ambitious\n",
       "9743  3.772415          weber\n",
       "9091  3.753873           tits\n",
       "3406  3.675948         female\n",
       "5397  3.671688           lush\n",
       "3735  3.632001         galaxy\n",
       "3080  3.433716  environmental\n",
       "5396  3.393050        lurking\n",
       "4281  3.305913          holds\n",
       "8717  3.255022        support\n",
       "1237  3.221957         bubble\n",
       "4800  3.061595         jasper\n",
       "2436  3.014215         deniro\n",
       "1033  2.959590         blamed\n",
       "4878  2.900542          judge"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2647</th>\n",
       "      <td>-3.658181</td>\n",
       "      <td>displayed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648</th>\n",
       "      <td>-3.697428</td>\n",
       "      <td>displaying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7499</th>\n",
       "      <td>-3.740254</td>\n",
       "      <td>ridley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7807</th>\n",
       "      <td>-3.762265</td>\n",
       "      <td>scum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8633</th>\n",
       "      <td>-3.765571</td>\n",
       "      <td>subjected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>-3.847381</td>\n",
       "      <td>anti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6106</th>\n",
       "      <td>-4.087997</td>\n",
       "      <td>noteworthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9378</th>\n",
       "      <td>-4.100726</td>\n",
       "      <td>unexpected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4384</th>\n",
       "      <td>-4.196183</td>\n",
       "      <td>humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2865</th>\n",
       "      <td>-4.258127</td>\n",
       "      <td>earliest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6749</th>\n",
       "      <td>-4.268642</td>\n",
       "      <td>posing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9908</th>\n",
       "      <td>-4.497154</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6151</th>\n",
       "      <td>-4.636251</td>\n",
       "      <td>objective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8934</th>\n",
       "      <td>-4.726659</td>\n",
       "      <td>tepid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6748</th>\n",
       "      <td>-5.357833</td>\n",
       "      <td>poses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>-5.981622</td>\n",
       "      <td>boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9688</th>\n",
       "      <td>-6.279449</td>\n",
       "      <td>warrant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>-6.458425</td>\n",
       "      <td>babs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>-7.572423</td>\n",
       "      <td>bam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9910</th>\n",
       "      <td>-9.164331</td>\n",
       "      <td>worse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          coef       token\n",
       "2647 -3.658181   displayed\n",
       "2648 -3.697428  displaying\n",
       "7499 -3.740254      ridley\n",
       "7807 -3.762265        scum\n",
       "8633 -3.765571   subjected\n",
       "514  -3.847381        anti\n",
       "6106 -4.087997  noteworthy\n",
       "9378 -4.100726  unexpected\n",
       "4384 -4.196183       humor\n",
       "2865 -4.258127    earliest\n",
       "6749 -4.268642      posing\n",
       "9908 -4.497154       worry\n",
       "6151 -4.636251   objective\n",
       "8934 -4.726659       tepid\n",
       "6748 -5.357833       poses\n",
       "1151 -5.981622         boy\n",
       "9688 -6.279449     warrant\n",
       "778  -6.458425        babs\n",
       "809  -7.572423         bam\n",
       "9910 -9.164331       worse"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments here\n",
    "# it makes sense that the words such as worse, tepid, worry, unexpected are shown in the largest negative coefficients, since these words are all negative words.\n",
    "# However, it seems the positive words do not work very well, probably because the result is not very accurate in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
