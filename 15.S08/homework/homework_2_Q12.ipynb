{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Obtain structured company data using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>page_title</th>\n",
       "      <th>page_views</th>\n",
       "      <th>intro_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>856</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>190485</td>\n",
       "      <td>Apple Inc. is an American multinational techno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2386</td>\n",
       "      <td>American Airlines</td>\n",
       "      <td>40829</td>\n",
       "      <td>American Airlines, Inc. (AA) is a major Americ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_id         page_title  page_views  \\\n",
       "0      856         Apple Inc.      190485   \n",
       "1     2386  American Airlines       40829   \n",
       "\n",
       "                                          intro_text  \n",
       "0  Apple Inc. is an American multinational techno...  \n",
       "1  American Airlines, Inc. (AA) is a major Americ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df = pd.read_csv('data/kdwd_r1k_articles.csv')\n",
    "wiki_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -) Write a regex to find unusually capitalized terms\n",
    "Sometimes product names will have unusual capitalization such as iPhone or ThinkPad. Find a list of such terms and investigate if you think some of them are products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "613174"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [i for i in wiki_df['intro_text']]\n",
    "all_text = ' '.join(data)\n",
    "len(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_products_ptn = r'\\b[A-Z]*[a-z]+[A-Z][a-z]+\\b' # find the correct regular expression here\n",
    "\n",
    "# code here!\n",
    "\n",
    "maybe_products_set = []\n",
    "maybe_products_set = re.findall(maybe_products_ptn, all_text)\n",
    "# print(len(maybe_products_set), 'terms found that are potential products')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that we found some of the key terms\n",
    "assert('iPhone' in maybe_products_set)\n",
    "assert('ThinkPad' in maybe_products_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -) Parse company acquisition data from plain text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking to identify the following types of patterns.<br />\n",
    "`'Citrix acquired Sequoia Software Corp'`<br />\n",
    "`'Moody\\'s was acquired by Dun & Bradstreet in 1962.'`<br />\n",
    "The idea here is to look for patterns around the word 'acquire' with two valid entities on either side, and an option year at the end.<br />\n",
    "<span style=\"color:orange\">Helpful Reminder:</span> you can create non-capturing capture groups via `(?:capture this|or that)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a way to capture, 'FedEx', 'Coca-Cola', 'Sequoia Software Corp', 'Dun & Bradstreet' and 'Moody\\'s'\n",
    "company_ptn = r'(?:[A-Z][a-z]+\\s)?[A-Z][a-z]+(?:\\-|\\s\\&\\s|\\'[s])?(?:[A-Z][a-z]+)?(?:\\s[Corp]+)?'  # write pattern\n",
    "maybe_companies_set = re.findall(company_ptn, all_text)\n",
    "# code here!\n",
    "# print(len(maybe_companies_set), 'terms found that are potential companies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that we found some of the key terms\n",
    "assert('FedEx' in maybe_companies_set)\n",
    "assert('Coca-Cola' in maybe_companies_set)\n",
    "assert('Sequoia Software Corp' in maybe_companies_set)\n",
    "assert('Dun & Bradstreet' in maybe_companies_set)\n",
    "assert('Moody\\'s' in maybe_companies_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquisition pattern\n",
    "acquisition_ptn = r'(?:acquired|was acquired by)'\n",
    "\n",
    "# find a way to optionally capture the year such as ' in 1962'\n",
    "optional_year_ptn = r'(?: in [12][0-9]{3}\\b)?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's assemble the patterns together to a full capture pattern\n",
    "full_acquisition_pattern = (\n",
    "    company_ptn + r'\\s+' + acquisition_ptn + r'\\s+' + company_ptn + optional_year_ptn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 potential acquisitions found.\n"
     ]
    }
   ],
   "source": [
    "acquisition_strings = []\n",
    "for _, row in wiki_df.iterrows():\n",
    "    acquisition_strings.extend(re.findall(full_acquisition_pattern, row['intro_text']))\n",
    "print(len(acquisition_strings), 'potential acquisitions found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that we found some of the key terms\n",
    "assert('Citrix acquired Sequoia Software Corp' in acquisition_strings)\n",
    "assert('Moody\\'s was acquired by Dun & Bradstreet in 1962' in acquisition_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -) Question: Are there any false positives in your results? If so, how could you account for them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are false positives in the results. Probably we could find a database for company names, and run some matching algo to further filter the companies mentioned in our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -) Let's look into speed of regex matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a simple pattern of your choice to search for in our dataset\n",
    "search_ptn = r'iPhone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble a list of strings\n",
    "doc_list = wiki_df['intro_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 ms ± 33.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for doc in doc_list:\n",
    "    re.search(search_ptn, doc)\n",
    "# evaluation loop here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compile the regex and see if this increases the speed using `re.compile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21 ms ± 37.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# evaluation loop here\n",
    "compiled_ptn = re.compile(r'iPhone')\n",
    "for doc in doc_list:\n",
    "    re.search(compiled_ptn, doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on speed: basic string operations are always faster than regex\n",
    "#### show this using `'my_string' in 'other_string'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.8 ns ± 2.13 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "'iPhone' in 'This is the new iPhone 13.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674 ns ± 42.8 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "re.search('iPhone', 'This is the new iPhone 13.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Word phrases\n",
    "\n",
    "### In this problem we will look at methods to identify valid n-grams such as 'New York' or 'Barack Obama' while eliminating statistical flukes such as `in the` or `i write`.\n",
    "\n",
    "### Preprocessing such as this can drastically improved embeddings since words can ngrams will often have a different meaning than the sum of its parts\n",
    "### `V('united')` + `V('states')` != `V('united states')`\n",
    "### `V('real')` + `V('estate')` != `V('real estate')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df = pd.read_csv('data/kdwd_r1k_articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get consecutive unigrams for the 'intro_text' column of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of lists of unigrams\n",
    "unigram_pattern = r'[a-z0-9]+'\n",
    "corpus = [re.findall(unigram_pattern, doc.lower()) for doc in wiki_df['intro_text'].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The package `gensim` has a convenient wrapper to obtain statistically significant ngrams/Phrase automatically\n",
    "\n",
    "### we need to first `pip install gensim`\n",
    "### `gensim` is a useful library for anything related to word representations and embeddings. It will come up a few more times. https://radimrehurek.com/gensim/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write some code to parse our corpus and use valid ngrams using `Phrases`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(corpus, min_count=1, threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_count_dict = {k: v for k, v in phrases.vocab.items()}\n",
    "\n",
    "n_grams = pd.Series(vocab_count_dict)\n",
    "n_grams = n_grams.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60689 n-grams found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "the            4873\n",
       "and            4173\n",
       "in             3706\n",
       "of             2422\n",
       "company        1884\n",
       "is             1686\n",
       "a              1336\n",
       "to             1072\n",
       "the_company     999\n",
       "s               961\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(n_grams.shape[0], 'n-grams found')\n",
    "n_grams.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do the results look? Can you improve the results by excluding common terms using the `connector_words` kwarg of `Phrases`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(corpus, min_count=1, threshold=1, connector_words=ENGLISH_CONNECTOR_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_count_dict = {k: v for k, v in phrases.vocab.items()}\n",
    "\n",
    "n_grams = pd.Series(vocab_count_dict)\n",
    "n_grams = n_grams.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60573 n-grams found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "company     1884\n",
       "is          1686\n",
       "s            961\n",
       "as           905\n",
       "it           757\n",
       "its          746\n",
       "was          657\n",
       "american     498\n",
       "inc          473\n",
       "largest      453\n",
       "dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(n_grams.shape[0], 'n-grams found')\n",
    "n_grams.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This was convenient, but it's also a black box where many of the knobs for tuning are actually broken in the newest version. Let's try to create our own solution for finding n-grams.\n",
    "\n",
    "### To do this, let's start by counting unigrams and bigrams within our corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip: use Counter for easy counting. It behaves similar to a dictionary with some added functionality around counting. such as `my_counter[unknown_key]` returning `0` for all unknown keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "unigram_counter = Counter()\n",
    "bigram_counter = Counter()\n",
    "for tokens in corpus:\n",
    "    # your code here\n",
    "    for idx in range(len(tokens)-1):\n",
    "        unigram_counter[tokens[idx]] +=1\n",
    "        bigram_counter[tokens[idx] + ' ' + tokens[idx+1]] +=1\n",
    "    unigram_counter[tokens[len(tokens)-1]] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to come up with a score for each bigram that helps us decide on its importance and the fact of whether it is truly a bigram or two independent unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-bf9ea4c7b725>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwiki_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'intro_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mtf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtf\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mdf\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "n,_ = wiki_df.shape\n",
    "tokens_tf = {}\n",
    "tokens_tf_idf = {}\n",
    "\n",
    "for token in bigram_counter.keys():\n",
    "    tf = 0\n",
    "    df = 0\n",
    "    tokens_tf[token] = []\n",
    "    for doc in wiki_df['intro_text'].tolist():\n",
    "        doc = doc.lower()\n",
    "        tf = doc.count(token)/(len(doc.split())-1)\n",
    "        if tf > 0:\n",
    "            df +=1\n",
    "        tokens_tf[token].append(tf)\n",
    "    idf = np.log((1+n)/(1+df)) + 1\n",
    "    tokens_tf_idf[token] = []\n",
    "    for tf in tokens_tf[token]:\n",
    "        tokens_tf_idf[token].append(tf*idf)\n",
    "        \n",
    "largest_tokens_tf_idf = {}\n",
    "for token in bigram_counter.keys():\n",
    "    largest_tokens_tf_idf[token] = max(tokens_tf_idf[token])\n",
    "bigram_df = pd.DataFrame(largest_tokens_tf_idf.items(), columns=['bigram','tf-idf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple inc</td>\n",
       "      <td>0.034528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inc is</td>\n",
       "      <td>0.121065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is an</td>\n",
       "      <td>0.147476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>an american</td>\n",
       "      <td>0.158072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>american multinational</td>\n",
       "      <td>0.282122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>multinational technology</td>\n",
       "      <td>0.077946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>technology company</td>\n",
       "      <td>0.122653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>company headquartered</td>\n",
       "      <td>0.192896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>headquartered in</td>\n",
       "      <td>0.145858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in cupertino</td>\n",
       "      <td>0.010270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bigram    tf-idf\n",
       "0                 apple inc  0.034528\n",
       "1                    inc is  0.121065\n",
       "2                     is an  0.147476\n",
       "3               an american  0.158072\n",
       "4    american multinational  0.282122\n",
       "5  multinational technology  0.077946\n",
       "6        technology company  0.122653\n",
       "7     company headquartered  0.192896\n",
       "8          headquartered in  0.145858\n",
       "9              in cupertino  0.010270"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find ways to sort and filter your output to bigrams that make sense, such as `wells fargo`, `apple inc` or `puerto rico`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple inc</td>\n",
       "      <td>0.034528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inc is</td>\n",
       "      <td>0.121065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is an</td>\n",
       "      <td>0.147476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>an american</td>\n",
       "      <td>0.158072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>american multinational</td>\n",
       "      <td>0.282122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50384</th>\n",
       "      <td>formerly l</td>\n",
       "      <td>0.079802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50385</th>\n",
       "      <td>l 3</td>\n",
       "      <td>0.138913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50386</th>\n",
       "      <td>3 communications</td>\n",
       "      <td>0.079802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50387</th>\n",
       "      <td>and harris</td>\n",
       "      <td>0.079802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50388</th>\n",
       "      <td>harris corporation</td>\n",
       "      <td>0.079802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28408 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       bigram    tf-idf\n",
       "0                   apple inc  0.034528\n",
       "1                      inc is  0.121065\n",
       "2                       is an  0.147476\n",
       "3                 an american  0.158072\n",
       "4      american multinational  0.282122\n",
       "...                       ...       ...\n",
       "50384              formerly l  0.079802\n",
       "50385                     l 3  0.138913\n",
       "50386        3 communications  0.079802\n",
       "50387              and harris  0.079802\n",
       "50388      harris corporation  0.079802\n",
       "\n",
       "[28408 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "filtered_bigram_df = bigram_df[bigram_df['tf-idf']>0.03]\n",
    "filtered_bigram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49911</th>\n",
       "      <td>alexandria real</td>\n",
       "      <td>0.631164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49738</th>\n",
       "      <td>provides b2b</td>\n",
       "      <td>0.631164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49913</th>\n",
       "      <td>equities is</td>\n",
       "      <td>0.631164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49739</th>\n",
       "      <td>b2b it</td>\n",
       "      <td>0.631164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49912</th>\n",
       "      <td>estate equities</td>\n",
       "      <td>0.631164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29985</th>\n",
       "      <td>in hamilton</td>\n",
       "      <td>0.653733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50072</th>\n",
       "      <td>reinsurance company</td>\n",
       "      <td>0.694280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50071</th>\n",
       "      <td>a reinsurance</td>\n",
       "      <td>0.694280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50069</th>\n",
       "      <td>everest re</td>\n",
       "      <td>0.694280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19817</th>\n",
       "      <td>marathon oil</td>\n",
       "      <td>0.747124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    bigram    tf-idf\n",
       "49911      alexandria real  0.631164\n",
       "49738         provides b2b  0.631164\n",
       "49913          equities is  0.631164\n",
       "49739               b2b it  0.631164\n",
       "49912      estate equities  0.631164\n",
       "29985          in hamilton  0.653733\n",
       "50072  reinsurance company  0.694280\n",
       "50071        a reinsurance  0.694280\n",
       "50069           everest re  0.694280\n",
       "19817         marathon oil  0.747124"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_bigram_df.sort_values(by = 'tf-idf').tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Corporate Similarity and Returns\n",
    "### In this example we'll explore how to use NLP to measure corporate similarity\n",
    "\n",
    "### In particular we will\n",
    " - ### Make word vectors for firms in order to get an NLP measure of similarity\n",
    " - ### Measure the quality of this similarity metric by predicting future co-movement of returns. \n",
    " \n",
    "## Step X: This problem uses a few concepts of basic modeling such as `sklearn.model_selection.train_test_split` and `sklearn.linear_model.LinearRegression`\n",
    "## Feel free to read some of the sklearn documentation, but otherwise we will cover these concepts next class\n",
    " \n",
    "\n",
    "# $ \\\\ $\n",
    "## Step 0: Load the MD&A section from Form-10-K from 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/parsed_mda.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, take only the first filing for each company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = []\n",
    "seen = set()\n",
    "for item in data:\n",
    "    if item['ticker'] in seen:\n",
    "        continue\n",
    "    else:\n",
    "        seen.add(item['ticker'])\n",
    "        clean.append(item)\n",
    "data = clean\n",
    "del clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now load the price data for 2015-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.read_csv(\n",
    "    '../../data/sp500_prices.csv', \n",
    "    index_col=0, \n",
    "    parse_dates=True\n",
    ").loc['2015-01-01':'2018-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tickers = [item['ticker'] for item in data]\n",
    "assert len(data_tickers) == len(set(data_tickers)), 'non-unique tickers, this will not work'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: clean the text\n",
    "### Much of NLP boils down to doing reasonable processing on text.\n",
    "### First, we'll try out very minimial processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mda_simple(mda):\n",
    "    return mda.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add import here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pairwise Word similarity\n",
    "### Calculate the pariwise cosine similarity between word vectors\n",
    "### Make the cosine similarities into a dataframe indexed/columned on ticker symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sims = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Why `cosine_similarity` and not another measure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Wrangle the price and word data\n",
    "### Our goal here is to have a dataframe which is indexed on PAIRS of tickers and has columns\n",
    " - ### `returns_correlation`: the correlation of returns for those two tickers from Jan 1 2016 to Jan 1 2017\n",
    " - ### `word_similarity`: the cosine similarity of the word vectors for the two companies' MD&A sections\n",
    " \n",
    "## Tips\n",
    " - ### NB: use pct_change to calculate returns in pandas\n",
    " - ### NB: use the pandas builtin corr function to calculate correlations (we don't need anything fancy)\n",
    " - ### NB: the index of the dataframe should have two columns (the tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one way you might do this is\n",
    "rets_cor = ... # calculate returns correlations\n",
    "word_cor = #  calcuate the word similarities in the right shape\n",
    "\n",
    "all_data = rets_cor.join(word_cor)\n",
    "all_data = all_data.dropna()\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a: \n",
    " - ### What is the contemperaneous correlation of these data?\n",
    " - ### Make a scatter plot of the returns correlation and word similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This should be about 12%. That's not bad, but we can do better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.plot.scatter(x='returns_correlation', y='word_similarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Try to predict the future returns correlations\n",
    "### Use OLS (`LinearRegression`) to predict `returns_correlation` from `word_similarity`. \n",
    "### What is the (contemperaneous) out of sample performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df =  ...\n",
    "feature_cols =  ...\n",
    "target_col =  ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here\n",
    "\n",
    "reg = ...# add code here\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(reg.coef_, index=feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is not amazing. We can do better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ \\\\ $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Repeat, but be careful\n",
    "### Here we will see if we can clean the data better\n",
    "\n",
    "### Things to try\n",
    " - ### Look at the histograms of word similarities to see if we can \"ignore\" some ill-behaved data\n",
    " - ### Try limiting how greedy the `TFIDFVectorizer` is: `min_df`, `max_df`, `max_features`, etc.\n",
    " \n",
    "### We will examine our data and look for things that look out of place\n",
    " - ### We will ultimately want our data to look normally distributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mda(mda):\n",
    "    paras = [p.lower() for p in mda.split('\\n') if len(p) > 40]\n",
    "    cleaned =  ' '.join(paras)\n",
    "    words = cleaned.split()\n",
    "    words = [word for word in words if len(word) > 2]\n",
    "    if len(words) > 10:\n",
    "        return ' '.join(words)\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(\n",
    "    min_df=...\n",
    "    max_df= ...\n",
    "    max_features= ...\n",
    ")\n",
    "word_vecs = vec.fit_transform((clean_mda(item['mda']) for item in data))\n",
    "\n",
    "\n",
    "word_sims =  ...\n",
    "# Lots of word similarities are all zeros- so we'll ignore\n",
    "# add code here to remove rows of word_sims where all the elements are zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the returns correlation and the cosine similarities as above\n",
    "all_data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect your data- make some histograms\n",
    "plt.figure(figsize=(12,7))\n",
    "all_data.returns_correlation.hist(bins=40)\n",
    "plt.title('Returns Correlation')\n",
    "plt.figure(figsize=(12,7))\n",
    "all_data.word_similarity.hist(bins=40)\n",
    "plt.title('Word Similarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning our data\n",
    "### It seems lots of things are identically 0 (no word overlap) or identically 1 (the MD&A section for one company perfectly overlaps itself). We will exclude those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine histograms again\n",
    "plt.figure(figsize=(12,7))\n",
    "all_data.returns_correlation.hist(bins=40)\n",
    "plt.title('Returns Correlation')\n",
    "plt.figure(figsize=(12,7))\n",
    "all_data.word_similarity.hist(bins=40)\n",
    "plt.title('Word Similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly, there is a bit of a \"hump\" at low `word_similarity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here\n",
    "all_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The contemperaneous correlation is twice as large!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Now, repeat the exercise of predicting future returns correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df =  ...# Add code here\n",
    "# add code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg =  ...\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(reg.coef_, index=feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is about 5 times better than before!\n",
    "## $ \\\\ $ \n",
    "## Part 7: What will happen if we include last year's returns correlation as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_year_corr =  ...\n",
    "data_df = last_year_corr.join( ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg =  ...\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(reg.coef_, index=feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indeed, we do much better, but the word features still help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9ed21ce3e79e551a27a11d2551dd0fb060a3f4c5f44a14454b2145fe2b135cb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
